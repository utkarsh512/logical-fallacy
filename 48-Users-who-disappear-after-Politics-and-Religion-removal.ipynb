{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNmuE1xHBum/3Y6P+sFjqWe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WGGtb6ND1guN"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"gCevtV4s2jSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   pprint                   import pprint\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)"],"metadata":{"id":"BJXXDYlY22PL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Helper functions"],"metadata":{"id":"KOxlx21q3BrA"}},{"cell_type":"code","source":["tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"n3E0PReF24h9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUR_WORDS = {\n","  \"jews\": [\n","    \"jews\",\n","    \"oven dodger\",\n","    \"nazi\",\n","    \"dirty jew\",\n","    \"holocaust\",\n","    \"kikesucker\",\n","    \"hook nose\",\n","    \"kike\"\n","  ],\n","  \"homosexual\": [\n","    \"faggots usually\",\n","    \"fucking queer\",\n","    \"the biggest faggot\",\n","    \"dyke\",\n","    \"you fucking faggot\",\n","    \"hate faggots\",\n","    \"queer\",\n","    \"homosexual\",\n","    \"the faggots\",\n","    \"faggot\",\n","    \"faggots usually have\",\n","    \"gay\",\n","    \"faggots\",\n","    \"dykey\",\n","    \"ugly dyke\",\n","    \"faggots like you\",\n","    \"you a fag\",\n","    \"lesbian\",\n","    \"homo\",\n","    \"is a faggot\",\n","    \"like a faggot\",\n","    \"dykes\",\n","    \"faggots like\",\n","    \"faggot if you ever\"\n","  ],\n","  \"women\": [\n","    \"ugly dyke\",\n","    \"woman terrorist\",\n","    \"nigress\",\n","    \"bitch\",\n","    \"slut\",\n","    \"women\",\n","    \"sheeboon\",\n","    \"negress\",\n","    \"mud shark\",\n","    \"women threat\",\n","    \"you a lame bitch\",\n","    \"your a cunt\",\n","    \"white bitch\",\n","    \"niggeress\",\n","    \"hoe\",\n","    \"dykes\",\n","    \"niggress\",\n","    \"sheboon\",\n","    \"feminazi\"\n","  ],\n","  \"blacks\": [\n","    \"pavement ape\",\n","    \"the niggers\",\n","    \"negress\",\n","    \"porch monkey\",\n","    \"that nigger\",\n","    \"this nigger\",\n","    \"sheboon\",\n","    \"all niggers\",\n","    \"eurafrica\",\n","    \"shut up nigger\",\n","    \"picaninny\",\n","    \"african attack\",\n","    \"spearchucker\",\n","    \"how many niggers\",\n","    \"nigger\",\n","    \"africa\",\n","    \"niggers are in my\",\n","    \"dindu nuffin\",\n","    \"stupid nigger\",\n","    \"moolie\",\n","    \"niggers\",\n","    \"bluegum\",\n","    \"nigger ass\",\n","    \"you niggers\",\n","    \"fucking nigger\",\n","    \"nigger music\",\n","    \"niggress\",\n","    \"you a nigger\",\n","    \"many niggers are\",\n","    \"nigress\",\n","    \"blacks\",\n","    \"teenaper\",\n","    \"sheeboon\",\n","    \"dumb nigger\",\n","    \"niggeress\",\n","    \"pickaninny\",\n","    \"nigga\"\n","  ],\n","  \"muslim\": [\n","    \"muslim immigrant\",\n","    \"islam\",\n","    \"mudslime\",\n","    \"mooslem\",\n","    \"muslim refugee\",\n","    \"musslime\",\n","    \"shitlam\",\n","    \"muslim invasion\",\n","    \"moslime\",\n","    \"mooslamic\",\n","    \"muzzie\",\n","    \"allah akbar\",\n","    \"mooslime\",\n","    \"musloid\",\n","    \"mudslimes\",\n","    \"muslim\",\n","    \"muslimes\",\n","    \"moslum\",\n","    \"mussie\",\n","    \"muzrat\",\n","    \"muslim countries\",\n","    \"muzzy\",\n","    \"moslim\",\n","    \"jihadi\",\n","    \"muslim country\",\n","    \"moslem\",\n","    \"muzzrat\",\n","    \"mooslim\"\n","  ],\n","  \"arabs\": [\n","    \"towel head\",\n","    \"goatfucker\",\n","    \"arabs\",\n","    \"goathumper\",\n","    \"raghead\",\n","    \"rag head\",\n","    \"goathumping\",\n","    \"towelhead\",\n","    \"camel jockey\",\n","    \"sandnigger\",\n","    \"camel fucker\",\n","    \"sand nigger\"\n","  ],\n","  \"generic\": [\n","    \"to rape\",\n","    \"raped and\",\n","    \"shithole country\",\n","    \"get raped\",\n","    \"raped\",\n","    \"is a fucking\",\n","    \"shit skin\",\n","    \"raped by\",\n","    \"hate you\",\n","    \"fake empowerment\",\n","    \"abusive women\",\n","    \"fuck you too\",\n","    \"violence\",\n","    \"wit a lame nigga\",\n","    \"they all look\",\n","    \"alllivesmatter\",\n","    \"shithole countries\",\n","    \"fucking hate\",\n","    \"trailer trash\",\n","    \"kill all\",\n","    \"terrorist threat\",\n","    \"harassment\",\n","    \"kill yourself\",\n","    \"shitskin\",\n","    \"okay to be white\",\n","    \"fucking hate you\"\n","  ],\n","  \"white\": [\n","    \"full of white\",\n","    \"white trash\",\n","    \"white devil\",\n","    \"white\",\n","    \"are all white\",\n","    \"white boy\",\n","    \"white ass\",\n","    \"white bitch\",\n","    \"hillbilly\",\n","    \"whigger\",\n","    \"white christian\",\n","    \"white person\",\n","    \"all white\",\n","    \"white nigger\",\n","    \"redneck\",\n","    \"white honky\",\n","    \"wigger\",\n","    \"them white\"\n","  ],\n","  \"economy\": [\n","    \"ghetto\"\n","  ],\n","  \"immigrant\": [\n","    \"illegal immigrants\",\n","    \"immigrant not welcome\",\n","    \"immigrant terror\",\n","    \"mexcrement\",\n","    \"go back to where you come from\",\n","    \"muslim refugee\",\n","    \"illegal aliens\",\n","    \"refugee\",\n","    \"protect from immigrants\",\n","    \"negro\",\n","    \"refugees\",\n","    \"immigrant\",\n","    \"refugee invasion\",\n","    \"go back to where they come from\",\n","    \"refugees impact\",\n","    \"bring ebola\",\n","    \"immigrants\",\n","    \"illegal alien\",\n","    \"immigrant invasion\",\n","    \"bring disease\"\n","  ],\n","  \"mental\": [\n","    \"retard\",\n","    \"mongoloid\",\n","    \"retarded\"\n","  ],\n","  \"asians\": [\n","    \"asians\",\n","    \"ching chong\",\n","    \"chinaman\"\n","  ]\n","}"],"metadata":{"id":"8AqwqpZf3FeK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading CreateDebate dataset"],"metadata":{"id":"E4Te87-G3Ls7"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"e9rsQ0Po3I_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append(v)\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append(v)\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"MNX-vQ4-3Ria"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading slur word statistics for CreateDebate"],"metadata":{"id":"gCKAn03f3z39"}},{"cell_type":"code","source":["# Loading computation from cache\n","\n","with open('/content/gdrive/MyDrive/Temp/47-ah-score.pkl', 'rb') as fp:\n","    ah_score = pickle.load(fp)\n","\n","with open('/content/gdrive/MyDrive/Temp/47-slur-count.pkl', 'rb') as fp:\n","    slur_count = pickle.load(fp)"],"metadata":{"id":"yUHoMLA_3U_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"qfZ8hr7P4HHo"}},{"cell_type":"code","source":["# Median ah score per category per author\n","#   key: category -> author\n","#   value: median ah score\n","ah_score_median = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_median[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_median[category][author] = np.median(ah_scores)"],"metadata":{"id":"0g2OPRWX3-L8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count = dict()\n","# key: category -> author\n","# value: number of comments written by author in the given forum\n","\n","for category in categories_selected:\n","    comment_count[category] = dict()\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        try:\n","            comment_count[category][author] += 1\n","        except KeyError:\n","            comment_count[category][author] = 1"],"metadata":{"id":"90C87oYc4J3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EXTREME_AH_UPPER = 1\n","EXTREME_AH_LOWER = 0.95\n","\n","MODERATE_AH_UPPER = 0.8\n","MODERATE_AH_LOWER = 0.7\n","\n","LOW_AH_UPPER = 0.6\n","LOW_AH_LOWER = 0.5"],"metadata":{"id":"pIkLEobo4Nip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_users(category, slur_group, ignore_politics=False, ignore_religion=False):\n","    \"\"\"\n","    @brief Return the partition of users for given category based on given flags\n","\n","    @param category: Category to investigate\n","    @param slur_group: Slur group to investigate\n","    @param ignore_politics: if True, remove users who also participate in Politics\n","    @param ignore_religion: if True, remove users who also participate in Religion\n","\n","    @return: extreme_ah_users, moderate_ah_users, low_ah_users (set)\n","    \"\"\"\n","    extreme_ah_users = set()\n","    moderate_ah_users = set()\n","    low_ah_users = set()\n","\n","    for author, median_ah_score in ah_score_median[category].items():\n","        n_politics_cnt = 0\n","        n_religion_cnt = 0\n","        try:\n","            n_politics_cnt = comment_count['politics2'][author]\n","        except KeyError:\n","            pass\n","        try:\n","            n_religion_cnt = comment_count['religion'][author]\n","        except KeyError:\n","            pass\n","\n","        if ignore_politics and n_politics_cnt > 0:\n","            continue\n","        \n","        if ignore_religion and n_religion_cnt > 0:\n","            continue\n","\n","        if LOW_AH_LOWER <= median_ah_score and median_ah_score <= LOW_AH_UPPER:\n","            if sum(slur_count[category][slur_group][author]) > 0:\n","                low_ah_users.add(author)\n","\n","        elif MODERATE_AH_LOWER <= median_ah_score and median_ah_score <= MODERATE_AH_UPPER:\n","            if sum(slur_count[category][slur_group][author]) > 0:\n","                moderate_ah_users.add(author)\n","        \n","        elif EXTREME_AH_LOWER <= median_ah_score and median_ah_score <= EXTREME_AH_UPPER:\n","            if sum(slur_count[category][slur_group][author]) > 0:\n","                extreme_ah_users.add(author)\n","\n","    return extreme_ah_users, moderate_ah_users, low_ah_users"],"metadata":{"id":"BPZ3goY34Sg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["disappeared_users_e = dict()\n","disappeared_users_m = dict()\n","disappeared_users_l = dict()\n","\n","for category in categories_selected:\n","    disappeared_users_e[category] = set()\n","    disappeared_users_m[category] = set()\n","    disappeared_users_l[category] = set()\n","\n","    for slur_group in SLUR_WORDS.keys():\n","        e1, m1, l1 = partition_users(category, slur_group)\n","        e2, m2, l2 = partition_users(category, slur_group, ignore_politics=True, ignore_religion=True)\n","\n","        disappeared_users_e[category] |= (e1 - e2)\n","        disappeared_users_m[category] |= (m1 - m2)\n","        disappeared_users_l[category] |= (l1 - l2)"],"metadata":{"id":"PRXNKfzC9sq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_top_20_ah_comments(category, user_list):\n","    comment_ah_scores = [0 for _ in range(20)]\n","    comment_texts = [\"\" for _ in range(20)]\n","    comment_authors = [\"\" for _ in range(20)]\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        if author not in user_list:\n","            continue\n","\n","        if comment['tag'] != category:\n","            continue\n","        \n","        ah_score_ = 1 - comment['score']\n","        text = comment['body']\n","\n","        min_score = min(comment_ah_scores)\n","        pos = comment_ah_scores.index(min_score)\n","\n","        if ah_score_ > min_score:\n","            comment_ah_scores[pos] = ah_score_\n","            comment_texts[pos] = text\n","            comment_authors[pos] = author\n","\n","    return tuple(comment_ah_scores), tuple(comment_texts), tuple(comment_authors)"],"metadata":{"id":"eSnJ2OzD-57V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Politics"],"metadata":{"id":"Fx2EG7U9kQRm"}},{"cell_type":"code","source":["e = set()\n","m = set()\n","l = set()\n","\n","for slur_group in SLUR_WORDS.keys():\n","    e1, m1, l1 = partition_users('politics2', slur_group)\n","    e2, m2, l2 = partition_users('politics2', slur_group, ignore_religion=True)\n","\n","    e |= (e1 - e2)\n","    m |= (m1 - m2)\n","    l |= (l1 - l2)"],"metadata":{"id":"Wq5ykoL2kS5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('politics2', e)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(e))"],"metadata":{"id":"pHMZiUzokwgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('politics2', m)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(m))"],"metadata":{"id":"30r5VBmcl5Mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('politics2', l)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(l))"],"metadata":{"id":"q6C9KcC2mcq2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Religion"],"metadata":{"id":"zweVAkKQnCtu"}},{"cell_type":"code","source":["e = set()\n","m = set()\n","l = set()\n","\n","for slur_group in SLUR_WORDS.keys():\n","    e1, m1, l1 = partition_users('religion', slur_group)\n","    e2, m2, l2 = partition_users('religion', slur_group, ignore_politics=True)\n","\n","    e |= (e1 - e2)\n","    m |= (m1 - m2)\n","    l |= (l1 - l2)"],"metadata":{"id":"5KncJbIdnF4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('religion', e)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(e))"],"metadata":{"id":"nzW3mnHZncel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('religion', m)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(m))"],"metadata":{"id":"_gt8FTvyoT0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('religion', l)\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(l))"],"metadata":{"id":"1MAjYpwspLay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## World news"],"metadata":{"id":"3PN8b_YFCWi9"}},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('world', disappeared_users_e['world'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_e['world']))"],"metadata":{"id":"tqdLAGtdCTsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('world', disappeared_users_m['world'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_m['world']))"],"metadata":{"id":"Ql7N3VWvCHeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('world', disappeared_users_l['world'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_l['world']))"],"metadata":{"id":"Uu_urUfVD9df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Science"],"metadata":{"id":"296bp8bDIpMj"}},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('science', disappeared_users_e['science'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_e['science']))"],"metadata":{"id":"kJrdJ97zHrKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('science', disappeared_users_m['science'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_m['science']))"],"metadata":{"id":"tWUzZYoeIyGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('science', disappeared_users_l['science'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_l['science']))"],"metadata":{"id":"y1MkT2YYJyC6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Law"],"metadata":{"id":"r8te6A_XLqQn"}},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('law', disappeared_users_e['law'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_e['law']))"],"metadata":{"id":"aHk39WzqKowe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('law', disappeared_users_m['law'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_m['law']))"],"metadata":{"id":"tsXMwQtbLxi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('law', disappeared_users_l['law'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_l['law']))"],"metadata":{"id":"4b6lak5VMrvn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Technology"],"metadata":{"id":"JANTzfccPDxz"}},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('technology', disappeared_users_e['technology'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_e['technology']))"],"metadata":{"id":"sgNxDL20OGSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('technology', disappeared_users_m['technology'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_m['technology']))"],"metadata":{"id":"NRUrJfqqPM3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores, texts, users = get_top_20_ah_comments('technology', disappeared_users_l['technology'])\n","\n","for text in texts:\n","    print(f'{text}\\n\\n')\n","\n","print(len(disappeared_users_l['technology']))"],"metadata":{"id":"jk1R8B69QKJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x1KrizmfQUNd"},"execution_count":null,"outputs":[]}]}