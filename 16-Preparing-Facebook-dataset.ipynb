{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16-Preparing-Facebook-dataset.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9g1tGjz83+YwtUD7tLx3I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hf2heFRtPMwt"},"source":["# Preparing Facebook dataset\n","* __Objective__: Create facebook dataset for ad hominem detection using Bert\n","* __File Management__: Using Google Drive\n","* __Runtime Type__: CPU"]},{"cell_type":"code","metadata":{"id":"9nglRegK8hi1"},"source":["!pip install langid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ewUBxwcpx2l"},"source":["import pandas as pd\n","import numpy as np\n","#import langid\n","from tqdm import tqdm\n","import pickle\n","#from langid.langid import LanguageIdentifier, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bvEWE5vnDfc"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqtlqmlKnhdA"},"source":["%cd /content/gdrive/MyDrive/DL/Facebook/dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W73Z5agnoG0x"},"source":["page_names_usa = ['barackobama', 'Breitbart', 'DonaldTrump', 'FoxNews', 'joebiden']\n","page_names_india = ['BJP4India', 'IndianNationalCongress', 'rahulgandhi', 'RepublicBharatHindi', 'narendramodi']\n","comments = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRo4Sp-Vu_uG"},"source":["def get_posts(country_name, page_name):\n","    df = pd.read_json(f'{country_name}/{page_name}/Page Posts.json')\n","    df = df[['Post Text', 'Post Link', 'Post Owner', 'Date Posted']]\n","    df = df.dropna(subset=['Post Text', 'Post Link'])\n","    texts = df['Post Text'].tolist()\n","    links = df['Post Link'].tolist()\n","    times = df['Date Posted'].tolist()\n","    incomments = []\n","    for i in range(len(df)):\n","        comment = dict()\n","        comment['text'] = texts[i]\n","        comment['link'] = links[i]\n","        comment['page'] = page_name\n","        comment['type'] = 'post'\n","        comment['username'] = page_name\n","        comment['time'] = times[i]\n","        incomments.append(comment)\n","    return incomments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtF_5VyLyCFC"},"source":["def get_comments(country_name, page_name):\n","    df = pd.read_json(f'{country_name}/{page_name}/Page Posts (Scraped User Comments).json')\n","    df = df[['Comment Text', 'Post Link', 'Replies', 'Username', 'Comment Time']]\n","    df = df.dropna(subset=['Comment Text', 'Post Link'])\n","    texts = df['Comment Text'].tolist()\n","    links = df['Post Link'].tolist()\n","    replies = df['Replies'].tolist()\n","    usrnames = df['Username'].tolist()\n","    times = df['Comment Time'].tolist()\n","    incomments = []\n","    for i in range(len(df)):\n","        comment = dict()\n","        comment['text'] = texts[i]\n","        comment['link'] = links[i]\n","        comment['page'] = page_name\n","        comment['type'] = 'comment'\n","        comment['username'] = usrnames[i]\n","        comment['time'] = times[i]\n","        incomments.append(comment)\n","        lst = replies[i]\n","        for j in range(len(lst)):\n","            r = dict()\n","            r['text'] = lst[j]['Comment Text']\n","            r['link'] = links[i]\n","            r['page'] = page_name\n","            r['type'] = 'reply'\n","            r['username'] = lst[j]['Username']\n","            r['time'] = lst[j]['Comment Time']\n","            incomments.append(r)\n","    return incomments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhZnTh-a4p4G"},"source":["# preparing US dataset\n","for page_name in page_names_usa:\n","    comments.extend(get_posts('USA', page_name))\n","    comments.extend(get_comments('USA', page_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuqYo0sQ6tE5"},"source":["len(comments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkN0FwQl61i7"},"source":["# preparing India dataset\n","for page_name in page_names_india:\n","    comments.extend(get_posts('India', page_name))\n","    comments.extend(get_comments('India', page_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVbYrN1n7cDB"},"source":["len(comments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ne_eWjlpIg0M"},"source":["comments[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jos8YQse7e4Z"},"source":["identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n","english_comments = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVh6sLJi9D_t"},"source":["for i in tqdm(range(len(comments))):\n","    x = comments[i]\n","    lang, score = identifier.classify(x['text'])\n","    if lang == 'en' and score >= 0.9:\n","        english_comments.append(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOiagqeX9015"},"source":["len(english_comments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBKl71v-_P6h"},"source":["with open('comments.pkl', 'wb') as f:\n","    pickle.dump(english_comments, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8WY1gke_mc0"},"source":["w_comments = pickle.load(open('comments.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXMNMKu8AXnl"},"source":["w_comments[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ykh9Y8Q5AaKS"},"source":["authors = set()\n","times = set()\n","cnt = 0\n","for x in w_comments:\n","    if x['page'] in page_names_usa:\n","        cnt += 1\n","        authors.add(x['username'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"204iLOygOGv9"},"source":["maxtime = 0\n","mintime = 20221212\n","for x in times:\n","    maxtime = max(maxtime, x)\n","    mintime = min(mintime, x)\n","print(mintime, maxtime)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJIHBmhUOP0r"},"source":["len(authors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeSR7uWTOqTv"},"source":["cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KO4mpUmv82_T"},"source":[""],"execution_count":null,"outputs":[]}]}