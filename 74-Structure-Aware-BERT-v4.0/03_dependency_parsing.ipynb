{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "In this notebook, we perform dependency parsing for the texts in the **LOGIC** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/utkarsh-am/.local/lib/python3.8/site-packages (3.4.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: setuptools in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/tools/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/tools/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install `spacy` package to perform dependency parsing\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from   matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from functools import lru_cache\n",
    "\n",
    "import spacy\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for plotting\n",
    "sns.set(style='darkgrid')\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # For lemmatizers\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Setup for spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "scapy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For caching objects\n",
    "\n",
    "def load_obj(file_path):\n",
    "    \"\"\"Load a pickled object from given path\n",
    "    :param file_path: Path to the pickle file of the object\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, file_path):\n",
    "    \"\"\"Save an object to given path via pickling\n",
    "    :param obj: Object to pickle\n",
    "    :param file_path: Path for pickling\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        return pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LOGIC dataset\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train.csv')\n",
    "dev_df   = pd.read_csv('./dataset/dev.csv')\n",
    "test_df  = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts        = list(train_df['text'])\n",
    "train_labels       = list(train_df['label'])\n",
    "train_masked_texts = list(train_df['masked_text'])\n",
    "\n",
    "dev_texts        = list(dev_df['text'])\n",
    "dev_labels       = list(dev_df['label'])\n",
    "dev_masked_texts = list(dev_df['masked_text'])\n",
    "\n",
    "test_texts        = list(test_df['text'])\n",
    "test_labels       = list(test_df['label'])\n",
    "test_masked_texts = list(test_df['masked_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distribution of sentence length\n",
    "\n",
    "text_length_dist = {'length': [], 'dataset': []}\n",
    "\n",
    "for text in train_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('train')\n",
    "\n",
    "for text in test_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('test')\n",
    "\n",
    "for text in dev_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('dev')\n",
    "\n",
    "ax = sns.barplot(data=text_length_dist, x='dataset', y='length')\n",
    "ax.set_xlabel('Partition')\n",
    "ax.set_ylabel('Word length')\n",
    "ax.set_title('Word length distribution across different partition')\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'#train: {len(train_df)}, #test: {len(test_df)}, #dev: {len(dev_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get empath dictionary\n",
    "\n",
    "!wget -O empath-dictionary.tsv https://drive.google.com/uc?id=1kH7_FVkIvF0NajMp8E09fssRDb4_h4j6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empath_dictionary():\n",
    "    \"\"\"\n",
    "    Returns a dict[str, list] object where keys are categories and values are \n",
    "    associated words for that category\n",
    "    \"\"\"\n",
    "    empath_dict = dict()\n",
    "    with open('./empath-dictionary.tsv', 'r') as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            name = cols[0]\n",
    "            terms = cols[1:]\n",
    "            empath_dict[name] = list()\n",
    "            for t in set(terms):\n",
    "                empath_dict[name].append(t)\n",
    "    return empath_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_dict = load_empath_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_count = list()\n",
    "for v in empath_dict.values():\n",
    "    tokens_count.append(len(v))\n",
    "print(f'Average token count {np.average(tokens_count)}, Std. dev {np.std(tokens_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Slur words dictionary \n",
    "\n",
    "!wget -O slur-word-dictionary.json https://drive.google.com/uc?id=1q_cEj_qlAEDnSpY5_dV-Bl3Nvm74bL9K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOTA slur word dictionary (from Punyajoy)\n",
    "with open('./slur-word-dictionary.json') as f:\n",
    "    slur_words_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hate categories\n",
    "\n",
    "!wget -O hate-categories.json https://drive.google.com/uc?id=1zsuchbPYSoTUfplkw7G6kVW8omLyf8tU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hate-targets broad categories\n",
    "# Paper: \"A Measurement Study of Hate Speech in Social Media\", Mainack Mondal\n",
    "with open('./hate-categories.json') as f:\n",
    "    hate_targets_dict = json.load(f)\n",
    "pprint(hate_targets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_targets_raw = dict()\n",
    "# key: hate_targets\n",
    "# value: list of raw tokens associated with that target\n",
    "\n",
    "for k, v in hate_targets_dict.items():\n",
    "    hate_targets_raw[k] = list()\n",
    "    for token_type in v:\n",
    "        if token_type in slur_words_dict:\n",
    "            hate_targets_raw[k].extend(slur_words_dict[token_type])\n",
    "        if token_type in empath_dict:\n",
    "            hate_targets_raw[k].extend(empath_dict[token_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "replace_underscores_with_whitespaces = lambda z: ' '.join(z.split('_'))\n",
    "\n",
    "hate_targets = dict()\n",
    "# key: hate_targets\n",
    "# value: list of processed tokens associated with that target\n",
    "\n",
    "for k, v in hate_targets_raw.items():\n",
    "    temp = list(map(lemmatizer.lemmatize, v))\n",
    "    hate_targets[k] = set(map(replace_underscores_with_whitespaces, temp))\n",
    "\n",
    "# pprint(hate_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependency_graph(doc):\n",
    "    \"\"\"Create dependency graph of tokens from scapy doc\n",
    "    :param doc: scapy doc instance\n",
    "    \"\"\"\n",
    "    dependency_edges = list() # (parent, child, relationship)\n",
    "    id_to_text = dict()\n",
    "    id_to_token = dict()\n",
    "    root = None\n",
    "    node_count = 0\n",
    "\n",
    "    for token in doc:\n",
    "        node_count += 1\n",
    "        parent = token.head.i\n",
    "        child = token.i\n",
    "        relationship = token.dep_\n",
    "        id_to_text[child] = lemmatizer.lemmatize(token.text)\n",
    "        id_to_token[child] = token\n",
    "        if relationship == 'ROOT':\n",
    "            root = child\n",
    "            continue\n",
    "        dependency_edges.append((parent, child, relationship))\n",
    "\n",
    "    dependency_graph = dict()\n",
    "    for i in range(node_count): \n",
    "        dependency_graph[i] = list()\n",
    "    for p, c, r in dependency_edges:\n",
    "        dependency_graph[p].append((c, r, 0))\n",
    "        dependency_graph[c].append((p, r, 1))\n",
    "    \n",
    "    return dependency_graph, id_to_text, id_to_token, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_ids(id_to_token):\n",
    "    \"\"\"Index generator for nouns / personal nouns / pronouns\"\"\"\n",
    "    for k, v in id_to_token.items():\n",
    "        if v.pos_ == \"NOUN\" or v.pos_ == \"PROPN\" or v.pos_==\"PRON\":\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_personal_pronoun_ids(id_to_token):\n",
    "    \"\"\"Index generator for personal pronouns\"\"\"\n",
    "    for k, v in id_to_token.items():\n",
    "        if v.tag_ == 'PRP': # Personal pronoun tag in scapy\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_ids(id_to_token):\n",
    "    \"\"\"Index generator for pronouns\"\"\"\n",
    "    for k, v in id_to_token.items():\n",
    "        if v.pos_ == 'PRON': # Pronoun tag in scapy\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger_ids(id_to_text, trigger_type):\n",
    "    \"\"\"Generates ids which are indices of triggers\n",
    "    :param id_to_text: id_to_text returned by create_dependency_graph\n",
    "    :type id_to_text: dict\n",
    "    :param trigger_type: What type of triggers?\n",
    "    :type trigger_type: str\n",
    "    \"\"\"\n",
    "    for k, v in id_to_text.items():\n",
    "        if v in hate_targets[trigger_type]:\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_first_search(dependency_graph, source):\n",
    "    \"\"\"Performs breadth first search\n",
    "    :param dependency_graph: Dependency graph returned by create_dependency_graph\n",
    "    :type dependency_graph: dict\n",
    "    :param source: Source node ID\n",
    "    :type source: int\n",
    "    \"\"\"\n",
    "    q = deque()\n",
    "    used = set()\n",
    "    d = dict() # distance of nodes from source\n",
    "    p = dict() # parent in bfs\n",
    "    r = dict() # relation observed\n",
    "\n",
    "    q.append(source)\n",
    "    used.add(source)\n",
    "    p[source] = -1\n",
    "    d[source] = 0\n",
    "\n",
    "    while len(q):\n",
    "        v = q.popleft()\n",
    "        for u, rel, orient in dependency_graph[v]:\n",
    "            if u in used:\n",
    "                continue\n",
    "            used.add(u)\n",
    "            q.append(u)\n",
    "            d[u] = d[v] + 1\n",
    "            p[u] = v\n",
    "            r[u] = (rel, orient)\n",
    "\n",
    "    return d, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path_from_bfs(source, dest, dist_dict, parent_dict, relation_dict):\n",
    "    \"\"\"Generate path from source to dest. Path will contain relationships \n",
    "    encountered in bfs.\n",
    "    \"\"\"\n",
    "    assert dist_dict[source] == 0\n",
    "    assert dest in dist_dict \n",
    "\n",
    "    indices_list = list()     # to store indices along the path\n",
    "\n",
    "    orientation_list = list() # store whether the edge is traversed\n",
    "                              # from parent to child\n",
    "                              # or child to parent\n",
    "    path = list()\n",
    "    cur = dest\n",
    "    while cur != source:\n",
    "        rel, orient = relation_dict[cur]\n",
    "        path.append(rel)\n",
    "        indices_list.append(cur)\n",
    "        orientation_list.append(orient)\n",
    "        cur = parent_dict[cur]\n",
    "    indices_list.append(cur)\n",
    "\n",
    "    return path, indices_list, orientation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing(texts, index_generator, n_process=8, batch_size=1000):\n",
    "    \"\"\"Perform dependency parsing\n",
    "\n",
    "    :param texts: list of comment body (text)\n",
    "    :param index_generator: `get_personal_pronoun_ids` or `get_pronoun_ids`\n",
    "    :param n_process: No. of processes spawned for processing, refer to pipe utility in spacy\n",
    "    :param batch_size: Batch size while processing, refer to pipe utility in spacy\n",
    "    \"\"\"\n",
    "\n",
    "    result = list()\n",
    "    # `result` will contain [comment_text, [(path, word_list, word_indices, orientation_list), ...]]\n",
    "\n",
    "    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n",
    "\n",
    "    for doc in tqdm(docs, total=len(texts)):\n",
    "        # Parse comment text and create dependency graph\n",
    "        local_result = list()\n",
    "        parsed_result = list()\n",
    "\n",
    "        local_result.append(doc.text)\n",
    "        dependency_graph, id_to_text, id_to_token, root \\\n",
    "                                             = create_dependency_graph(doc)\n",
    "\n",
    "        # Extract the indices using iterator\n",
    "        for index in index_generator(id_to_token):\n",
    "            dist, parent, relation = breadth_first_search(dependency_graph,\n",
    "                                                          index)\n",
    "            for target_id, target_dist in dist.items():\n",
    "                path, word_indices, orientation = \\\n",
    "                    generate_path_from_bfs(index, target_id, dist,\n",
    "                                            parent, relation)\n",
    "                if 'punct' in path:\n",
    "                    # Ignore dependency paths related to punctuation marks\n",
    "                    continue\n",
    "\n",
    "                words = list()\n",
    "                for word_index in word_indices:\n",
    "                    words.append(id_to_text[word_index])\n",
    "\n",
    "                parsed_result.append((tuple(path), tuple(words), tuple(word_indices), tuple(orientation)))\n",
    "\n",
    "        local_result.append(tuple(parsed_result))\n",
    "        result.append(tuple(local_result))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_lc = [s.lower() for s in train_texts]\n",
    "test_texts_lc  = [s.lower() for s in test_texts]\n",
    "dev_texts_lc   = [s.lower() for s in dev_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_parsed = dependency_parsing(train_texts_lc, get_noun_ids)\n",
    "test_texts_parsed  = dependency_parsing(test_texts_lc,  get_noun_ids)\n",
    "dev_texts_parsed   = dependency_parsing(dev_texts_lc,   get_noun_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_parsing_results = (train_texts_parsed, test_texts_parsed, dev_texts_parsed)\n",
    "\n",
    "save_obj(dependency_parsing_results, './dataset/dependency_parsing_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
