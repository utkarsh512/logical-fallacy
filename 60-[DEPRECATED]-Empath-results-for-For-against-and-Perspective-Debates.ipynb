{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOvWgkyhGhQzQTfI819u6D8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Empath results: For-against vs. Perspective Debates\n","In this notebook, we will use the dictionary used in Empath module and count the occurence of personal pronouns in the vicinity of the words present in the dictinary.\n","\n","For example, [*you*] **scum**.\n","\n","**Runtime Type**: CPU\n","\n","**Author**: Utkarsh Patel\n","\n","**Date**: 2022-12-27"],"metadata":{"id":"D4I2-BcBvP1a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui1p_WH7vAdd"},"outputs":[],"source":["# Mount Google drive to Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested"],"metadata":{"id":"Z3H7Q0H2wPlf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   collections              import namedtuple\n","from   copy                     import deepcopy\n","# import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","# from   transformers             import BertModel, BertTokenizer\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"Ue9JRwOKwdNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setup for Empath"],"metadata":{"id":"Slv0ULCd-he8"}},{"cell_type":"code","source":["# Download empath dictinary\n","# !curl https://raw.githubusercontent.com/Ejhfast/empath-client/master/empath/data/categories.tsv -o /content/gdrive/MyDrive/DL/empath/dictionary.tsv\n","\n","# NOTE: Dictionary already downloaded on first run!"],"metadata":{"id":"rgvK1jUQwqKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_empath_dictionary():\n","    \"\"\"\n","    Returns a dict[str, list] object where keys are categories and values are \n","    associated words for that category\n","    \"\"\"\n","    empath_dict = dict()\n","    with open('/content/gdrive/MyDrive/DL/empath/dictionary.tsv', 'r') as f:\n","        for line in f:\n","            cols = line.strip().split(\"\\t\")\n","            name = cols[0]\n","            terms = cols[1:]\n","            empath_dict[name] = list()\n","            for t in set(terms):\n","                empath_dict[name].append(t)\n","    return empath_dict"],"metadata":{"id":"Hr_I0bMIzpAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empath = load_empath_dictionary()"],"metadata":{"id":"U7tT1-yK035v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(empath.keys())"],"metadata":{"id":"a2DQtKrC1ALu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_count = list()\n","for v in empath.values():\n","    tokens_count.append(len(v))"],"metadata":{"id":"onml7dIY1OzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Average token count {np.average(tokens_count)}, Std. dev {np.std(tokens_count)}')"],"metadata":{"id":"gG07M9mP7mu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empath_selected_tokens = list()\n","\n","with open('/content/gdrive/MyDrive/DL/empath/empath_selected_categories.txt', 'r') as f:\n","    for line in f:\n","        empath_selected_tokens.append(line.strip())"],"metadata":{"id":"k9pU3Z6jeBgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUR_WORDS = {\n","  \"jews\": [\n","    \"jews\",\n","    \"oven dodger\",\n","    \"nazi\",\n","    \"dirty jew\",\n","    \"holocaust\",\n","    \"kikesucker\",\n","    \"hook nose\",\n","    \"kike\"\n","  ],\n","  \"homosexual\": [\n","    \"faggots usually\",\n","    \"fucking queer\",\n","    \"the biggest faggot\",\n","    \"dyke\",\n","    \"you fucking faggot\",\n","    \"hate faggots\",\n","    \"queer\",\n","    \"homosexual\",\n","    \"the faggots\",\n","    \"faggot\",\n","    \"faggots usually have\",\n","    \"gay\",\n","    \"faggots\",\n","    \"dykey\",\n","    \"ugly dyke\",\n","    \"faggots like you\",\n","    \"you a fag\",\n","    \"lesbian\",\n","    \"homo\",\n","    \"is a faggot\",\n","    \"like a faggot\",\n","    \"dykes\",\n","    \"faggots like\",\n","    \"faggot if you ever\"\n","  ],\n","  \"women\": [\n","    \"ugly dyke\",\n","    \"woman terrorist\",\n","    \"nigress\",\n","    \"bitch\",\n","    \"slut\",\n","    \"women\",\n","    \"sheeboon\",\n","    \"negress\",\n","    \"mud shark\",\n","    \"women threat\",\n","    \"you a lame bitch\",\n","    \"your a cunt\",\n","    \"white bitch\",\n","    \"niggeress\",\n","    \"hoe\",\n","    \"dykes\",\n","    \"niggress\",\n","    \"sheboon\",\n","    \"feminazi\"\n","  ],\n","  \"blacks\": [\n","    \"pavement ape\",\n","    \"the niggers\",\n","    \"negress\",\n","    \"porch monkey\",\n","    \"that nigger\",\n","    \"this nigger\",\n","    \"sheboon\",\n","    \"all niggers\",\n","    \"eurafrica\",\n","    \"shut up nigger\",\n","    \"picaninny\",\n","    \"african attack\",\n","    \"spearchucker\",\n","    \"how many niggers\",\n","    \"nigger\",\n","    \"africa\",\n","    \"niggers are in my\",\n","    \"dindu nuffin\",\n","    \"stupid nigger\",\n","    \"moolie\",\n","    \"niggers\",\n","    \"bluegum\",\n","    \"nigger ass\",\n","    \"you niggers\",\n","    \"fucking nigger\",\n","    \"nigger music\",\n","    \"niggress\",\n","    \"you a nigger\",\n","    \"many niggers are\",\n","    \"nigress\",\n","    \"blacks\",\n","    \"teenaper\",\n","    \"sheeboon\",\n","    \"dumb nigger\",\n","    \"niggeress\",\n","    \"pickaninny\",\n","    \"nigga\"\n","  ],\n","  \"muslim\": [\n","    \"muslim immigrant\",\n","    \"islam\",\n","    \"mudslime\",\n","    \"mooslem\",\n","    \"muslim refugee\",\n","    \"musslime\",\n","    \"shitlam\",\n","    \"muslim invasion\",\n","    \"moslime\",\n","    \"mooslamic\",\n","    \"muzzie\",\n","    \"allah akbar\",\n","    \"mooslime\",\n","    \"musloid\",\n","    \"mudslimes\",\n","    \"muslim\",\n","    \"muslimes\",\n","    \"moslum\",\n","    \"mussie\",\n","    \"muzrat\",\n","    \"muslim countries\",\n","    \"muzzy\",\n","    \"moslim\",\n","    \"jihadi\",\n","    \"muslim country\",\n","    \"moslem\",\n","    \"muzzrat\",\n","    \"mooslim\"\n","  ],\n","  \"arabs\": [\n","    \"towel head\",\n","    \"goatfucker\",\n","    \"arabs\",\n","    \"goathumper\",\n","    \"raghead\",\n","    \"rag head\",\n","    \"goathumping\",\n","    \"towelhead\",\n","    \"camel jockey\",\n","    \"sandnigger\",\n","    \"camel fucker\",\n","    \"sand nigger\"\n","  ],\n","  \"generic\": [\n","    \"to rape\",\n","    \"raped and\",\n","    \"shithole country\",\n","    \"get raped\",\n","    \"raped\",\n","    \"is a fucking\",\n","    \"shit skin\",\n","    \"raped by\",\n","    \"hate you\",\n","    \"fake empowerment\",\n","    \"abusive women\",\n","    \"fuck you too\",\n","    \"violence\",\n","    \"wit a lame nigga\",\n","    \"they all look\",\n","    \"alllivesmatter\",\n","    \"shithole countries\",\n","    \"fucking hate\",\n","    \"trailer trash\",\n","    \"kill all\",\n","    \"terrorist threat\",\n","    \"harassment\",\n","    \"kill yourself\",\n","    \"shitskin\",\n","    \"okay to be white\",\n","    \"fucking hate you\"\n","  ],\n","  \"white\": [\n","    \"full of white\",\n","    \"white trash\",\n","    \"white devil\",\n","    \"white\",\n","    \"are all white\",\n","    \"white boy\",\n","    \"white ass\",\n","    \"white bitch\",\n","    \"hillbilly\",\n","    \"whigger\",\n","    \"white christian\",\n","    \"white person\",\n","    \"all white\",\n","    \"white nigger\",\n","    \"redneck\",\n","    \"white honky\",\n","    \"wigger\",\n","    \"them white\"\n","  ],\n","  \"economy\": [\n","    \"ghetto\"\n","  ],\n","  \"immigrant\": [\n","    \"illegal immigrants\",\n","    \"immigrant not welcome\",\n","    \"immigrant terror\",\n","    \"mexcrement\",\n","    \"go back to where you come from\",\n","    \"muslim refugee\",\n","    \"illegal aliens\",\n","    \"refugee\",\n","    \"protect from immigrants\",\n","    \"negro\",\n","    \"refugees\",\n","    \"immigrant\",\n","    \"refugee invasion\",\n","    \"go back to where they come from\",\n","    \"refugees impact\",\n","    \"bring ebola\",\n","    \"immigrants\",\n","    \"illegal alien\",\n","    \"immigrant invasion\",\n","    \"bring disease\"\n","  ],\n","  \"mental\": [\n","    \"retard\",\n","    \"mongoloid\",\n","    \"retarded\"\n","  ],\n","  \"asians\": [\n","    \"asians\",\n","    \"ching chong\",\n","    \"chinaman\"\n","  ]\n","}"],"metadata":{"id":"K-MCiro4fbtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOKEN_GROUP = dict()\n","\n","TOKEN_GROUP.update(SLUR_WORDS)\n","\n","for key in empath_selected_tokens:\n","    TOKEN_GROUP[key] = list()\n","    for token in empath[key]:\n","        TOKEN_GROUP[key].append(' '.join(token.split('_')))"],"metadata":{"id":"08QOJY_bfd4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOKEN_GROUP.keys()"],"metadata":{"id":"6HzrRedKgFN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["personal_pronouns = ('i', 'you', 'she', 'he', 'it', 'they', 'we', 'you', 'they', # subject pronouns\n","                     'me', 'her', 'him', 'them', 'us')                           # object pronouns"],"metadata":{"id":"pIEbSJ1U8VID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setup for For-against and Perspective debates"],"metadata":{"id":"GdOc4pcI-l0r"}},{"cell_type":"code","source":["# Custom routine to clean texts scraped from Web.\n","# It removes hyperlinks, punctuation marks (except apostrophe)\n","\n","tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"lvR_RBE0-VLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"zS-k4Fb0-2W1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"M0mtBe0f-4TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_score_comments = dict()\n","\n","for cat in categories_selected:\n","    for comment in comments[cat]:\n","        ah_score_comments[comment['id']] = 1 - comment['score']"],"metadata":{"id":"HKkfc4Pw-7Kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_tstring(tstring):\n","    \"\"\"\n","    Parses comment's time to an integer to enable\n","    comparison between comments based on their time of posting\n","    \"\"\"\n","    if tstring == 'Not Available':\n","        raise ValueError('Invalid posting time for parse_tstring')\n","    tstring = tstring.replace('T', '-').replace(':', '-').replace('+', '-').split('-')\n","    return int(''.join(tstring[:-2]))"],"metadata":{"id":"OJrKZAGv_EZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_debates = dict()\n","perspective_debates = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates[cat] = list()\n","    perspective_debates[cat] = list()\n","\n","    for comment in comments[cat]:\n","        if comment['polarity'] == 'Not Available':\n","            perspective_debates[cat].append(deepcopy(comment))\n","        else:\n","            for_against_debates[cat].append(deepcopy(comment))"],"metadata":{"id":"46mpoXKV_OwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","for cat in categories_selected:\n","    print(len(for_against_debates[cat]), len(perspective_debates[cat]))"],"metadata":{"id":"oPKiGoqFBf9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_debates_cleaned = dict()\n","perspective_debates_cleaned = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates_cleaned[cat] = list()\n","    perspective_debates_cleaned[cat] = list()\n","\n","    for comment in tqdm(for_against_debates[cat]):\n","        for_against_debates_cleaned[cat].append(dict(cid=comment['id'], body=' '.join(clean_text(comment['body']))))\n","    \n","    for comment in tqdm(perspective_debates[cat]):\n","        perspective_debates_cleaned[cat].append(dict(cid=comment['id'], body=' '.join(clean_text(comment['body']))))"],"metadata":{"id":"eShx3F-CBxsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_tokens(comment, token_group):\n","    \"\"\"\n","    Returns count of personal-pronoun-preceded-succeded words under `token_group`\n","    \"\"\"\n","    token_count = 0\n","    for token in TOKEN_GROUP[token_group]:\n","        for pronoun in personal_pronouns:\n","            pre = f'{token} {pronoun}'\n","            post = f'{pronoun} {token}'\n","\n","            token_count += comment['body'].count(pre)\n","            token_count += comment['body'].count(post)\n","    return token_count"],"metadata":{"id":"7HQXRDqELUtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_empath_token_count = dict()\n","perspective_empath_token_count = dict()\n","\n","for cat in ['politics2']:\n","    for_against_empath_token_count[cat] = dict()\n","    perspective_empath_token_count[cat] = dict()\n","\n","    for token_group in tqdm(TOKEN_GROUP.keys()):\n","        for_against_empath_token_count[cat][token_group] = list()\n","        perspective_empath_token_count[cat][token_group] = list()\n","\n","        for comment in (for_against_debates_cleaned[cat]):\n","            for_against_empath_token_count[cat][token_group].append(count_tokens(comment, token_group))\n","        \n","        for comment in (perspective_debates_cleaned[cat]):\n","            perspective_empath_token_count[cat][token_group].append(count_tokens(comment, token_group))"],"metadata":{"id":"k1BnPQVdE1yR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with open('/content/gdrive/MyDrive/Temp/nb-60-for_against_empath_token_count.pkl', 'wb') as f:\n","#     pickle.dump(for_against_empath_token_count, f)\n","\n","# with open('/content/gdrive/MyDrive/Temp/nb-60-perspective_empath_token_count.pkl', 'wb') as f:\n","#     pickle.dump(perspective_empath_token_count, f)"],"metadata":{"id":"6sVYGqYiOYG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/nb-60-for_against_empath_token_count.pkl', 'rb') as f:\n","    for_against_empath_token_count = pickle.load(f)\n","\n","with open('/content/gdrive/MyDrive/Temp/nb-60-perspective_empath_token_count.pkl', 'rb') as f:\n","    perspective_empath_token_count = pickle.load(f)"],"metadata":{"id":"K14YyBm3bC-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in ['religion']:\n","    for_against_empath_token_count[cat] = dict()\n","    perspective_empath_token_count[cat] = dict()\n","\n","    for token_group in tqdm(TOKEN_GROUP.keys()):\n","        for_against_empath_token_count[cat][token_group] = list()\n","        perspective_empath_token_count[cat][token_group] = list()\n","\n","        for comment in (for_against_debates_cleaned[cat]):\n","            for_against_empath_token_count[cat][token_group].append(count_tokens(comment, token_group))\n","        \n","        for comment in (perspective_debates_cleaned[cat]):\n","            perspective_empath_token_count[cat][token_group].append(count_tokens(comment, token_group))"],"metadata":{"id":"Yspvsj8J9F-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(TOKEN_GROUP)"],"metadata":{"id":"k6lcHGLBcO3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Plot(token_groups):\n","    \"\"\"Comparing two slur probabilities by plotting them\n","    \"\"\"\n","    x = token_groups\n","    y1 = [] \n","    y2 = [] \n","\n","    for token_group in token_groups:\n","        y1.append(np.average(for_against_empath_token_count['politics2'][token_group]))\n","        y2.append(np.average(perspective_empath_token_count['politics2'][token_group]))\n","\n","    ticks = np.arange(len(x))\n","    width = 0.30\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width / 2, y1, width, label='for-against', tick_label=x)\n","    subplot2 = ax.bar(ticks + width / 2, y2, width, label='perspective', tick_label=x)\n","\n","    ax.set_ylabel('Word counts (averaged)')\n","    ax.set_title(f'Word counts (preceded/succeded by personal pronouns)')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"ABq9_JntcVh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lkeys = list(TOKEN_GROUP.keys())"],"metadata":{"id":"5jIA9XOAeZG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 0"],"metadata":{"id":"mPwwsxgQgSjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Plot(lkeys[50:])\n","index += 10"],"metadata":{"id":"Uvf8giWneemS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in for_against_empath_token_count['politics2'].items():\n","    print(f'{k} - {np.average(v)}')"],"metadata":{"id":"qZrFPlSx9bdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nnCJzJGIbfgL"},"execution_count":null,"outputs":[]}]}