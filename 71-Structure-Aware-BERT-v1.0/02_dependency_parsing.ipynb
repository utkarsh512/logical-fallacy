{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "In this notebook, we perform dependency parsing for the texts in the **LOGIC** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/utkarsh-am/.local/lib/python3.8/site-packages (3.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: jinja2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: six in /opt/tools/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/tools/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/tools/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install `spacy` package to perform dependency parsing\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from   matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from functools import lru_cache\n",
    "\n",
    "import spacy\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for plotting\n",
    "sns.set(style='darkgrid')\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/utkarsh-am/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/utkarsh-\n",
      "[nltk_data]     am/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/utkarsh-\n",
      "[nltk_data]     am/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/utkarsh-\n",
      "[nltk_data]     am/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-04 02:32:40.924197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 02:32:41.777451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-04 02:32:41.777547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-04 02:32:41.777574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-04 02:32:42.530463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-04 02:32:42.532125: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: jinja2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.47.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (20.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.24.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/tools/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/tools/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/tools/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Setup for nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # For lemmatizers\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Setup for spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "scapy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For caching objects\n",
    "\n",
    "def load_obj(file_path):\n",
    "    \"\"\"Load a pickled object from given path\n",
    "    :param file_path: Path to the pickle file of the object\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, file_path):\n",
    "    \"\"\"Save an object to given path via pickling\n",
    "    :param obj: Object to pickle\n",
    "    :param file_path: Path for pickling\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        return pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LOGIC dataset\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train.csv')\n",
    "dev_df   = pd.read_csv('./dataset/dev.csv')\n",
    "test_df  = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts        = list(train_df['text'])\n",
    "train_labels       = list(train_df['label'])\n",
    "train_masked_texts = list(train_df['masked_text'])\n",
    "\n",
    "dev_texts        = list(dev_df['text'])\n",
    "dev_labels       = list(dev_df['label'])\n",
    "dev_masked_texts = list(dev_df['masked_text'])\n",
    "\n",
    "test_texts        = list(test_df['text'])\n",
    "test_labels       = list(test_df['label'])\n",
    "test_masked_texts = list(test_df['masked_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the distribution of sentence length\n",
    "\n",
    "text_length_dist = {'length': [], 'dataset': []}\n",
    "\n",
    "for text in train_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('train')\n",
    "\n",
    "for text in test_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('test')\n",
    "\n",
    "for text in dev_texts:\n",
    "    text_length_dist['length'].append(len(text.strip().split()))\n",
    "    text_length_dist['dataset'].append('dev')\n",
    "\n",
    "ax = sns.barplot(data=text_length_dist, x='dataset', y='length')\n",
    "ax.set_xlabel('Partition')\n",
    "ax.set_ylabel('Word length')\n",
    "ax.set_title('Word length distribution across different partition')\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 1849, #test: 300, #dev: 300\n"
     ]
    }
   ],
   "source": [
    "print(f'#train: {len(train_df)}, #test: {len(test_df)}, #dev: {len(dev_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-04 02:33:04--  https://drive.google.com/uc?id=1kH7_FVkIvF0NajMp8E09fssRDb4_h4j6\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.183.14, 2404:6800:4009:820::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.183.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0g-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v4htvgj0nlgflojdietgmdfn93n7ah19/1680555750000/14292413845157007490/*/1kH7_FVkIvF0NajMp8E09fssRDb4_h4j6?uuid=0861fde8-2415-4e81-a256-d7f784573706 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2023-04-04 02:33:04--  https://doc-0g-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v4htvgj0nlgflojdietgmdfn93n7ah19/1680555750000/14292413845157007490/*/1kH7_FVkIvF0NajMp8E09fssRDb4_h4j6?uuid=0861fde8-2415-4e81-a256-d7f784573706\n",
      "Resolving doc-0g-7o-docs.googleusercontent.com (doc-0g-7o-docs.googleusercontent.com)... 142.250.199.129, 2404:6800:4009:82d::2001\n",
      "Connecting to doc-0g-7o-docs.googleusercontent.com (doc-0g-7o-docs.googleusercontent.com)|142.250.199.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 135655 (132K) [text/tab-separated-values]\n",
      "Saving to: ‘empath-dictionary.tsv’\n",
      "\n",
      "empath-dictionary.t 100%[===================>] 132.48K   594KB/s    in 0.2s    \n",
      "\n",
      "2023-04-04 02:33:05 (594 KB/s) - ‘empath-dictionary.tsv’ saved [135655/135655]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get empath dictionary\n",
    "\n",
    "!wget -O empath-dictionary.tsv https://drive.google.com/uc?id=1kH7_FVkIvF0NajMp8E09fssRDb4_h4j6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empath_dictionary():\n",
    "    \"\"\"\n",
    "    Returns a dict[str, list] object where keys are categories and values are \n",
    "    associated words for that category\n",
    "    \"\"\"\n",
    "    empath_dict = dict()\n",
    "    with open('./empath-dictionary.tsv', 'r') as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            name = cols[0]\n",
    "            terms = cols[1:]\n",
    "            empath_dict[name] = list()\n",
    "            for t in set(terms):\n",
    "                empath_dict[name].append(t)\n",
    "    return empath_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_dict = load_empath_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token count 83.29381443298969, Std. dev 28.771070501829353\n"
     ]
    }
   ],
   "source": [
    "tokens_count = list()\n",
    "for v in empath_dict.values():\n",
    "    tokens_count.append(len(v))\n",
    "print(f'Average token count {np.average(tokens_count)}, Std. dev {np.std(tokens_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-04 02:33:14--  https://drive.google.com/uc?id=1q_cEj_qlAEDnSpY5_dV-Bl3Nvm74bL9K\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.183.14, 2404:6800:4009:820::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.183.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-10-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3mb3vdm6amiqf8rgf2d1k7851m3ut11h/1680555750000/14292413845157007490/*/1q_cEj_qlAEDnSpY5_dV-Bl3Nvm74bL9K?uuid=e15d358b-aa2d-4ff2-ad5b-396f53ba7302 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2023-04-04 02:33:14--  https://doc-10-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3mb3vdm6amiqf8rgf2d1k7851m3ut11h/1680555750000/14292413845157007490/*/1q_cEj_qlAEDnSpY5_dV-Bl3Nvm74bL9K?uuid=e15d358b-aa2d-4ff2-ad5b-396f53ba7302\n",
      "Resolving doc-10-7o-docs.googleusercontent.com (doc-10-7o-docs.googleusercontent.com)... 142.250.199.129, 2404:6800:4009:82d::2001\n",
      "Connecting to doc-10-7o-docs.googleusercontent.com (doc-10-7o-docs.googleusercontent.com)|142.250.199.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3853 (3.8K) [application/json]\n",
      "Saving to: ‘slur-word-dictionary.json’\n",
      "\n",
      "slur-word-dictionar 100%[===================>]   3.76K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2023-04-04 02:33:16 (414 KB/s) - ‘slur-word-dictionary.json’ saved [3853/3853]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Slur words dictionary \n",
    "\n",
    "!wget -O slur-word-dictionary.json https://drive.google.com/uc?id=1q_cEj_qlAEDnSpY5_dV-Bl3Nvm74bL9K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOTA slur word dictionary (from Punyajoy)\n",
    "with open('./slur-word-dictionary.json') as f:\n",
    "    slur_words_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-04 02:33:19--  https://drive.google.com/uc?id=1zsuchbPYSoTUfplkw7G6kVW8omLyf8tU\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.183.14, 2404:6800:4009:820::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.183.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0o-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pcdf93courvek94c5ruub2o70b7rq1e3/1680555750000/14292413845157007490/*/1zsuchbPYSoTUfplkw7G6kVW8omLyf8tU?uuid=70d48398-a525-43a9-bdf8-24381232ee5c [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2023-04-04 02:33:20--  https://doc-0o-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pcdf93courvek94c5ruub2o70b7rq1e3/1680555750000/14292413845157007490/*/1zsuchbPYSoTUfplkw7G6kVW8omLyf8tU?uuid=70d48398-a525-43a9-bdf8-24381232ee5c\n",
      "Resolving doc-0o-7o-docs.googleusercontent.com (doc-0o-7o-docs.googleusercontent.com)... 142.250.199.129, 2404:6800:4009:82d::2001\n",
      "Connecting to doc-0o-7o-docs.googleusercontent.com (doc-0o-7o-docs.googleusercontent.com)|142.250.199.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 804 [application/json]\n",
      "Saving to: ‘hate-categories.json’\n",
      "\n",
      "hate-categories.jso 100%[===================>]     804  --.-KB/s    in 0.001s  \n",
      "\n",
      "2023-04-04 02:33:20 (1.46 MB/s) - ‘hate-categories.json’ saved [804/804]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get hate categories\n",
    "\n",
    "!wget -O hate-categories.json https://drive.google.com/uc?id=1zsuchbPYSoTUfplkw7G6kVW8omLyf8tU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Behavior': ['negative_emotion',\n",
      "              'timidity',\n",
      "              'disappointment',\n",
      "              'animal',\n",
      "              'smell',\n",
      "              'anger',\n",
      "              'torment',\n",
      "              'shame',\n",
      "              'lust',\n",
      "              'sadness',\n",
      "              'rage',\n",
      "              'dominant_personality',\n",
      "              'violence',\n",
      "              'childish',\n",
      "              'pet',\n",
      "              'irritability',\n",
      "              'fear',\n",
      "              'sexual',\n",
      "              'ridicule',\n",
      "              'wealthy',\n",
      "              'weakness',\n",
      "              'nervousness',\n",
      "              'envy',\n",
      "              'aggression',\n",
      "              'hate'],\n",
      " 'Class': ['economy', 'poor', 'stealing'],\n",
      " 'Crime': ['prison', 'crime', 'terrorism'],\n",
      " 'Disablity': ['mental'],\n",
      " 'Ethnicity': ['immigrant', 'arabs', 'asians'],\n",
      " 'Gender': ['women', 'feminine'],\n",
      " 'Physical': ['monster',\n",
      "              'ugliness',\n",
      "              'youth',\n",
      "              'appearance',\n",
      "              'disgust',\n",
      "              'hygiene',\n",
      "              'medical_emergency',\n",
      "              'health'],\n",
      " 'Politics': ['politics', 'war', 'government'],\n",
      " 'Race': ['blacks', 'white'],\n",
      " 'Religion': ['jews', 'muslim'],\n",
      " 'Sexual orientation': ['homosexual']}\n"
     ]
    }
   ],
   "source": [
    "# Hate-targets broad categories\n",
    "# Paper: \"A Measurement Study of Hate Speech in Social Media\", Mainack Mondal\n",
    "with open('./hate-categories.json') as f:\n",
    "    hate_targets_dict = json.load(f)\n",
    "pprint(hate_targets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_targets_raw = dict()\n",
    "# key: hate_targets\n",
    "# value: list of raw tokens associated with that target\n",
    "\n",
    "for k, v in hate_targets_dict.items():\n",
    "    hate_targets_raw[k] = list()\n",
    "    for token_type in v:\n",
    "        if token_type in slur_words_dict:\n",
    "            hate_targets_raw[k].extend(slur_words_dict[token_type])\n",
    "        if token_type in empath_dict:\n",
    "            hate_targets_raw[k].extend(empath_dict[token_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "replace_underscores_with_whitespaces = lambda z: ' '.join(z.split('_'))\n",
    "\n",
    "hate_targets = dict()\n",
    "# key: hate_targets\n",
    "# value: list of processed tokens associated with that target\n",
    "\n",
    "for k, v in hate_targets_raw.items():\n",
    "    temp = list(map(lemmatizer.lemmatize, v))\n",
    "    hate_targets[k] = set(map(replace_underscores_with_whitespaces, temp))\n",
    "\n",
    "# pprint(hate_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependency_graph(doc):\n",
    "    \"\"\"Create dependency graph of tokens from scapy doc\n",
    "    :param doc: scapy doc instance\n",
    "    \"\"\"\n",
    "    dependency_edges = list() # (parent, child, relationship)\n",
    "    id_to_text = dict()\n",
    "    id_to_token = dict()\n",
    "    root = None\n",
    "    node_count = 0\n",
    "\n",
    "    for token in doc:\n",
    "        node_count += 1\n",
    "        parent = token.head.i\n",
    "        child = token.i\n",
    "        relationship = token.dep_\n",
    "        id_to_text[child] = lemmatizer.lemmatize(token.text)\n",
    "        id_to_token[child] = token\n",
    "        if relationship == 'ROOT':\n",
    "            root = child\n",
    "            continue\n",
    "        dependency_edges.append((parent, child, relationship))\n",
    "\n",
    "    dependency_graph = dict()\n",
    "    for i in range(node_count): \n",
    "        dependency_graph[i] = list()\n",
    "    for p, c, r in dependency_edges:\n",
    "        dependency_graph[p].append((c, r, 0))\n",
    "        dependency_graph[c].append((p, r, 1))\n",
    "    \n",
    "    return dependency_graph, id_to_text, id_to_token, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_personal_pronoun_ids(id_to_token):\n",
    "    \"\"\"Index generator for personal pronouns\"\"\"\n",
    "    for k, v in id_to_token.items():\n",
    "        if v.tag_ == 'PRP': # Personal pronoun tag in scapy\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_ids(id_to_token):\n",
    "    \"\"\"Index generator for pronouns\"\"\"\n",
    "    for k, v in id_to_token.items():\n",
    "        if v.pos_ == 'PRON': # Pronoun tag in scapy\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger_ids(id_to_text, trigger_type):\n",
    "    \"\"\"Generates ids which are indices of triggers\n",
    "    :param id_to_text: id_to_text returned by create_dependency_graph\n",
    "    :type id_to_text: dict\n",
    "    :param trigger_type: What type of triggers?\n",
    "    :type trigger_type: str\n",
    "    \"\"\"\n",
    "    for k, v in id_to_text.items():\n",
    "        if v in hate_targets[trigger_type]:\n",
    "            yield k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_first_search(dependency_graph, source):\n",
    "    \"\"\"Performs breadth first search\n",
    "    :param dependency_graph: Dependency graph returned by create_dependency_graph\n",
    "    :type dependency_graph: dict\n",
    "    :param source: Source node ID\n",
    "    :type source: int\n",
    "    \"\"\"\n",
    "    q = deque()\n",
    "    used = set()\n",
    "    d = dict() # distance of nodes from source\n",
    "    p = dict() # parent in bfs\n",
    "    r = dict() # relation observed\n",
    "\n",
    "    q.append(source)\n",
    "    used.add(source)\n",
    "    p[source] = -1\n",
    "    d[source] = 0\n",
    "\n",
    "    while len(q):\n",
    "        v = q.popleft()\n",
    "        for u, rel, orient in dependency_graph[v]:\n",
    "            if u in used:\n",
    "                continue\n",
    "            used.add(u)\n",
    "            q.append(u)\n",
    "            d[u] = d[v] + 1\n",
    "            p[u] = v\n",
    "            r[u] = (rel, orient)\n",
    "\n",
    "    return d, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path_from_bfs(source, dest, dist_dict, parent_dict, relation_dict):\n",
    "    \"\"\"Generate path from source to dest. Path will contain relationships \n",
    "    encountered in bfs.\n",
    "    \"\"\"\n",
    "    assert dist_dict[source] == 0\n",
    "    assert dest in dist_dict \n",
    "\n",
    "    indices_list = list()     # to store indices along the path\n",
    "\n",
    "    orientation_list = list() # store whether the edge is traversed\n",
    "                              # from parent to child\n",
    "                              # or child to parent\n",
    "    path = list()\n",
    "    cur = dest\n",
    "    while cur != source:\n",
    "        rel, orient = relation_dict[cur]\n",
    "        path.append(rel)\n",
    "        indices_list.append(cur)\n",
    "        orientation_list.append(orient)\n",
    "        cur = parent_dict[cur]\n",
    "    indices_list.append(cur)\n",
    "\n",
    "    return path, indices_list, orientation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing(texts, index_generator, n_process=8, batch_size=1000):\n",
    "    \"\"\"Perform dependency parsing\n",
    "\n",
    "    :param texts: list of comment body (text)\n",
    "    :param index_generator: `get_personal_pronoun_ids` or `get_pronoun_ids`\n",
    "    :param n_process: No. of processes spawned for processing, refer to pipe utility in spacy\n",
    "    :param batch_size: Batch size while processing, refer to pipe utility in spacy\n",
    "    \"\"\"\n",
    "\n",
    "    result = list()\n",
    "    # `result` will contain [comment_text, [(path, word_list, word_indices, orientation_list), ...]]\n",
    "\n",
    "    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n",
    "\n",
    "    for doc in tqdm(docs, total=len(texts)):\n",
    "        # Parse comment text and create dependency graph\n",
    "        local_result = list()\n",
    "        parsed_result = list()\n",
    "\n",
    "        local_result.append(doc.text)\n",
    "        dependency_graph, id_to_text, id_to_token, root \\\n",
    "                                             = create_dependency_graph(doc)\n",
    "\n",
    "        # Extract the indices using iterator\n",
    "        for index in index_generator(id_to_token):\n",
    "            dist, parent, relation = breadth_first_search(dependency_graph,\n",
    "                                                          index)\n",
    "            for trigger_type in hate_targets.keys():\n",
    "                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n",
    "                    if trigger_id in dist:\n",
    "                        path, word_indices, orientation = \\\n",
    "                            generate_path_from_bfs(index, trigger_id, dist,\n",
    "                                                   parent, relation)\n",
    "                        words = list()\n",
    "                        for word_index in word_indices:\n",
    "                            words.append(id_to_text[word_index])\n",
    "\n",
    "                        parsed_result.append((tuple(path), tuple(words), tuple(word_indices), tuple(orientation)))\n",
    "\n",
    "        local_result.append(tuple(parsed_result))\n",
    "        result.append(tuple(local_result))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_lc = [s.lower() for s in train_texts]\n",
    "test_texts_lc  = [s.lower() for s in test_texts]\n",
    "dev_texts_lc   = [s.lower() for s in dev_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:04<00:00, 443.50it/s]\n",
      "100%|██████████| 300/300 [00:01<00:00, 251.09it/s]\n",
      "100%|██████████| 300/300 [00:01<00:00, 257.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_texts_parsed = dependency_parsing(train_texts_lc, get_personal_pronoun_ids)\n",
    "test_texts_parsed  = dependency_parsing(test_texts_lc,  get_personal_pronoun_ids)\n",
    "dev_texts_parsed   = dependency_parsing(dev_texts_lc,   get_personal_pronoun_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_parsing_results = (train_texts_parsed, test_texts_parsed, dev_texts_parsed)\n",
    "\n",
    "save_obj(dependency_parsing_results, './dataset/dependency_parsing_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough work\n",
    "\n",
    "sent = \"senator randall isn't lying when she says she cares about her constituents—she wouldn't lie to people she cares about.\"\n",
    "doc = scapy_nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senator\n",
      "randall\n",
      "is\n",
      "n't\n",
      "lying\n",
      "when\n",
      "she\n",
      "says\n",
      "she\n",
      "cares\n",
      "about\n",
      "her\n",
      "constituents\n",
      "—\n",
      "she\n",
      "would\n",
      "n't\n",
      "lie\n",
      "to\n",
      "people\n",
      "she\n",
      "cares\n",
      "about\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"senator randall isn't lying when she says she cares about her \"\n",
      " \"constituents—she wouldn't lie to people she cares about.\",\n",
      " ((('ccomp', 'nsubj'), ('care', 'say', 'she'), (9, 7, 6), (0, 1)),\n",
      "  (('ccomp', 'advcl', 'nsubj'),\n",
      "   ('lie', 'lying', 'say', 'she'),\n",
      "   (17, 4, 7, 6),\n",
      "   (1, 1, 1)),\n",
      "  (('relcl', 'pobj', 'prep', 'ccomp', 'advcl', 'nsubj'),\n",
      "   ('care', 'people', 'to', 'lie', 'lying', 'say', 'she'),\n",
      "   (21, 19, 18, 17, 4, 7, 6),\n",
      "   (0, 0, 0, 1, 1, 1)),\n",
      "  (('compound', 'nsubj', 'advcl', 'nsubj'),\n",
      "   ('senator', 'randall', 'lying', 'say', 'she'),\n",
      "   (0, 1, 4, 7, 6),\n",
      "   (0, 0, 1, 1)),\n",
      "  (('nsubj',), ('care', 'she'), (9, 8), (1,)),\n",
      "  (('ccomp', 'advcl', 'ccomp', 'nsubj'),\n",
      "   ('lie', 'lying', 'say', 'care', 'she'),\n",
      "   (17, 4, 7, 9, 8),\n",
      "   (1, 1, 1, 1)),\n",
      "  (('relcl', 'pobj', 'prep', 'ccomp', 'advcl', 'ccomp', 'nsubj'),\n",
      "   ('care', 'people', 'to', 'lie', 'lying', 'say', 'care', 'she'),\n",
      "   (21, 19, 18, 17, 4, 7, 9, 8),\n",
      "   (0, 0, 0, 1, 1, 1, 1)),\n",
      "  (('compound', 'nsubj', 'advcl', 'ccomp', 'nsubj'),\n",
      "   ('senator', 'randall', 'lying', 'say', 'care', 'she'),\n",
      "   (0, 1, 4, 7, 9, 8),\n",
      "   (0, 0, 1, 1, 1)),\n",
      "  (('ccomp', 'advcl', 'ccomp', 'nsubj'),\n",
      "   ('care', 'say', 'lying', 'lie', 'she'),\n",
      "   (9, 7, 4, 17, 14),\n",
      "   (0, 0, 0, 1)),\n",
      "  (('nsubj',), ('lie', 'she'), (17, 14), (1,)),\n",
      "  (('relcl', 'pobj', 'prep', 'nsubj'),\n",
      "   ('care', 'people', 'to', 'lie', 'she'),\n",
      "   (21, 19, 18, 17, 14),\n",
      "   (0, 0, 0, 1)),\n",
      "  (('compound', 'nsubj', 'ccomp', 'nsubj'),\n",
      "   ('senator', 'randall', 'lying', 'lie', 'she'),\n",
      "   (0, 1, 4, 17, 14),\n",
      "   (0, 0, 0, 1)),\n",
      "  (('ccomp', 'advcl', 'ccomp', 'prep', 'pobj', 'relcl', 'nsubj'),\n",
      "   ('care', 'say', 'lying', 'lie', 'to', 'people', 'care', 'she'),\n",
      "   (9, 7, 4, 17, 18, 19, 21, 20),\n",
      "   (0, 0, 0, 1, 1, 1, 1)),\n",
      "  (('prep', 'pobj', 'relcl', 'nsubj'),\n",
      "   ('lie', 'to', 'people', 'care', 'she'),\n",
      "   (17, 18, 19, 21, 20),\n",
      "   (1, 1, 1, 1)),\n",
      "  (('nsubj',), ('care', 'she'), (21, 20), (1,)),\n",
      "  (('compound', 'nsubj', 'ccomp', 'prep', 'pobj', 'relcl', 'nsubj'),\n",
      "   ('senator', 'randall', 'lying', 'lie', 'to', 'people', 'care', 'she'),\n",
      "   (0, 1, 4, 17, 18, 19, 21, 20),\n",
      "   (0, 0, 0, 1, 1, 1, 1))))\n"
     ]
    }
   ],
   "source": [
    "pprint(train_texts_parsed[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
