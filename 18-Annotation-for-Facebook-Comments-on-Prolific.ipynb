{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"18-Annotation-for-Facebook-Comments-on-Prolific.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["7rm0Q44W4VUN"],"authorship_tag":"ABX9TyPaL4kUx+4XwTEIvnOL6GpE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9CHDDJN0yV-H"},"source":["# Facebook Comment Annotation on Prolific\n","* __Objective__: Create mini-batches (of 20) of comments to be used for annotation on Prolific\n","* __File Management__: Google Drive\n","* __Runtime Type__: GPU"]},{"cell_type":"code","metadata":{"id":"ZPg27jpT0JoE"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7gMwg800G7Q"},"source":["## Training BERT Model in PyTorch"]},{"cell_type":"code","metadata":{"id":"DSMHasR7wumU"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzNZ9DFJ0Mk0"},"source":["import os\n","\n","def read_split(dir):\n","    texts = []\n","    labels = []\n","    with open(dir, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()[1:]\n","        for line in lines:\n","            line = line.strip().split(\",\")\n","            assert(len(line) == 2)\n","            label = line[0].strip()\n","            text = line[1].strip()\n","            texts.append(text)\n","            if label == 'AH':\n","                labels.append(1)\n","            else:\n","                labels.append(0)\n","    return texts, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fANj4Iyc0Wk3"},"source":["train_texts, train_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/train.csv')\n","test_texts, test_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQJvlGb-0ZBk"},"source":["!pip install transformers\n","from transformers import BertTokenizer, BertForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsJ02rNr0cg-"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xO9zRFgN1u5d"},"source":["max_seq_length = 64\n","train_encodings = tokenizer(train_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")\n","test_encodings = tokenizer(test_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBBkYVjG1yUr"},"source":["import torch\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hN7bpjV31522"},"source":["from torch.utils.data import DataLoader\n","from transformers import AdamW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyuKXtIj19fo"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"doakZXL-2AWs"},"source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)\n","model.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xbz-TRJK2Dzy"},"source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","optim = AdamW(model.parameters(), lr=5e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMf9RSJu2QpT"},"source":["from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEgZPgsc2TGV"},"source":["for epoch in range(4):\n","    for batch in tqdm(train_loader):\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phoVVBTo2WhP"},"source":["import numpy as np\n","!pip install datasets\n","from datasets import load_metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pY-isM4C4FES"},"source":["metric= load_metric(\"accuracy\")\n","model.eval()\n","eval_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","for batch in eval_loader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CW1u8G9S4JtL"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './utkbert/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rm0Q44W4VUN"},"source":["## Extracting attention scores"]},{"cell_type":"code","metadata":{"id":"V3D3fGva4PkO"},"source":["from transformers import BertModel, BertTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOtkiTp34aYC"},"source":["model_version = 'utkbert'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrffA96q4cUW"},"source":["def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PB9xvBRP4iiW"},"source":["## Loading Facebook comments"]},{"cell_type":"code","metadata":{"id":"Z1onpGfA4giP"},"source":["import pickle\n","import matplotlib.pyplot as plt\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pntTqjjI4r5U"},"source":["comments = pickle.load(open('/content/gdrive/MyDrive/DL/Facebook/dataset/classified_comments.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zk8o4U14-v_"},"source":["comments[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5m-gMQC5Jh2"},"source":["comments = sorted(comments, key = lambda x: x['score'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SI-GOwl5jNU"},"source":["top_ah_comments = []\n","top_none_comments = []\n","\n","for i in range(1000):\n","    top_ah_comments.append(comments[i])\n","    top_none_comments.append(comments[-(i + 1)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfvOWj745k6_"},"source":["np.random.RandomState(seed=42).shuffle(top_ah_comments)\n","np.random.RandomState(seed=42).shuffle(top_none_comments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc-LPeBF_T96"},"source":["X = []\n","Y = []\n","\n","for x in top_ah_comments:\n","    c = len(x['text'].strip().split())\n","    if c >= 20 and c <= 80:\n","        X.append(x)\n","\n","for x in top_none_comments:\n","    c = len(x['text'].strip().split())\n","    if c >= 20 and c <= 80:\n","        Y.append(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjNKW9ab_wL4"},"source":["print(len(X), len(Y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d437v9avf74q"},"source":["pageX = dict()\n","pageY = dict()\n","\n","for x in X:\n","    try:\n","        pageX[x['page']].append(x)\n","    except:\n","        pageX[x['page']] = []\n","        pageX[x['page']].append(x)\n","\n","for y in Y:\n","    try:\n","        pageY[y['page']].append(y)\n","    except:\n","        pageY[y['page']] = []\n","        pageY[y['page']].append(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adMMrnJ8giIh"},"source":["for k, v in pageX.items():\n","    print(f'{k}: {len(v)}')\n","for k, v in pageY.items():\n","    print(f'{k}: {len(v)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmILGLFnQ6kw"},"source":["pages = ['DonaldTrump', 'FoxNews', 'Breitbart', 'joebiden', 'barackobama']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9e9qJ968XA1"},"source":["groups = []\n","\n","cnt = dict()\n","\n","for k in pages:\n","    cnt[k] = 0\n","\n","for i in range(5):\n","    group = []\n","    for k in pages:\n","        group.append(pageX[k][cnt[k]])\n","        group.append(pageY[k][cnt[k]])\n","        cnt[k] += 1\n","    for k in pages:\n","        group.append(pageX[k][cnt[k]])\n","        group.append(pageY[k][cnt[k]])\n","        cnt[k] += 1\n","    np.random.RandomState(seed=i+40).shuffle(group)\n","    groups.append(group)\n","    \n","np.random.RandomState(seed=42).shuffle(groups)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxr9jNgJ8kWs"},"source":["delimiter = '@#@#@'\n","delimiter2 = '##$$##@@'\n","\n","# Each group is balanced class-wise as well as length-wise and has 20 comments\n","\n","low, high = 4, 5  # index of groups which will be used in the form\n","\n","ctr = 0\n","\n","with open('/content/gdrive/MyDrive/DL/Facebook/dataset/CommentsForGoogleForm.txt', 'w', encoding='utf-8') as f:\n","    content = []\n","    for i in range(low, high):\n","        content_ = []\n","        for x in groups[i]:\n","            content_.append(f'{x[\"text\"]}{delimiter2}{x[\"link\"]}')\n","            ctr += 1\n","        content_ = delimiter.join([x for x in content_])\n","        content.append(content_)\n","    content = delimiter.join([x for x in content])\n","    content = re.sub(\"\\s+\", \" \", content)\n","    f.write(content)\n","    print(content)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_EVkqtF9I8f"},"source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return (W, A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xn1sCwCm9VDP"},"source":["def sanitize(x):\n","    x = x.lower()\n","    x = re.sub(\"\\s+\", \" \", x)  # converting space-like character to single white space\n","    x = ''.join([y for y in x if y.isalnum() or y ==' '])\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CksMztB-CoK"},"source":["delim3 = '$#$#$#$#$#$#@@@@'\n","delim4 = '@#$$#@@#@@#'\n","\n","def top_three_tokens(text):\n","    text = sanitize(text)\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    words, attentions = clean_array(words, attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    top_three_tkns = list()\n","    for i in range(3):\n","        idx = top_tokens[ind[i]][1]\n","        cur = ''\n","        if idx > 1:\n","            cur += '...'\n","        if idx != 0:\n","            cur += words[idx - 1] + ' '\n","        cur += words[idx]\n","        if idx != len(words) - 1:\n","            cur += ' ' + words[idx + 1]\n","        if idx < len(words) - 2:\n","            cur += '...'\n","        top_three_tkns.append(cur)\n","    return top_three_tkns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGZNXiOG-HdJ"},"source":["content = []\n","for i in range(low, high):\n","    for x in groups[i]:\n","        print(x['text'])\n","        trigrams = top_three_tokens(x['text'])\n","        np.random.shuffle(trigrams)\n","        content_ = delim3.join(trigrams)\n","        print(content_)\n","        content.append(content_)\n","content = delim4.join(content)\n","with open('/content/gdrive/MyDrive/DL/Facebook/dataset/trigrams.txt', 'w', encoding='utf-8') as f:\n","    f.write(content)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldTJ6-iiDmXr"},"source":[""],"execution_count":null,"outputs":[]}]}