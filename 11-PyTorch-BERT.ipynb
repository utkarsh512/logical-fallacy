{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMR08+BTYyeTYBurgCDnLD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2UB32Hoz0sAC"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ILskofGUXNG"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9Zcn4Y0r0UL"},"source":["import os\n","\n","def read_split(dir):\n","    texts = []\n","    labels = []\n","    with open(dir, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()[1:]\n","        for line in lines:\n","            line = line.strip().split(\",\")\n","            assert(len(line) == 2)\n","            label = line[0].strip()\n","            text = line[1].strip()\n","            texts.append(text)\n","            if label == 'AH':\n","                labels.append(1)\n","            else:\n","                labels.append(0)\n","    return texts, labels\n","\n","\n","train_texts, train_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/train.csv')\n","test_texts, test_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b18ewtiCD1P"},"source":["#R = 0.15 # Ratio of training data to be used\n","#train_texts = train_texts[:int(R * len(train_texts))]\n","#train_labels = train_labels[:int(R * len(train_labels))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJZ5Gb8YsSKQ"},"source":["!pip install transformers\n","from transformers import BertTokenizer, BertForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0nzyN_2vnMS"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZQnq6xn1Lm1"},"source":["max_seq_length = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdJFlttCwcWx"},"source":["train_encodings = tokenizer(train_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")\n","test_encodings = tokenizer(test_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjHdoljTwjjA"},"source":["import torch\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UH71dteJw4HJ"},"source":["from torch.utils.data import DataLoader\n","from transformers import AdamW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kkc4S69lxHgY"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e11eUsb_xMPY"},"source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)\n","model.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiJ1VGqVxVoJ"},"source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","optim = AdamW(model.parameters(), lr=5e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_XuD0Q8z71s"},"source":["!pip install tqdm\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbFu_CWZyMWD"},"source":["for epoch in range(3):\n","    for batch in tqdm(train_loader):\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIW8SLiK4rnl"},"source":["import numpy as np\n","!pip install datasets\n","from datasets import load_metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80ZWQQ3FyRAQ"},"source":["metric= load_metric(\"accuracy\")\n","model.eval()\n","eval_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","for batch in eval_loader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTB9OKNA2wLG"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = '/content/gdrive/MyDrive/DL/cnerg-bert-adhominem/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9H9r0xKN5Yj8"},"source":["# Visualization"]},{"cell_type":"markdown","metadata":{"id":"BFbzK9z17Sr9"},"source":["## Helper functions"]},{"cell_type":"code","metadata":{"id":"ske-ZcEptXIU"},"source":["from transformers import BertModel, BertTokenizer\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTbUpsAH7ZYk"},"source":["model_version = 'utkbert'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BeC2ET9i7bgR"},"source":["INTENSITY = 70\n","\n","def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UT-z6tWQ8elJ"},"source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return clean_apos(W, A)\n","\n","def clean_apos(w, a):\n","    W = []\n","    A = []\n","    ctr = 0\n","    while ctr != len(w):\n","        if w[ctr] == '\\'':\n","            W[-1] += w[ctr] + w[ctr + 1]\n","            A[-1] = min(INTENSITY, A[-1] + a[ctr] + a[ctr + 1])\n","            ctr += 2\n","        else:\n","            W.append(w[ctr])\n","            A.append(a[ctr])\n","            ctr += 1\n","    return W, A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7U-388dJ8vXS"},"source":["def top_three_tokens(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    for w in xx:\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                scores[j] = INTENSITY\n","    return words, scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uY8PlAD9nNW"},"source":["def clean_word(word_list):\n","  new_word_list = []\n","  for word in word_list:\n","    for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\",  \"{\", \"}\"]:\n","      if latex_sensitive in word:\n","        word = word.replace(latex_sensitive, '\\\\'+latex_sensitive)\n","    new_word_list.append(word)\n","  return new_word_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9BWebYL-h48"},"source":["import string\n","\n","def sanitize(text):\n","    text = text.lower()\n","    text = re.sub(\"\\s+\", \" \", text)  # converting space-like character to single white space\n","    text = re.sub(\"\\u2018\", '\\'', text)    # encoding apostrophe to X\n","    text = re.sub(\"\\u2019\", '\\'', text)    # encoding apostrophe to X\n","    xx = ''\n","    for x in text:\n","        if x in string.punctuation and x != '\\'':\n","            xx += ' '\n","        xx += x\n","    text = xx\n","    text = text.split()\n","    new_text = []\n","    for x in text:\n","        ok = False\n","        for y in x:\n","            ok = ok or y.isalnum()\n","        if ok:\n","            for c in string.punctuation:\n","                x = x.strip(c)\n","            new_text.append(x)\n","    return ' '.join(clean_word(new_text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhVrDNznAdLk"},"source":["sanitize(\"'Lol, who ain't you bro??'\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vIQe9cLxe-o"},"source":["header = r'''\\documentclass[10pt,a4paper]{article}\n","\\usepackage[left=1.00cm, right=1.00cm, top=1.00cm, bottom=2.00cm]{geometry}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}''' + '\\n\\n'\n","\n","footer = r'''\\end{CJK*}\n","\\end{document}'''\n","\n","def heatmap(word_list, attention_list, label_list, latex_file, title, batch_size=20, color='blue'):\n","    '''Routine to generate attention heatmaps for given texts\n","    ---------------------------------------------------------\n","    Input:\n","    :param word_list: array of texts\n","    :param attention_list: array of attention scores for each text\n","    :param label_list: label for each text\n","    :param latex_file: name of the latex file\n","    :param title: title of latex file\n","    :param batch_size: Number of comments in each batch\n","    '''\n","    with open(latex_file, 'w', encoding='utf-8') as f:\n","        f.write(header)\n","        f.write('\\\\section{%s}\\n\\n' % title)\n","\n","        n_examples = len(word_list)\n","        n_batches = n_examples // batch_size\n","\n","        for i in range(n_batches):\n","            batch_word_list = word_list[i * batch_size: (i + 1) * batch_size]\n","            batch_attention_list = attention_list[i * batch_size: (i + 1) * batch_size]\n","            batch_label_list = label_list[i * batch_size: (i + 1) * batch_size]\n","            f.write('\\\\subsection{Batch %d}\\n\\n' % (i + 1))\n","            for j in range(batch_size):\n","                f.write('\\\\subsubsection{Comment %d - %s}\\n\\n' % (j + 1, batch_label_list[j]))\n","                sentence = batch_word_list[j]\n","                score = batch_attention_list[j]\n","                assert len(sentence) == len(score)\n","                f.write('\\\\noindent')\n","                for k in range(len(sentence)):\n","                    f.write('\\\\colorbox{%s!%s}{' % (color, score[k]) + '\\\\strut ' + sentence[k] + '} ')\n","                f.write('\\n\\n')\n","\n","        f.write(footer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQnUfjm652TL"},"source":["## Visualizing Facebook dataset"]},{"cell_type":"code","metadata":{"id":"WliXdD9dHOnW"},"source":["import numpy as np\n","import pandas as pd\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1IUoeVg5Wec"},"source":["comments = pickle.load(open('/content/gdrive/MyDrive/DL/Facebook/dataset/classified_comments.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1UNQUsT6BeF"},"source":["comments = sorted(comments, key = lambda x: x['score'])\n","top_ah_comments = []\n","top_none_comments = []\n","\n","for i in range(1000):\n","    top_ah_comments.append(comments[i])\n","    top_none_comments.append(comments[-(i + 1)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6m0Q2v6F6Ea2"},"source":["np.random.RandomState(seed=42).shuffle(top_ah_comments)\n","np.random.RandomState(seed=42).shuffle(top_none_comments)\n","\n","X = []\n","Y = []\n","\n","for x in top_ah_comments:\n","    c = len(x['text'].strip().split())\n","    if c >= 20 and c <= 80:\n","        X.append(x)\n","\n","for x in top_none_comments:\n","    c = len(x['text'].strip().split())\n","    if c >= 20 and c <= 80:\n","        Y.append(x)\n","\n","pageX = dict()\n","pageY = dict()\n","\n","for x in X:\n","    try:\n","        pageX[x['page']].append(x)\n","    except:\n","        pageX[x['page']] = []\n","        pageX[x['page']].append(x)\n","\n","for y in Y:\n","    try:\n","        pageY[y['page']].append(y)\n","    except:\n","        pageY[y['page']] = []\n","        pageY[y['page']].append(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gb67mulM6HtW"},"source":["pages = ['DonaldTrump', 'FoxNews', 'Breitbart', 'joebiden', 'barackobama']\n","groups = []\n","\n","cnt = dict()\n","\n","for k in pages:\n","    cnt[k] = 0\n","\n","for i in range(5):\n","    group = []\n","    for k in pages:\n","        group.append(pageX[k][cnt[k]])\n","        group.append(pageY[k][cnt[k]])\n","        cnt[k] += 1\n","    for k in pages:\n","        group.append(pageX[k][cnt[k]])\n","        group.append(pageY[k][cnt[k]])\n","        cnt[k] += 1\n","    np.random.RandomState(seed=i+40).shuffle(group)\n","    groups.append(group)\n","    \n","np.random.RandomState(seed=42).shuffle(groups)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOdfZBMz6Kiw"},"source":["cols = ['Do you think this is an ad-hominem comment?']\n","for i in range(1, 20):\n","    cols.append(cols[0] + '.' + str(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bxbQVaF6PDU"},"source":["addr = '/content/gdrive/MyDrive/DL/Facebook/annotations/batch{}.csv'\n","labels = []\n","\n","for i in range(1, 6):\n","    df = pd.read_csv(addr.format(i))\n","    for x in cols:\n","        lbl = df[x].value_counts().idxmax()\n","        if lbl == 'Yes':\n","            labels.append('Ad hominem')\n","        else:\n","            labels.append('None')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ql-UWa3G6aHm"},"source":["texts = []\n","scores = []\n","\n","for i in range(5):\n","    for x in groups[i]:\n","        sent = x['text']\n","        sent = sanitize(sent)\n","        texts_, scores_ = top_three_tokens(sent)\n","        texts.append(texts_)\n","        scores.append(scores_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8N40_3r7Sto"},"source":["heatmap(texts, scores, labels, 'fb.tex', 'Facebook Corpus', color='cyan')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFeMog2F9Oq9"},"source":["## Visualizing Create Debate"]},{"cell_type":"code","metadata":{"id":"kVK-NRTA7iuL"},"source":["!git clone https://github.com/utkarsh512/CreateDebate-Scraper.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"an5nGN7t9oWV"},"source":["%cd CreateDebate-Scraper/src/nested/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"epixAlnc9r30"},"source":["import re\n","import pickle\n","from thread import Thread, Comment\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0xs_pD79u6P"},"source":["dir = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/{}.log'\n","comments_with_score = list()\n","\n","with open(dir.format('comments_with_score'), 'rb') as f:\n","    comments_with_score = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ia7pwJa49xTv"},"source":["reader_addr = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/threads.log'\n","reader = open(reader_addr, 'rb')\n","threads = list()\n","e = Thread()\n","\n","try:\n","    while True:\n","        e = pickle.load(reader)\n","        threads.append(e)\n","except:\n","    reader.close()\n","\n","authors = dict()\n","tot_comment_cnt = 0\n","idx = -1\n","\n","for thread in threads:\n","    idx += 1\n","    for key in thread.comments.keys():\n","        tot_comment_cnt += 1\n","        comment = thread.comments[key]\n","        cur_text = comment.body\n","        cur_author = comment.author\n","        url = thread.url\n","        try:\n","            authors[cur_author].append((cur_text, url, idx))\n","        except:\n","            authors[cur_author] = list()\n","            authors[cur_author].append((cur_text, url, idx))\n","\n","cur_author_cnt = 0\n","cur_comment_cnt = 0\n","tot_author_cnt = len(authors.keys())\n","comments_with_url = list()\n","\n","for author in authors.keys():\n","    cur_author_cnt += 1\n","    for i in range(len(authors[author])):\n","        cur_comment_cnt += 1\n","        text = [authors[author][i][0]]\n","        url = authors[author][i][1]\n","        idx = authors[author][i][2]\n","        comments_with_url.append((url, text, idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKzhUH6W91tK"},"source":["idx = np.random.randint(len(comments_with_score))\n","print(comments_with_score[idx][1][0])\n","print(comments_with_url[idx][1][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA31suXF95eT"},"source":["v = list()\n","for i in range(len(comments_with_score)):\n","    score = comments_with_score[i][0]\n","    text = comments_with_score[i][1]\n","    url = comments_with_url[i][0]\n","    idx = comments_with_url[i][2]\n","    v.append((score, text, url, idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsmPduAj99R6"},"source":["v = sorted(v)\n","top_ah_comments = []\n","top_none_comments = []\n","\n","for i in range(1000):\n","    top_ah_comments.append(v[i])\n","    top_none_comments.append(v[-(i + 1)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUSxfM4b-AL6"},"source":["# Random shuffle of top comments\n","np.random.RandomState(seed=42).shuffle(top_ah_comments)\n","np.random.RandomState(seed=42).shuffle(top_none_comments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36W6LjK4-Ck9"},"source":["small_ah_comments = []\n","medium_ah_comments = []\n","large_ah_comments = []\n","\n","small_none_comments = []\n","medium_none_comments = []\n","large_none_comments = []\n","\n","for x in top_ah_comments:\n","    c = len(x[1][0].strip().split())\n","    if c < 38:\n","        small_ah_comments.append(x)\n","    elif c >= 80:\n","        large_ah_comments.append(x)\n","    else:\n","        medium_ah_comments.append(x)\n","\n","for x in top_none_comments:\n","    c = len(x[1][0].strip().split())\n","    if c < 34:\n","        small_none_comments.append(x)\n","    elif c >= 72:\n","        large_none_comments.append(x)\n","    else:\n","        medium_none_comments.append(x)\n","\n","total_groups_possible = int(len(small_ah_comments) / 5)\n","total_groups_possible = min(total_groups_possible, int(len(small_none_comments) / 5))\n","total_groups_possible = min(total_groups_possible, int(len(large_ah_comments) / 2))\n","total_groups_possible = min(total_groups_possible, int(len(large_none_comments) / 2))\n","total_groups_possible = min(total_groups_possible, int(len(medium_ah_comments) / 3))\n","total_groups_possible = min(total_groups_possible, int(len(medium_none_comments) / 3))\n","print(total_groups_possible)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iatXjFQ4-FMf"},"source":["groups = []\n","\n","small_ah_count = 0\n","small_none_count = 0\n","medium_ah_count = 0\n","medium_none_count = 0\n","large_ah_count = 0\n","large_none_count = 0\n","\n","for i in range(96):\n","    group = []\n","    for j in range(5):\n","        group.append(small_ah_comments[small_ah_count])\n","        small_ah_count += 1\n","        group.append(small_none_comments[small_none_count])\n","        small_none_count += 1\n","    for j in range(3):\n","        group.append(medium_ah_comments[medium_ah_count])\n","        medium_ah_count += 1\n","        group.append(medium_none_comments[medium_none_count])\n","        medium_none_count += 1\n","    for j in range(2):\n","        group.append(large_ah_comments[large_ah_count])\n","        large_ah_count += 1\n","        group.append(large_none_comments[large_none_count])\n","        large_none_count += 1  \n","    np.random.RandomState(seed=42).shuffle(group)\n","    groups.append(group)\n","    \n","np.random.RandomState(seed=42).shuffle(groups)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVeyZVid-ISG"},"source":["texts = []\n","scores = []\n","\n","low, high = 1, 6\n","\n","for i in range(low, high):\n","    for x in groups[i]:\n","        sent = x[1][0].strip()\n","        sent = sanitize(sent)\n","        texts_, scores_ = top_three_tokens(sent)\n","        texts.append(texts_)\n","        scores.append(scores_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJozvcBs-hIW"},"source":["labels = []\n","\n","for i in range(1, 6):\n","    df = pd.read_csv(f'/content/gdrive/MyDrive/DL/CreateDebate/Politics/annotations/batch{i}.csv')\n","    for x in cols:\n","        lbl = df[x].value_counts().idxmax()\n","        if lbl == 'Yes':\n","            labels.append('Ad hominem')\n","        else:\n","            labels.append('None')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krT4j0A2_L6M"},"source":["heatmap(texts, scores, labels, 'cd.tex', 'Create Debate Corpus', color='cyan')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppXKGSHc_VoM"},"source":[],"execution_count":null,"outputs":[]}]}