{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOddAkJGm9fC7fDg2uDBkRt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"skft0usVlRE-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RzPFsUpk4pJ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"8FL29UIllUjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"Ee0QFnfslh5X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Helper routines and data"],"metadata":{"id":"xeYrluyUluo-"}},{"cell_type":"code","source":["tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"JEFBFLPEload"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUR_WORDS = {\n","  \"jews\": [\n","    \"jews\",\n","    \"oven dodger\",\n","    \"nazi\",\n","    \"dirty jew\",\n","    \"holocaust\",\n","    \"kikesucker\",\n","    \"hook nose\",\n","    \"kike\"\n","  ],\n","  \"homosexual\": [\n","    \"faggots usually\",\n","    \"fucking queer\",\n","    \"the biggest faggot\",\n","    \"dyke\",\n","    \"you fucking faggot\",\n","    \"hate faggots\",\n","    \"queer\",\n","    \"homosexual\",\n","    \"the faggots\",\n","    \"faggot\",\n","    \"faggots usually have\",\n","    \"gay\",\n","    \"faggots\",\n","    \"dykey\",\n","    \"ugly dyke\",\n","    \"faggots like you\",\n","    \"you a fag\",\n","    \"lesbian\",\n","    \"homo\",\n","    \"is a faggot\",\n","    \"like a faggot\",\n","    \"dykes\",\n","    \"faggots like\",\n","    \"faggot if you ever\"\n","  ],\n","  \"women\": [\n","    \"ugly dyke\",\n","    \"woman terrorist\",\n","    \"nigress\",\n","    \"bitch\",\n","    \"slut\",\n","    \"women\",\n","    \"sheeboon\",\n","    \"negress\",\n","    \"mud shark\",\n","    \"women threat\",\n","    \"you a lame bitch\",\n","    \"your a cunt\",\n","    \"white bitch\",\n","    \"niggeress\",\n","    \"hoe\",\n","    \"dykes\",\n","    \"niggress\",\n","    \"sheboon\",\n","    \"feminazi\"\n","  ],\n","  \"blacks\": [\n","    \"pavement ape\",\n","    \"the niggers\",\n","    \"negress\",\n","    \"porch monkey\",\n","    \"that nigger\",\n","    \"this nigger\",\n","    \"sheboon\",\n","    \"all niggers\",\n","    \"eurafrica\",\n","    \"shut up nigger\",\n","    \"picaninny\",\n","    \"african attack\",\n","    \"spearchucker\",\n","    \"how many niggers\",\n","    \"nigger\",\n","    \"africa\",\n","    \"niggers are in my\",\n","    \"dindu nuffin\",\n","    \"stupid nigger\",\n","    \"moolie\",\n","    \"niggers\",\n","    \"bluegum\",\n","    \"nigger ass\",\n","    \"you niggers\",\n","    \"fucking nigger\",\n","    \"nigger music\",\n","    \"niggress\",\n","    \"you a nigger\",\n","    \"many niggers are\",\n","    \"nigress\",\n","    \"blacks\",\n","    \"teenaper\",\n","    \"sheeboon\",\n","    \"dumb nigger\",\n","    \"niggeress\",\n","    \"pickaninny\",\n","    \"nigga\"\n","  ],\n","  \"muslim\": [\n","    \"muslim immigrant\",\n","    \"islam\",\n","    \"mudslime\",\n","    \"mooslem\",\n","    \"muslim refugee\",\n","    \"musslime\",\n","    \"shitlam\",\n","    \"muslim invasion\",\n","    \"moslime\",\n","    \"mooslamic\",\n","    \"muzzie\",\n","    \"allah akbar\",\n","    \"mooslime\",\n","    \"musloid\",\n","    \"mudslimes\",\n","    \"muslim\",\n","    \"muslimes\",\n","    \"moslum\",\n","    \"mussie\",\n","    \"muzrat\",\n","    \"muslim countries\",\n","    \"muzzy\",\n","    \"moslim\",\n","    \"jihadi\",\n","    \"muslim country\",\n","    \"moslem\",\n","    \"muzzrat\",\n","    \"mooslim\"\n","  ],\n","  \"arabs\": [\n","    \"towel head\",\n","    \"goatfucker\",\n","    \"arabs\",\n","    \"goathumper\",\n","    \"raghead\",\n","    \"rag head\",\n","    \"goathumping\",\n","    \"towelhead\",\n","    \"camel jockey\",\n","    \"sandnigger\",\n","    \"camel fucker\",\n","    \"sand nigger\"\n","  ],\n","  \"generic\": [\n","    \"to rape\",\n","    \"raped and\",\n","    \"shithole country\",\n","    \"get raped\",\n","    \"raped\",\n","    \"is a fucking\",\n","    \"shit skin\",\n","    \"raped by\",\n","    \"hate you\",\n","    \"fake empowerment\",\n","    \"abusive women\",\n","    \"fuck you too\",\n","    \"violence\",\n","    \"wit a lame nigga\",\n","    \"they all look\",\n","    \"alllivesmatter\",\n","    \"shithole countries\",\n","    \"fucking hate\",\n","    \"trailer trash\",\n","    \"kill all\",\n","    \"terrorist threat\",\n","    \"harassment\",\n","    \"kill yourself\",\n","    \"shitskin\",\n","    \"okay to be white\",\n","    \"fucking hate you\"\n","  ],\n","  \"white\": [\n","    \"full of white\",\n","    \"white trash\",\n","    \"white devil\",\n","    \"white\",\n","    \"are all white\",\n","    \"white boy\",\n","    \"white ass\",\n","    \"white bitch\",\n","    \"hillbilly\",\n","    \"whigger\",\n","    \"white christian\",\n","    \"white person\",\n","    \"all white\",\n","    \"white nigger\",\n","    \"redneck\",\n","    \"white honky\",\n","    \"wigger\",\n","    \"them white\"\n","  ],\n","  \"economy\": [\n","    \"ghetto\"\n","  ],\n","  \"immigrant\": [\n","    \"illegal immigrants\",\n","    \"immigrant not welcome\",\n","    \"immigrant terror\",\n","    \"mexcrement\",\n","    \"go back to where you come from\",\n","    \"muslim refugee\",\n","    \"illegal aliens\",\n","    \"refugee\",\n","    \"protect from immigrants\",\n","    \"negro\",\n","    \"refugees\",\n","    \"immigrant\",\n","    \"refugee invasion\",\n","    \"go back to where they come from\",\n","    \"refugees impact\",\n","    \"bring ebola\",\n","    \"immigrants\",\n","    \"illegal alien\",\n","    \"immigrant invasion\",\n","    \"bring disease\"\n","  ],\n","  \"mental\": [\n","    \"retard\",\n","    \"mongoloid\",\n","    \"retarded\"\n","  ],\n","  \"asians\": [\n","    \"asians\",\n","    \"ching chong\",\n","    \"chinaman\"\n","  ]\n","}"],"metadata":{"id":"c91_iSgbl0BM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load CreateDebate dataset"],"metadata":{"id":"QNfqQV1kl7zy"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"afWOq4Xcl3L5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append(v)\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append(v)\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"m7Sk-xjmmBub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load AH score and Slur count dataset"],"metadata":{"id":"d6dsEmnOmYnA"}},{"cell_type":"code","source":["# Loading computation from cache\n","\n","with open('/content/gdrive/MyDrive/Temp/47-ah-score.pkl', 'rb') as fp:\n","    ah_score = pickle.load(fp)\n","\n","with open('/content/gdrive/MyDrive/Temp/47-slur-count.pkl', 'rb') as fp:\n","    slur_count = pickle.load(fp)"],"metadata":{"id":"nf1tUp5jmHPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load CreateDebate profile dataset"],"metadata":{"id":"fW600VzPx8xa"}},{"cell_type":"code","source":["df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)"],"metadata":{"id":"kYIJ4V94yBRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"EQtgBf2PyBKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_stats_from_profile_data(user_subset):\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        rewards_.append(reward_points_map[user])\n","        efficiency_.append(efficiency_map[user])\n","        n_allies.append(allies_map[user])\n","        n_enemies.append(enemies_map[user])\n","        n_hostiles.append(hostiles_map[user])\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"A6nyZzPoyA5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Computing `ah_score_median`, `comment_count`, `comment_count_user` and `user_list`"],"metadata":{"id":"28_G8HYCsUEp"}},{"cell_type":"code","source":["# Median ah score per category per author\n","#   key: category -> author\n","#   value: median ah score\n","ah_score_median = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_median[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_median[category][author] = np.median(ah_scores)"],"metadata":{"id":"-TzCPf6BmcSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count = dict()\n","# key: category -> author\n","# value: number of comments written by author in the given forum\n","\n","for category in categories_selected:\n","    comment_count[category] = dict()\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        try:\n","            comment_count[category][author] += 1\n","        except KeyError:\n","            comment_count[category][author] = 1"],"metadata":{"id":"Rz3elq3vmibD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"DToDzkhrpib2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count_user = dict()\n","# key: user -> category -> year (in string)\n","# value: number of comments posted by the user for that category in the given year\n","\n","for user in user_list:\n","    comment_count_user[user] = dict()\n","    for category in categories_selected:\n","        comment_count_user[user][category] = dict()\n","        for year in range(2008, 2022):\n","            syear = str(year)\n","            comment_count_user[user][category][syear] = 0\n","\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        if comment['time'] == 'Not Available':\n","            continue\n","        year = comment['time'][:4]\n","        assert(int(year) < 2022 and int(year) >= 2008)\n","        comment_count_user[ comment['author'] ][ category ][ year ] += 1 "],"metadata":{"id":"fWoIUY2ypxZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments['politics2'][0]"],"metadata":{"id":"yQnhX69-mliA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# User migration\n"],"metadata":{"id":"XPeLDasVsGBd"}},{"cell_type":"code","source":["def get_migrated_users(categories_1, categories_2):\n","    \"\"\"\n","    Returns a list of users who were active on forums included in `categories_1`,\n","    but later started posting in forums included in `categories_2`\n","    \"\"\"\n","    if not (isinstance(categories_1, list) and isinstance(categories_2, list)):\n","        raise ValueError('Arguments should be list')\n","\n","    user_yom_list = list()\n","    for user in user_list:\n","        count_1 = [0 for _ in range(2008, 2022)]\n","        count_2 = [0 for _ in range(2008, 2022)]\n","\n","        for year in range(2008, 2022):\n","            for category in categories_1:\n","                count_1[year - 2008] += comment_count_user[user][category][str(year)]\n","            for category in categories_2:\n","                count_2[year - 2008] += comment_count_user[user][category][str(year)] \n","\n","        idx_nz_cat1 = -1   # index of first non-zero entry in count_1\n","        idx_nz_cat2 = -1   # index of first non-zero entry in count_2\n","\n","        for i in range(len(count_1)):\n","            if count_1[i] != 0:\n","                idx_nz_cat1 = i\n","                break\n","            \n","        for i in range(len(count_2)):\n","            if count_2[i] != 0:\n","                idx_nz_cat2 = i\n","                break\n","        \n","        if idx_nz_cat1 == -1 or idx_nz_cat2 == -1:\n","            # This user has not posted any comment in forums included in \n","            # categories_1 / categories_2. Hence, go to next user.\n","            continue\n","\n","        if idx_nz_cat1 >= idx_nz_cat2:\n","            # User first posted in forum for cat2, ignore this user\n","            continue\n","\n","        # Check if this user has at least one comment per user in count_1 till\n","        # he migrated to other forums\n","        ok = True\n","        for i in range(idx_nz_cat1, idx_nz_cat2):\n","            ok = ok and (count_1[i] != 0)\n","        \n","        # Append (user, year_of_migration) pair\n","        user_yom_list.append((user, (idx_nz_cat1 + 2008, idx_nz_cat2 + 2008)))\n","\n","    return user_yom_list"],"metadata":{"id":"egMaHyU9tKjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Partition the result of above function into extreme, moderate and low AH groups\n","\n","EXTREME_AH_UPPER = 1\n","EXTREME_AH_LOWER = 0.95\n","\n","MODERATE_AH_UPPER = 0.8\n","MODERATE_AH_LOWER = 0.7\n","\n","LOW_AH_UPPER = 0.6\n","LOW_AH_LOWER = 0.5\n","\n","\n","def partition_migrated_users(user_yom_list, categories_1, categories_2):\n","    extreme_user_yom_list = list()\n","    moderate_user_yom_list = list()\n","    low_user_yom_list = list()\n","\n","    for user, yom in user_yom_list:\n","        user_ah_score = []\n","        for category in (categories_1 + categories_2):\n","            user_ah_score.append(ah_score_median[category][user])\n","        \n","        user_ah_score = np.average(user_ah_score)\n","\n","        if user_ah_score <= EXTREME_AH_UPPER and user_ah_score >= EXTREME_AH_LOWER:\n","            extreme_user_yom_list.append((user, yom))\n","        \n","        elif user_ah_score <= MODERATE_AH_UPPER and user_ah_score >= MODERATE_AH_LOWER:\n","            moderate_user_yom_list.append((user, yom))\n","\n","        elif user_ah_score <= LOW_AH_UPPER and user_ah_score >= LOW_AH_LOWER:\n","            low_user_yom_list.append((user, yom))\n","\n","    return extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list"],"metadata":{"id":"T8zyIvyD3MdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plotting subroutines"],"metadata":{"id":"PY20ozos_xUr"}},{"cell_type":"code","source":["def plot_y1(user_yom_list):\n","    \"\"\"\n","    Plots y1 distribution\n","    \"\"\"\n","    cnt = [0 for i in range(14)]\n","    for user, yom in user_yom_list:\n","        y1, y2 = yom\n","        cnt[y1 - 2008] += 1\n","\n","    plt.bar([i for i in range(2008, 2022)], cnt)\n","    plt.xlabel('Year of joining first forum')\n","    plt.ylabel('#users')\n","    plt.show()"],"metadata":{"id":"1wT7vtzlCsz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_y1_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list):\n","    \"\"\"\n","    Plots y1 distribution\n","    \"\"\"\n","    cnt_1 = [0 for i in range(14)]\n","    cnt_2 = [0 for i in range(14)]\n","    cnt_3 = [0 for i in range(14)]\n","\n","    labels = [i for i in range(2008, 2022)]\n","\n","    for user, yom in extreme_user_yom_list:\n","        y1, y2 = yom\n","        cnt_1[y1 - 2008] += 1\n","\n","    for user, yom in moderate_user_yom_list:\n","        y1, y2 = yom\n","        cnt_2[y1 - 2008] += 1\n","    \n","    for user, yom in low_user_yom_list:\n","        y1, y2 = yom\n","        cnt_3[y1 - 2008] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width, cnt_1, width, label='Extreme AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks, cnt_2, width, label='Moderate AH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width, cnt_3, width, label='Low AH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Year of joining forum 1')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"rKvl6B4gIV56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_y2(user_yom_list):\n","    \"\"\"\n","    Plots y2 distribution\n","    \"\"\"\n","    cnt = [0 for i in range(14)]\n","    for user, yom in user_yom_list:\n","        y1, y2 = yom\n","        cnt[y2 - 2008] += 1\n","\n","    plt.bar([i for i in range(2008, 2022)], cnt)\n","    plt.xlabel('Year of joining second forum')\n","    plt.ylabel('#users')\n","    plt.show()"],"metadata":{"id":"6ERkj0r0DPNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_y2_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list):\n","    \"\"\"\n","    Plots y2 distribution\n","    \"\"\"\n","    cnt_1 = [0 for i in range(14)]\n","    cnt_2 = [0 for i in range(14)]\n","    cnt_3 = [0 for i in range(14)]\n","\n","    labels = [i for i in range(2008, 2022)]\n","\n","    for user, yom in extreme_user_yom_list:\n","        y1, y2 = yom\n","        cnt_1[y2 - 2008] += 1\n","\n","    for user, yom in moderate_user_yom_list:\n","        y1, y2 = yom\n","        cnt_2[y2 - 2008] += 1\n","    \n","    for user, yom in low_user_yom_list:\n","        y1, y2 = yom\n","        cnt_3[y2 - 2008] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width, cnt_1, width, label='Extreme AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks, cnt_2, width, label='Moderate AH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width, cnt_3, width, label='Low AH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Year of joining forum 2')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"CWlUwKW5L_qA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_delta(user_yom_list):\n","    \"\"\"\n","    Plots (Y2 - Y1) distribution\n","    \"\"\"\n","    max_diff = 0\n","    for user, yom in user_yom_list:\n","        y1, y2 = yom\n","        max_diff = max(max_diff, y2 - y1)\n","\n","    cnt = [0 for i in range(max_diff + 1)]\n","\n","    for user, yom in user_yom_list:\n","        y1, y2 = yom\n","        cnt[y2 - y1] += 1\n","\n","    print(cnt)\n","\n","    plt.bar([i for i in range(1, max_diff + 1)], cnt[1:])\n","    plt.xlabel('Difference b/w years of joining two forums')\n","    plt.ylabel('#users')\n","    plt.show()"],"metadata":{"id":"h9_xXkMW_0Xo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_delta_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list):\n","    \"\"\"\n","    Plots y2 distribution\n","    \"\"\"\n","    max_diff = 0\n","\n","    for user, yom in extreme_user_yom_list:\n","        y1, y2 = yom\n","        max_diff = max(max_diff, y2 - y1)\n","\n","    for user, yom in moderate_user_yom_list:\n","        y1, y2 = yom\n","        max_diff = max(max_diff, y2 - y1)\n","    \n","    for user, yom in low_user_yom_list:\n","        y1, y2 = yom\n","        max_diff = max(max_diff, y2 - y1)\n","\n","    cnt_1 = [0 for i in range(max_diff + 1)]\n","    cnt_2 = [0 for i in range(max_diff + 1)]\n","    cnt_3 = [0 for i in range(max_diff + 1)]\n","\n","    labels = [i for i in range(1, max_diff + 1)]\n","\n","    for user, yom in extreme_user_yom_list:\n","        y1, y2 = yom\n","        cnt_1[y2 - y1] += 1\n","\n","    for user, yom in moderate_user_yom_list:\n","        y1, y2 = yom\n","        cnt_2[y2 - y1] += 1\n","    \n","    for user, yom in low_user_yom_list:\n","        y1, y2 = yom\n","        cnt_3[y2 - y1] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width, cnt_1[1:], width, label='Extreme AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks, cnt_2[1:], width, label='Moderate AH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width, cnt_3[1:], width, label='Low AH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Difference b/w years of joining two forums')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"nTyf3rHIMfxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_wordcloud(user_name, categories, time_1, time_2):\n","    texts = []\n","\n","    for category in categories:\n","        for comment in comments[category]:\n","            if comment['author'] != user_name:\n","                continue\n","            if comment['time'] == 'Not Available':\n","                continue\n","            time_ = comment['time'][:10]\n","            if time_ < time_1 or time_ >= time_2:\n","                continue\n","            cleaned_comment_body = ' '.join(clean_text(comment['body']))\n","            texts.append(cleaned_comment_body)\n","\n","    texts = ' '.join(texts)\n","    word_cloud = wordcloud.WordCloud(stopwords=STOP_WORDS,\n","                                     collocations=False).generate(texts)\n","\n","    plt.imshow(word_cloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","# plot_wordcloud('excon', ['politics2'], '2018-01-01', '2019-01-01')"],"metadata":{"id":"rhh_r3g7QaHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_list = []\n","\n","for year in range(2008, 2022):\n","    for month in range(1, 10):\n","        time_list.append(f'{year}-0{month}-01')\n","    for month in range(10, 13):\n","        time_list.append(f'{year}-{month}-01')"],"metadata":{"id":"Z364xrKmZBJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_time_idx(time_):\n","    for i, x in enumerate(time_list):\n","        if x == time_:\n","            return i\n","    return -1"],"metadata":{"id":"j7Lg1snTaIZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## From Politics to Religion"],"metadata":{"id":"SEpiCxLwslfS"}},{"cell_type":"code","source":["# Find a set of users who were initially active on Politics, but later migrated\n","# to Religion.\n","categories_1 = ['politics2']\n","categories_2 = ['religion']\n","\n","user_yom_list = get_migrated_users(categories_1, categories_2)"],"metadata":{"id":"uGpCK5DksrJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list = partition_migrated_users(user_yom_list, categories_1, categories_2)"],"metadata":{"id":"s0StCbGIJzCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'{len(user_yom_list)} | {len(extreme_user_yom_list)} + {len(moderate_user_yom_list)} + {len(low_user_yom_list)}')"],"metadata":{"id":"AWv9KkCupaZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Network Study"],"metadata":{"id":"caKyUKIjydXr"}},{"cell_type":"code","source":["reader_addr = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/threads.log'\n","reader = open(reader_addr, 'rb')\n","threads = []\n","try:\n","    while True:\n","        e = pickle.load(reader)\n","        threads.append(e)\n","except:\n","    reader.close()"],"metadata":{"id":"u6XfpyBXyQm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_graph(user_subset, n1 = 0, n2 = 0):\n","    \"\"\"Builds support graph and dispute graph from hyper-parameters n1 and n2\n","    inputs\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    output\n","    (author_map : dict, reverse_map : list, author_count : int, support_graph : nx.DiGraph, support_matrix: list, dispute_graph : nxDiGraph, dispute_matrix : list)\n","    \"\"\"\n","\n","    athr = dict()\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    athr = dict()\n","\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","        cur_author = cmntMap[cid].author\n","\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    author_map = dict()\n","    reverse_map = [\"\"] * len(A)\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","\n","    support_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","    dispute_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        cur_pol = cmntMap[cid].polarity\n","        \n","        if cur_author in author_map and cur_pol != 'Not Available':\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                nxt_pol = cmntMap[key].polarity\n","                if nxt_author in author_map and nxt_pol != 'Not Available':\n","                    nxt_author_id = author_map[nxt_author]\n","                    if cur_pol == nxt_pol:\n","                        support_matrix[nxt_author_id][cur_author_id] += 1\n","                    else:\n","                        dispute_matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","\n","    support_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if support_matrix[i][j] != 0:\n","                support_graph.add_weighted_edges_from([(i, j, support_matrix[i][j])])\n","\n","    dispute_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if dispute_matrix[i][j] != 0:\n","                dispute_graph.add_weighted_edges_from([(i, j, dispute_matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, support_graph, support_matrix, dispute_graph, dispute_matrix)"],"metadata":{"id":"RdgFjrMUypT2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vary x_user_yom_list to\n","#    - user_yom_list\n","#    - extreme_user_yom_list\n","#    - moderate_user_yom_list\n","#    - low_user_yom_list\n","\n","user_subset = [user for (user, yom) in low_user_yom_list]\n","\n","_1, _2, _3, support_graph, _4, dispute_graph, _6 = build_graph(user_subset)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph)\n","except:\n","    support_graph_r = 0\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph)\n","except:\n","    dispute_graph_r = 0\n","\n","avgs, stds = get_stats_from_profile_data(user_subset)\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","print('Average / std-dev of reward points', avgs[0], stds[0])\n","print('Average / std-dev of efficiency', avgs[1], stds[1])\n","print('Avergae / std-dev of n_allies', avgs[2], stds[2])\n","print('Avergae / std-dev of n_enemies', avgs[3], stds[3])\n","print('Avergae / std-dev of n_hostiles', avgs[4], stds[4])\n","\n","print(len(user_subset))"],"metadata":{"id":"ybETMnQ0yuEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6pfSTxGjzV_i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plots"],"metadata":{"id":"gADIQ5chhu_S"}},{"cell_type":"code","source":["plot_y1(user_yom_list)"],"metadata":{"id":"dV_GH0D9DaKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_y1_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list)"],"metadata":{"id":"smpmZfMnLYcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_y2(user_yom_list)"],"metadata":{"id":"EGwvfXBnDiBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_y2_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list)"],"metadata":{"id":"l5D_vfAJMLJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_delta(user_yom_list)"],"metadata":{"id":"Iadmzfqp6TaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_delta_v2(extreme_user_yom_list, moderate_user_yom_list, low_user_yom_list)"],"metadata":{"id":"TEUSPoK4NU7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extreme AH users"],"metadata":{"id":"qcrJVTQOh1ID"}},{"cell_type":"code","source":["# Plotting wordclouds \n","\n","extreme_user_yom_list"],"metadata":{"id":"7pT5Alob6f2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: Oceaneer"],"metadata":{"id":"ACtiF4xiiAdU"}},{"cell_type":"code","source":["comment_count['politics2']['Oceaneer']"],"metadata":{"id":"ieSc0b1PiHG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count['religion']['Oceaneer']"],"metadata":{"id":"C2tQH-CziG2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2017-01-01')"],"metadata":{"id":"fNzYEGwPiGig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","print(time_list[idx])\n","\n","try:\n","    plot_wordcloud(user_name='Oceaneer',\n","                categories=['politics2'],\n","                time_1=time_list[idx],\n","                time_2=time_list[idx + 1])\n","except:\n","    pass\n","\n","finally:\n","    idx += 1"],"metadata":{"id":"W3UvkCkdiYbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2018-01-01')"],"metadata":{"id":"veGaxBXoikgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","print(time_list[idx])\n","\n","try:\n","    plot_wordcloud(user_name='Oceaneer',\n","                categories=['politics2'],\n","                time_1=time_list[idx],\n","                time_2=time_list[idx + 1])\n","except:\n","    pass\n","\n","finally:\n","    idx += 1"],"metadata":{"id":"8H_16nkdimLm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: jamesbody"],"metadata":{"id":"jmJFVDoVioyA"}},{"cell_type":"code","source":["comment_count['politics2']['jamesbody']"],"metadata":{"id":"sC7P-p25ivlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count['religion']['jamesbody']"],"metadata":{"id":"3QS0LK0Ni3tY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2019-01-01')"],"metadata":{"id":"NWYnUTeIaFB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(time_list[idx])\n","\n","try:\n","    plot_wordcloud(user_name='jamesbody',\n","                categories=['religion'],\n","                time_1=time_list[idx],\n","                time_2=time_list[idx + 1])\n","except:\n","    pass\n","\n","finally:\n","    idx += 1"],"metadata":{"id":"cv6CrxJB6ps7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Moderate AH users"],"metadata":{"id":"HAc87wFLjBmn"}},{"cell_type":"code","source":["for user, yom in moderate_user_yom_list:\n","    print(f'User: {user}, Politics Y: {yom[0]}, Religion Y: {yom[1]}, Politics C: {comment_count[\"politics2\"][user]}, Religion C: {comment_count[\"religion\"][user]}')"],"metadata":{"id":"j9gcvbVvdJbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: Rickinmich61"],"metadata":{"id":"ZUHPeFciolMS"}},{"cell_type":"code","source":["idx = find_time_idx('2009-01-01')"],"metadata":{"id":"CKTYHXv5oqMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Rickinmich61',\n","                    categories=['politics2'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","    except:\n","        pass\n","\n","    finally:\n","        idx += 1"],"metadata":{"id":"lIhJkyxeoxoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2011-01-01')"],"metadata":{"id":"a8X8F-UvquwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Rickinmich61',\n","                    categories=['religion'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","\n","    finally:\n","        idx += 1"],"metadata":{"id":"CX68MEKGqpXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: Pickleface"],"metadata":{"id":"NOIVTZxqkxAC"}},{"cell_type":"code","source":["idx = find_time_idx('2018-01-01')"],"metadata":{"id":"yzFh9GaLjMRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","print(time_list[idx])\n","\n","try:\n","    plot_wordcloud(user_name='Pickleface',\n","                categories=['politics2'],\n","                time_1=time_list[idx],\n","                time_2=time_list[idx + 1])\n","except:\n","    pass\n","\n","finally:\n","    idx += 1"],"metadata":{"id":"9yc_PdOdk04D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2019-01-01')"],"metadata":{"id":"K7lVLVmOnp7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","print(time_list[idx])\n","\n","try:\n","    plot_wordcloud(user_name='Pickleface',\n","                categories=['religion'],\n","                time_1=time_list[idx],\n","                time_2=time_list[idx + 1])\n","except:\n","    pass\n","\n","finally:\n","    idx += 1"],"metadata":{"id":"UW5yvj80oBW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Low AH users"],"metadata":{"id":"ejj-XDOSravM"}},{"cell_type":"code","source":["for user, yom in low_user_yom_list:\n","    print(f'User: {user}, Politics Y: {yom[0]}, Religion Y: {yom[1]}, Politics C: {comment_count[\"politics2\"][user]}, Religion C: {comment_count[\"religion\"][user]}')"],"metadata":{"id":"x0waXuKfraUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: umaroth"],"metadata":{"id":"6XpB2ikpvZMB"}},{"cell_type":"code","source":["idx = find_time_idx('2013-01-01')\n","idx"],"metadata":{"id":"dkcad21pvc88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='umaroth',\n","                    categories=['religion'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","\n","    finally:\n","        idx += 1"],"metadata":{"id":"mKSWH3MXvY0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### User: JugsNThugs"],"metadata":{"id":"zcG8rxhproRq"}},{"cell_type":"code","source":["idx = find_time_idx('2019-01-01')"],"metadata":{"id":"dncKzkteuOvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='JugsNThugs',\n","                    categories=['religion'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","\n","    finally:\n","        idx += 1"],"metadata":{"id":"eWsivd3ZuRFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2018-01-01')"],"metadata":{"id":"w40iz3wgrtbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Press Ctrl + Enter \n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='JugsNThugs',\n","                    categories=['politics2'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","\n","    finally:\n","        idx += 1"],"metadata":{"id":"9PtKee9PreTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MRiokp0JsfEm"},"execution_count":null,"outputs":[]}]}