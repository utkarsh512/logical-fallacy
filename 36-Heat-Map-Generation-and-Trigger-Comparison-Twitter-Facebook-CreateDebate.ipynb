{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"36-Heat-Map-Generation-and-Trigger-Comparison-Twitter-Facebook-CreateDebate.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPpw2RWD+zR1Yz5iUOwUej2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["__Objective__: To train PyTorch BERT model on _Change My View_ dataset and use it to generate heat maps for ad hominem tweets\n","\n","__Runtime__: GPU"],"metadata":{"id":"o7VZCsbXRjTu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgVgFpsURbld"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm \n","import pickle as pkl\n","import matplotlib.pyplot as plt\n","from matplotlib import colors"],"metadata":{"id":"kxRMsjbZSdWO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training PyTorch BERT"],"metadata":{"id":"oaHlTt0qS01T"}},{"cell_type":"code","source":["import os\n","\n","def read_split(dir):\n","    texts = []\n","    labels = []\n","    with open(dir, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()[1:]\n","        for line in lines:\n","            line = line.strip().split(\",\")\n","            assert(len(line) == 2)\n","            label = line[0].strip()\n","            text = line[1].strip()\n","            texts.append(text)\n","            if label == 'AH':\n","                labels.append(1)\n","            else:\n","                labels.append(0)\n","    return texts, labels\n","\n","\n","train_texts, train_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/train.csv')\n","test_texts, test_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/test.csv')"],"metadata":{"id":"YrGj9PjLSwaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","from transformers import BertTokenizer, BertForSequenceClassification"],"metadata":{"id":"1mFAGhiWS6JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"YXFSWXs1S-Iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_seq_length = 64\n","train_encodings = tokenizer(train_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")\n","test_encodings = tokenizer(test_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")"],"metadata":{"id":"o38aIDKbTGE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"],"metadata":{"id":"KSoPDryCTRHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from transformers import AdamW"],"metadata":{"id":"kWkOqxycTYHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"Bs6swOdLTdUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)\n","model.train()"],"metadata":{"id":"sECKCUE5Tf5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","optim = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"NS9cf7_JTh_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","for epoch in range(3):\n","    for batch in tqdm(train_loader):\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()"],"metadata":{"id":"UosaxDhvTrKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './utkbert/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"metadata":{"id":"zivXeBiITvyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization set-up"],"metadata":{"id":"NsJRa6lUVIde"}},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","import re"],"metadata":{"id":"OZx3uKvBVBf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_version = 'utkbert'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"metadata":{"id":"D3qdmT2RVWpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INTENSITY = 70\n","\n","def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"metadata":{"id":"mlNSypKLVaI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return clean_apos(W, A)\n","\n","def clean_apos(w, a):\n","    W = []\n","    A = []\n","    ctr = 0\n","    while ctr != len(w):\n","        if w[ctr] == '\\'':\n","            W[-1] += w[ctr] + w[ctr + 1]\n","            A[-1] = min(INTENSITY, A[-1] + a[ctr] + a[ctr + 1])\n","            ctr += 2\n","        else:\n","            W.append(w[ctr])\n","            A.append(a[ctr])\n","            ctr += 1\n","    return W, A"],"metadata":{"id":"IozNBhTeVjS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def top_three_tokens(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    for w in xx:\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                scores[j] = INTENSITY\n","    return words, scores"],"metadata":{"id":"ZAWW3nVDVnCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_word(word_list):\n","  new_word_list = []\n","  for word in word_list:\n","    for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\",  \"{\", \"}\"]:\n","      if latex_sensitive in word:\n","        word = word.replace(latex_sensitive, '\\\\'+latex_sensitive)\n","    new_word_list.append(word)\n","  return new_word_list"],"metadata":{"id":"PG4eswy6VqT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["header = r'''\\documentclass[10pt,a4paper]{article}\n","\\usepackage[left=1.00cm, right=1.00cm, top=1.00cm, bottom=2.00cm]{geometry}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}''' + '\\n\\n'\n","\n","footer = r'''\\end{CJK*}\n","\\end{document}'''\n","\n","def heatmap(word_list, attention_list, label_list, latex_file, title, batch_size=20, color='blue'):\n","    '''Routine to generate attention heatmaps for given texts\n","    ---------------------------------------------------------\n","    Input:\n","    :param word_list: array of texts\n","    :param attention_list: array of attention scores for each text\n","    :param label_list: label for each text\n","    :param latex_file: name of the latex file\n","    :param title: title of latex file\n","    :param batch_size: Number of comments in each batch\n","    '''\n","    with open(latex_file, 'w', encoding='utf-8') as f:\n","        f.write(header)\n","        f.write('\\\\section{%s}\\n\\n' % title)\n","\n","        n_examples = len(word_list)\n","        n_batches = n_examples // batch_size\n","\n","        for i in range(n_batches):\n","            batch_word_list = word_list[i * batch_size: (i + 1) * batch_size]\n","            batch_attention_list = attention_list[i * batch_size: (i + 1) * batch_size]\n","            batch_label_list = label_list[i * batch_size: (i + 1) * batch_size]\n","            f.write('\\\\subsection{Batch %d}\\n\\n' % (i + 1))\n","            for j in range(batch_size):\n","                f.write('\\\\subsubsection{Comment %d - %s}\\n\\n' % (j + 1, batch_label_list[j]))\n","                sentence = batch_word_list[j]\n","                score = batch_attention_list[j]\n","                assert len(sentence) == len(score)\n","                f.write('\\\\noindent')\n","                for k in range(len(sentence)):\n","                    f.write('\\\\colorbox{%s!%s}{' % (color, score[k]) + '\\\\strut ' + sentence[k] + '} ')\n","                f.write('\\n\\n')\n","\n","        f.write(footer)"],"metadata":{"id":"ZdGedzO1VtYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","def sanitize(text):\n","    text = text.lower()\n","    text = re.sub(\"\\s+\", \" \", text)  # converting space-like character to single white space\n","    text = re.sub(\"\\u2018\", '\\'', text)    # encoding apostrophe to X\n","    text = re.sub(\"\\u2019\", '\\'', text)    # encoding apostrophe to X\n","    xx = ''\n","    for x in text:\n","        if x in string.punctuation and x != '\\'':\n","            xx += ' '\n","        xx += x\n","    text = xx\n","    text = text.split()\n","    new_text = []\n","    for x in text:\n","        ok = False\n","        for y in x:\n","            ok = ok or y.isalnum()\n","        if ok:\n","            for c in string.punctuation:\n","                x = x.strip(c)\n","            new_text.append(x)\n","    return ' '.join(clean_word(new_text))"],"metadata":{"id":"Dm4O_mCiZ1eY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# modified implementation of `top_three_tokens` routine\n","# this will return those top tokens instead of token, score lists\n","\n","def top_three_tokens2(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    res = list()\n","    for w in xx:\n","        res_ = list()\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                res_.append(words[j])\n","        res.append(tuple(res_))\n","    #return words, scores\n","    return res"],"metadata":{"id":"qR40P51CPvZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Twitter Data"],"metadata":{"id":"8l51MKcBV982"}},{"cell_type":"code","source":["# loading the saved tweets\n","tw_base_addr = '/content/gdrive/MyDrive/DL/Twitter/classified/{}.csv'\n","tw_pages = ['nytimes', 'npr', 'foxnews', 'breitbart']\n","\n","df = dict()\n","for tw_page in tw_pages:\n","    df[tw_page] = pd.read_csv(tw_base_addr.format(tw_page))"],"metadata":{"id":"hkwhsrIUVzEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# combined df\n","cdf = pd.concat([df['nytimes'], df['npr'], df['foxnews'], df['breitbart']])"],"metadata":{"id":"oh8Ezp2MJA58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(cdf)"],"metadata":{"id":"g5vxcH14LMMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf['score'].hist(bins=[x/100 for x in range(101)]) # classification score distribution"],"metadata":{"id":"jHBty3arW49D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf = cdf[cdf['score'] < 0.05] # filtering the top ad hominem tweets\n","len(cdf)"],"metadata":{"id":"ert6gXWDMqPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf"],"metadata":{"id":"c9Xkw6IaNCt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preparing trigrams\n","# these will be used for comparison with Facebook and CreateDebate\n","\n","tw_texts = list(cdf['pptweet']) \n","tw_freq = dict()\n","tw_success = 0 \n","\n","for tw_text in tqdm(tw_texts):\n","    try:\n","        for tw_tr in top_three_tokens2(sanitize(tw_text)):\n","            try:\n","                tw_freq[tw_tr] += 1 \n","            except KeyError:\n","                tw_freq[tw_tr] = 1\n","        tw_success += 1 \n","    except: \n","        pass"],"metadata":{"id":"P3xzZ9LmNA0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the computation (useful in case of session crash)\n","\n","with open('/content/gdrive/MyDrive/Temp/btp_36_tw_freq.pkl', 'wb') as fp:\n","    pkl.dump(tw_freq, fp)"],"metadata":{"id":"YeJgT9PnVr_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_tw_freq.pkl', 'rb') as fp:\n","    tw_freq = pkl.load(fp)"],"metadata":{"id":"hL7wtRbOVz2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tw_data = list()\n","\n","for k, v in tw_freq.items():\n","    tw_data.append((v, k))\n","\n","tw_data = sorted(tw_data, reverse=True)"],"metadata":{"id":"eUYqHMtpV-D-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cnt, tr in tw_data[20:40]:\n","    print(f'{str(tr):40} - {cnt}')"],"metadata":{"id":"BS5HHSBmWefd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tw_ad_texts = ' '.join(tw_texts)"],"metadata":{"id":"NPbDMUTFZ5L4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extracting top 100 ad hominem tweets from each `tw_page`\n","\n","tw_vdata = dict()\n","\n","for tw_page in tw_pages: \n","    tw_vdata[tw_page] = list()\n","\n","    for index, row in df[tw_page].iterrows():\n","        tw_vdata[tw_page].append((row['score'], row['pptweet']))\n","    tw_vdata[tw_page].sort()\n","\n","tw_viz_texts = dict()\n","\n","for tw_page in tw_pages:\n","    tw_viz_texts[tw_page] = list()\n","    for _score, _text in tw_vdata[tw_page][:100]:\n","        tw_viz_texts[tw_page].append(_text)"],"metadata":{"id":"ceuUJnPnXEuS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating visualizations\n","\n","for tw_page in tw_pages:\n","    tw_vtexts = list()\n","    tw_vscores = list()\n","    for tw_text in tw_viz_texts[tw_page]:\n","        sent = sanitize(tw_text)\n","        try:\n","            tw_t, tw_s = top_three_tokens(sent)\n","            tw_vtexts.append(tw_t) \n","            tw_vscores.append(tw_s)\n","        except:\n","            pass\n","    heatmap(tw_vtexts, tw_vscores, ['Ad hominem'] * len(tw_vtexts), f'{tw_page}.tex', f'Ad hominem tweets: {tw_page}', color='cyan')"],"metadata":{"id":"FWaNUv-0YgFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Comparing triggers in FB Data and CreateDebate"],"metadata":{"id":"7yHPE3wkcVbC"}},{"cell_type":"markdown","source":["## Facebook"],"metadata":{"id":"p4vKm9ddyxO2"}},{"cell_type":"code","source":["fb_df = pd.read_csv('/content/gdrive/MyDrive/DL/Facebook/fbscraper/nytimes/2016/2016c.csv')"],"metadata":{"id":"4snwsr-2bHXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_df"],"metadata":{"id":"tBQ0D_UecxNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_df['score'].hist(bins=[x/100 for x in range(101)])"],"metadata":{"id":"k1FMEJpuJsu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# considering comments which are classified as ad hominem\n","# with at least 95% confidence\n","fb_df = fb_df[fb_df['score'] < 0.05] "],"metadata":{"id":"ESBtNfeedyQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(fb_df) / 126328 * 100"],"metadata":{"id":"LXr7m-jewPBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_texts = list(fb_df['processedText'])"],"metadata":{"id":"Hly18s7daKzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fb_texts = list(fb_df['processedText'])\n","fb_freq = dict() # dictionary with token trigrams: frequency\n","fb_success = 0\n","for fb_text in tqdm(fb_texts): \n","    try:\n","        for fb_tr in top_three_tokens2(sanitize(fb_text)):\n","            try:\n","                fb_freq[fb_tr] += 1\n","            except KeyError: \n","                fb_freq[fb_tr] = 1\n","        fb_success += 1\n","    except:\n","        # probably fb_text is too short to contain 3 trigrams\n","        pass"],"metadata":{"id":"EvE7iipVczas"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the computation (useful in case of session crash)\n","\n","with open('/content/gdrive/MyDrive/Temp/btp_36_fb_freq.pkl', 'wb') as fp:\n","    pkl.dump(fb_freq, fp)"],"metadata":{"id":"Kli8PEocBsSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_fb_freq.pkl', 'rb') as fp:\n","    fb_freq = pkl.load(fp)"],"metadata":{"id":"rQxsGqsgk74M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_data = list()\n","for k, v in fb_freq.items():\n","    fb_data.append((v, k))\n","fb_data = sorted(fb_data, reverse=True)"],"metadata":{"id":"BXjn8RghgOFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cnt, tr in fb_data[:20]:\n","    print(f'{str(tr):40} - {cnt}')"],"metadata":{"id":"RDSeidFgu5Fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(fb_data)"],"metadata":{"id":"86VhDnOVvibf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_ad_text = ' '.join(fb_texts)"],"metadata":{"id":"wPYcuks1v3pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_fb_ad_text.pkl', 'wb') as fp:\n","    pkl.dump(fb_ad_text, fp)"],"metadata":{"id":"RhQWS-cZCo-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CreateDebate"],"metadata":{"id":"X9P2hFDty1Dx"}},{"cell_type":"code","source":["# loading createdebate corpus\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"S1SzLArQyK9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from thread import Thread, Comment # for CreateDebate corpus\n","import pickle\n","from copy import deepcopy"],"metadata":{"id":"EjE8ZILtyW0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_comments = dict()\n","cd_categories = ['politics2', 'religion', 'world', 'science', 'law', 'technology']\n","\n","for cat in cd_categories:\n","    cd_comments[cat] = list()"],"metadata":{"id":"x38KUNsvyfjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in tqdm(cd_categories):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","    print(f'{cat} - {len(threads)}')\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append(v)\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append(v)\n","    ctr = 0\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            cd_comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"6NMDR1P4zOkW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_comments['law'][0]"],"metadata":{"id":"l5W-a5Ayz46G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting score distribution\n","\n","cd_scores = list()\n","\n","for cat in cd_categories:\n","    for cd_comment in cd_comments[cat]:\n","        cd_scores.append(cd_comment['score'])"],"metadata":{"id":"7Y6GXDctKJQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(cd_scores, bins=[x/100 for x in range(101)])"],"metadata":{"id":"Ztw2IWpfKYf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(cd_scores)"],"metadata":{"id":"-V5crQRwK4uS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# considering only ad hominem comments with 95% classification score\n","\n","cd_texts = list()\n","cd_authors = set()\n","\n","for cat in cd_categories:\n","    for cd_comment in tqdm(cd_comments[cat]):\n","        cd_authors.add(cd_comment['author'].lower())\n","        if cd_comment['score'] < 0.05:\n","            cd_texts.append(sanitize(cd_comment['body']))"],"metadata":{"id":"tArWkPAS0PI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_texts = cd_texts[:10000]"],"metadata":{"id":"Q2KTWrem1Haq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing name of authors from the comment text\n","\n","cd_texts_pp = list()\n","for cd_text in tqdm(cd_texts):\n","    pp_tokens = list()\n","    for token in cd_text.split():\n","        if token not in cd_authors: \n","            pp_tokens.append(token) \n","    cd_texts_pp.append(' '.join(pp_tokens))"],"metadata":{"id":"qrErTvl3mYwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_it = 56\n","print(f'{cd_texts[cd_it]}\\n\\n{cd_texts_pp[cd_it]}')"],"metadata":{"id":"BbStaESpnNOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_freq = dict() \n","cd_success = 0 \n","for cd_text in tqdm(cd_texts_pp): \n","    try:\n","        for cd_tr in top_three_tokens2(cd_text):\n","            try:\n","                cd_freq[cd_tr] += 1 \n","            except KeyError:\n","                cd_freq[cd_tr] = 1  \n","        cd_success += 1  \n","    except:\n","        pass"],"metadata":{"id":"V-mtui6wDfAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_cd_freq.pkl', 'wb') as fp:\n","    pkl.dump(cd_freq, fp)"],"metadata":{"id":"4wcDKwAFDU1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_cd_freq.pkl', 'rb') as fp:\n","    cd_freq = pkl.load(fp)"],"metadata":{"id":"8J69PmRJlnkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_data = list()\n","for k, v in cd_freq.items():\n","    cd_data.append((v, k))  \n","cd_data = sorted(cd_data, reverse=True)  "],"metadata":{"id":"6F_K2aebS23m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cd_s, cd_t in cd_data[:20]:   \n","    print(f'{str(cd_t):30} - {cd_s}')"],"metadata":{"id":"ds_ZwhrzTIaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd_ad_text = ' '.join(cd_texts_pp)"],"metadata":{"id":"ZhrmiDjASYba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/btp_36_cd_ad_text.pkl', 'wb') as fp:\n","    pkl.dump(cd_ad_text, fp)"],"metadata":{"id":"3jSNUhnhSb-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparison"],"metadata":{"id":"AVO_EDLhSmm-"}},{"cell_type":"code","source":["!pip install shifterator"],"metadata":{"id":"r2i3CiW2Sk6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shifterator as sh"],"metadata":{"id":"Dk7ygMpeSskt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_set = set(fb_freq.keys())\n","cd_set = set(cd_freq.keys())"],"metadata":{"id":"OWAnYJSESz0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _x in fb_set[:100]:\n","    print(_x)"],"metadata":{"id":"C8L_TtTl-JVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# jacquard overlap\n","len(fb_set & cd_set) / len(fb_set | cd_set) * 100"],"metadata":{"id":"1QbFvhrkUaOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fb_tokens = dict()  \n","cd_tokens = dict() \n","tw_tokens = dict() \n","\n","for token in fb_ad_text.strip().split():\n","    token = token.strip() \n","    try:\n","        fb_tokens[token] += 1 \n","    except KeyError:\n","        fb_tokens[token] = 1  \n","\n","for token in cd_ad_text.strip().split():\n","    token = token.strip() \n","    try:\n","        cd_tokens[token] += 1  \n","    except KeyError:\n","        cd_tokens[token] = 1   \n","\n","for token in tw_ad_texts.strip().split():\n","    token = token.strip() \n","    try:\n","        tw_tokens[token] += 1  \n","    except KeyError:\n","        tw_tokens[token] = 1  "],"metadata":{"id":"VF3pcQIiUorl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jsd_shift_1 = sh.JSDivergenceShift(type2freq_1=tw_tokens,\n","                                   type2freq_2=fb_tokens,\n","                                   weight_1=0.5,\n","                                   weight_2=0.5,\n","                                   base=2,\n","                                   alpha=1)"],"metadata":{"id":"1gH1cMCpVfMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jsd_shift_1.get_shift_graph(title='Jensen-Shannon Divergence Shifts b/w Twitter and Facebook')"],"metadata":{"id":"K2eDxPeqVooM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(cd_data)"],"metadata":{"id":"jBIS7gTlV2LA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fr = [1, 2, 5, 10, 20, 50, 100]\n","\n","jac_matrix = [[0 for i in range(len(fr))] for j in range(len(fr))]\n","\n","for i in range(len(fr)):\n","    for j in tqdm(range(len(fr))):\n","        tw_limit = int(len(tw_data) * (fr[i] / 100))\n","        fb_limit = int(len(fb_data) * (fr[j] / 100))\n","        tw_set = set()\n","        fb_set = set()\n","        for _, tokens in tw_data[:tw_limit]:\n","            tw_set.add(tokens) \n","        for _, tokens in fb_data[:fb_limit]: \n","            fb_set.add(tokens) \n","        jac_matrix[i][j] = len(tw_set & fb_set) / len(tw_set | fb_set) * 100"],"metadata":{"id":"31PCgmOl-kc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fr_str = list()\n","for _x in fr:\n","    fr_str.append(str(_x))"],"metadata":{"id":"V7IllHpcBD2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(jac_matrix, interpolation='nearest')\n","fig.colorbar(cax)\n","ax.set_xticks(np.arange(len(fr)))\n","ax.set_yticks(np.arange(len(fr)))\n","ax.set_xticklabels(fr_str)\n","ax.set_yticklabels(fr_str)\n","ax.set_ylabel(\"% Top trigrams in Twitter\", rotation='vertical')\n","ax.set_xlabel(\"% Top trigrams in Facebook\")\n","plt.setp(ax.get_xticklabels(), rotation=90)\n","plt.show()"],"metadata":{"id":"Gy2D0FiTAPg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(tw_data)"],"metadata":{"id":"7R9l8UZSBb-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7_pcn8w6Xrxk"},"execution_count":null,"outputs":[]}]}