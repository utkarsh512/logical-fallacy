{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Sentences from Dependency Parsing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from   matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for plotting\n",
    "sns.set(style='darkgrid')\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 10:39:48.403004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 10:39:49.261909: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-11 10:39:49.262007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-11 10:39:49.262018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-11 10:39:50.038204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/utkarsh-am/opt/openmpi/lib\n",
      "2023-04-11 10:39:50.039405: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.47.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tools/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.24.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tools/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/utkarsh-am/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/tools/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/tools/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Setup for spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "scapy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For caching objects\n",
    "\n",
    "def load_obj(file_path):\n",
    "    \"\"\"Load a pickled object from given path\n",
    "    :param file_path: Path to the pickle file of the object\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, file_path):\n",
    "    \"\"\"Save an object to given path via pickling\n",
    "    :param obj: Object to pickle\n",
    "    :param file_path: Path for pickling\n",
    "    :type file_path: string\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        return pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LOGIC dataset\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train.csv')\n",
    "dev_df   = pd.read_csv('./dataset/dev.csv')\n",
    "test_df  = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts        = list(train_df['text'])\n",
    "train_labels       = list(train_df['label'])\n",
    "train_masked_texts = list(train_df['masked_text'])\n",
    "\n",
    "dev_texts        = list(dev_df['text'])\n",
    "dev_labels       = list(dev_df['label'])\n",
    "dev_masked_texts = list(dev_df['masked_text'])\n",
    "\n",
    "test_texts        = list(test_df['text'])\n",
    "test_labels       = list(test_df['label'])\n",
    "test_masked_texts = list(test_df['masked_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 1849, #test: 300, #dev: 300\n"
     ]
    }
   ],
   "source": [
    "# Load the results from dependency parsing\n",
    "\n",
    "train_parsed, test_parsed, dev_parsed = load_obj('./dataset/dependency_parsing_results.pkl')\n",
    "print(f'#train: {len(train_parsed)}, #test: {len(test_parsed)}, #dev: {len(dev_parsed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the entries in the parsed results are at the same indices as\n",
    "# their corressponding dataframes.\n",
    "\n",
    "for i in range(len(train_texts)):\n",
    "    assert(train_texts[i].lower() == train_parsed[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_structure(parsed_result):\n",
    "    \"\"\"Transforms the parsed_result \n",
    "    \n",
    "    from:\n",
    "        (text, ((path_1, words_1, indices_1, orient_1), (path_2, words_2, indices_2, orient_2), ...))\n",
    "\n",
    "    to:\n",
    "        (text, {\n",
    "            path_i: ((words_1, indices_1, orient_1), (words_2, indices_2, orient_2), ...),\n",
    "        })\n",
    "    \"\"\"\n",
    "    transformed_result = list()\n",
    "\n",
    "    for text, dp in parsed_result:\n",
    "        local_dict = dict()\n",
    "        for path, words, indices, orient in dp:\n",
    "            if path not in local_dict:\n",
    "                local_dict[path] = list()\n",
    "            local_dict[path].append((words, indices, orient))\n",
    "\n",
    "        # We need to sort this dictionary by decreasing size in value list\n",
    "        local_list = list()\n",
    "        for k, v in local_dict.items():\n",
    "            local_list.append((k, len(v)))\n",
    "        local_list = sorted(local_list, key=lambda z: z[1], reverse=True)\n",
    "\n",
    "        ordered_dict = dict()\n",
    "        for k, _ in local_list:\n",
    "            ordered_dict[k] = local_dict[k]\n",
    "\n",
    "        transformed_result.append(ordered_dict)\n",
    "\n",
    "    return transformed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parsed_ord = transform_data_structure(train_parsed)\n",
    "test_parsed_ord  = transform_data_structure(test_parsed)\n",
    "dev_parsed_ord   = transform_data_structure(dev_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auxiliary_text(text, parsed, n=-1, masked=False):\n",
    "    \"\"\"Create auxiliary sentences from dependency parsing\n",
    "\n",
    "    n:\n",
    "        number of auxiliary texts to generate, -1 for all\n",
    "\n",
    "    masked:\n",
    "        if True, uses masks like [MASK_i] to shadow the words that exist along\n",
    "        the dependency path\n",
    "    \"\"\"\n",
    "    doc = scapy_nlp(text.lower())\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    mask = dict()\n",
    "    auxiliary_texts = list()\n",
    "\n",
    "    for path in parsed.keys():\n",
    "        if len(auxiliary_texts) == n:\n",
    "            break\n",
    "\n",
    "        local_list = []\n",
    "\n",
    "        for token in path:\n",
    "            detailed_token = spacy.explain(token)\n",
    "            if detailed_token:\n",
    "                local_list.append(detailed_token)\n",
    "        \n",
    "\n",
    "        auxiliary_texts.append(' leads to '.join(local_list))\n",
    "        \n",
    "        # for words, indices, orient in parsed[path]:\n",
    "\n",
    "        #     if len(auxiliary_texts) == n:\n",
    "        #         break\n",
    "\n",
    "        #     local_list = []\n",
    "\n",
    "        #     if not masked:\n",
    "        #         for j in range(-1, -len(words), -1):\n",
    "        #             t1 = tokens[indices[j]]\n",
    "        #             t2 = tokens[indices[j - 1]]\n",
    "        #             if not orient[j]:\n",
    "        #                 t1, t2 = t2, t1\n",
    "        #             r = spacy.explain(path[j])\n",
    "        #             s = f'{t1} is {r} of {t2}'\n",
    "        #             local_list.append(s)\n",
    "        #     else:\n",
    "        #         for j in range(-1, -len(words), -1):\n",
    "        #             idx_1 = indices[j]\n",
    "        #             idx_2 = indices[j - 1]\n",
    "        #             if idx_1 not in mask:\n",
    "        #                 mask[idx_1] = f'[MASK_{len(mask)}]'\n",
    "        #                 tokens[idx_1] = mask[idx_1]\n",
    "        #             if idx_2 not in mask:\n",
    "        #                 mask[idx_2] = f'[MASK_{len(mask)}]'\n",
    "        #                 tokens[idx_2] = mask[idx_2]\n",
    "        #             m1 = mask[idx_1]\n",
    "        #             m2 = mask[idx_2]\n",
    "        #             if not orient[j]:\n",
    "        #                 m1, m2 = m2, m1\n",
    "        #             r = spacy.explain(path[j])\n",
    "        #             s = f'{m1} is {r} of {m2}'\n",
    "        #             local_list.append(s)\n",
    "                    \n",
    "        #     auxiliary_texts.append(' and '.join(local_list))\n",
    "\n",
    "    main_txt = ' '.join(tokens)\n",
    "    aux_txt  = '. '.join(auxiliary_texts) + '.'\n",
    "    \n",
    "    return main_txt, aux_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"you oppose a senator 's proposal to extend government - funded health care \"\n",
      " 'to poor minority children because that senator is a liberal democrat .')\n",
      "'adverbial clause modifier leads to nominal subject.'\n"
     ]
    }
   ],
   "source": [
    "main_text, aux_text = create_auxiliary_text(train_texts[11], train_parsed_ord[11], masked=False, n=1)\n",
    "\n",
    "pprint(main_text)\n",
    "pprint(aux_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auxiliary_texts(texts, parsed_ord, n=-1, masked=False):\n",
    "    \"\"\"Create auxiliary sentences \n",
    "\n",
    "    texts:\n",
    "        train_texts / test_texts / dev_texts\n",
    "\n",
    "    parsed_ord:\n",
    "        train_parsed_ord / test_parsed_ord / dev_parsed_ord\n",
    "\n",
    "    n:\n",
    "        number of auxiliary texts to generate per entry, -1 for all\n",
    "\n",
    "    masked:\n",
    "        if True, uses masks like [MASK_i] to shadow the words that exist along\n",
    "        the dependency path\n",
    "    \"\"\"\n",
    "    assert(len(texts) == len(parsed_ord))\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for text, parsed in zip(texts, parsed_ord):\n",
    "        result.append(create_auxiliary_text(text, parsed, n, masked))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [01:57<03:16, 39.37s/it]/home/utkarsh-am/.local/lib/python3.8/site-packages/spacy/glossary.py:19: UserWarning: [W118] Term 'predet' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "100%|██████████| 8/8 [05:15<00:00, 39.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create auxiliary sentences using top-1, top-2, top-3, top-4, top-5, top-10,\n",
    "# top-20 and all dependency paths, both with and without masking.\n",
    "\n",
    "N = [1, 2, 3, 4, 5, 10, 20, -1]\n",
    "\n",
    "for n in tqdm(N):\n",
    "    for masked in (False, True):\n",
    "        train_aux = create_auxiliary_texts(train_texts, train_parsed_ord, n=n, masked=masked)\n",
    "        test_aux  = create_auxiliary_texts(test_texts,  test_parsed_ord,  n=n, masked=masked)\n",
    "        dev_aux   = create_auxiliary_texts(dev_texts,   dev_parsed_ord,   n=n, masked=masked)\n",
    "        result = (train_aux, test_aux, dev_aux)\n",
    "        save_obj(result, f'./dataset/aux-sentences-n-{n}-masked-{masked}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
