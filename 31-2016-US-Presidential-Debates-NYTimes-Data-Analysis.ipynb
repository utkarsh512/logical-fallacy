{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"31-2016-US-Presidential-Debates-NYTimes-Data-Analysis.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOia3jB4DlYU4LZ2xvFH05d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Analyzing The New York Times FB page activity during 2016 US Presidential Debates\n","\n"],"metadata":{"id":"oqT9LHeylUTm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nsgp9Z_VXKAI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/DL/Facebook/fbscraper/nytimes/2016"],"metadata":{"id":"wDDSxW4eYkSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle as pkl\n","import pandas as pd \n","import numpy as np\n","import nltk\n","import re\n","import matplotlib.pyplot as plt\n","nltk.download('punkt') # For tokenizers\n","from nltk.tokenize import TweetTokenizer\n","import urllib.parse\n","from tqdm import tqdm"],"metadata":{"id":"H9ZxJpNuYzTM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading the posts into the memory \n","\n","posts = []\n","\n","with open(\"2016pre.pkl\", \"rb\") as f:\n","    try:\n","        while True:\n","            posts.append(pkl.load(f))\n","    except EOFError:\n","        pass\n","\n","with open(\"2016post.pkl\", \"rb\") as f:\n","    try:\n","        while True:\n","            posts.append(pkl.load(f))\n","    except EOFError:\n","        pass"],"metadata":{"id":"oiUQu0b5Y3de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(posts)"],"metadata":{"id":"3AEdxtArZD-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["posts[0] # how post is stored in the list"],"metadata":{"id":"8DgOrx8RZbZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transitioning from post-level data to entry-level data\n","\n","texts = [] # {text, authorName, authorURL, time, type}\n","\n","err = 0\n","\n","for post in posts:\n","    try:\n","        texts.append(dict(\n","            text=post[\"text\"],\n","            authorName=post[\"author\"][\"name\"],\n","            authorURL=post[\"author\"][\"url\"],\n","            time_=post[\"time\"],\n","            type_=\"post\"\n","        ))\n","    except:\n","        err += 1\n","    for comment in post[\"comments\"]:\n","        try:\n","            texts.append(dict(\n","                text=comment[\"text\"],\n","                authorName=comment[\"author\"][\"name\"],\n","                authorURL=comment[\"author\"][\"url\"],\n","                time_=post[\"time\"],\n","                type_=\"comment\"\n","            ))\n","        except:\n","            err += 1\n","        for reply in comment[\"replies\"]:\n","            try:\n","                texts.append(dict(\n","                    text=reply[\"text\"],\n","                    authorName=reply[\"author\"][\"name\"],\n","                    authorURL=reply[\"author\"][\"url\"],\n","                    time_=post[\"time\"],\n","                    type_=\"reply\"\n","                ))\n","            except:\n","                err += 1"],"metadata":{"id":"ZyeChr1uZHJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts), err"],"metadata":{"id":"bleZ4U2HZPy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(texts)"],"metadata":{"id":"UklA1qOsa4j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"wIFDAJRVbTIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pre-processing text\n","\n","tknz = TweetTokenizer()\n","\n","def cleanText(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return ' '.join(wordTokens)"],"metadata":{"id":"GBC_bAJHbUIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"processedText\"] = df[\"text\"].apply(cleanText)"],"metadata":{"id":"7Jf-EvLhbhh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"Si5uHyy3bmgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extracting unique ID for the users\n","\n","def extractUid(url):\n","    url = url[25:]\n","    url = url.replace('/', '')\n","    url = url.split('?')\n","    if url[0] != 'profile.php':\n","        return url[0]\n","    assert len(url) == 2\n","    foo = urllib.parse.parse_qs(url[1])\n","    return foo['id'][0]"],"metadata":{"id":"mDernd7kiJcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"authorID\"] = df[\"authorURL\"].apply(extractUid)"],"metadata":{"id":"Mmkk2QtziORm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"m7XOEAXiiX7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"2016.csv\")"],"metadata":{"id":"t3acvShFbrhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nComments = []\n","nReplies = []\n","\n","for post in posts:\n","    nComments.append(len(post[\"comments\"]))\n","    for comment in post[\"comments\"]:\n","        nReplies.append(len(comment[\"replies\"]))"],"metadata":{"id":"_Ql6e17vb9Iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nComments = np.array(nComments)\n","nReplies = np.array(nReplies)"],"metadata":{"id":"wb-oNrJBcBwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nCommentsAvg = np.average(nComments)\n","nCommentsMed = np.median(nComments)\n","nRepliesAvg = np.average(nReplies)\n","nRepliesMed = np.median(nReplies)\n","print(nCommentsAvg, nCommentsMed, nRepliesAvg, nRepliesMed)"],"metadata":{"id":"YJHJ56ZacFO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["authorID = df['authorID'].tolist()"],"metadata":{"id":"2MEmC7TQcHxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(authorID)"],"metadata":{"id":"I9llcmugeCmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for extracting unique users\n","\n","authorIDDist = dict()\n","\n","for ID in authorID: \n","    try: \n","        authorIDDist[ID] += 1 \n","    except KeyError: \n","        authorIDDist[ID] = 1"],"metadata":{"id":"T34Wb4tWeMgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(authorIDDist)"],"metadata":{"id":"OkoaiKL6i6vL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["authorIDDistSorted = list()\n","\n","for k, v in authorIDDist.items():\n","    authorIDDistSorted.append((v, k))\n","authorIDDistSorted = sorted(authorIDDistSorted, reverse=True) \n","\n","for entryCount, ID in authorIDDistSorted[:10]:\n","    print(f'{ID:30} - {entryCount}')"],"metadata":{"id":"F9ocx36ijI7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uCQnHO5Xk8Ls"},"execution_count":null,"outputs":[]}]}