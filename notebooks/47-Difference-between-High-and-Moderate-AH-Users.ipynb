{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"47-Difference-between-High-and-Moderate-AH-Users.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5NiQERrfzOfSIuVoQa/A2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup environment"],"metadata":{"id":"gY5PrH-Rpb4k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-j-hgd6KoG2e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"Eg2z7O3DoWds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   pprint                   import pprint\n","matplotlib.rcParams.update({'font.size': 18})"],"metadata":{"id":"qHsV_pvjoloS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Helper functions"],"metadata":{"id":"8R8pw2wspp-M"}},{"cell_type":"code","source":["tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"N8eB1QUdpQZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUR_WORDS = {\n","  \"jews\": [\n","    \"jews\",\n","    \"oven dodger\",\n","    \"nazi\",\n","    \"dirty jew\",\n","    \"holocaust\",\n","    \"kikesucker\",\n","    \"hook nose\",\n","    \"kike\"\n","  ],\n","  \"homosexual\": [\n","    \"faggots usually\",\n","    \"fucking queer\",\n","    \"the biggest faggot\",\n","    \"dyke\",\n","    \"you fucking faggot\",\n","    \"hate faggots\",\n","    \"queer\",\n","    \"homosexual\",\n","    \"the faggots\",\n","    \"faggot\",\n","    \"faggots usually have\",\n","    \"gay\",\n","    \"faggots\",\n","    \"dykey\",\n","    \"ugly dyke\",\n","    \"faggots like you\",\n","    \"you a fag\",\n","    \"lesbian\",\n","    \"homo\",\n","    \"is a faggot\",\n","    \"like a faggot\",\n","    \"dykes\",\n","    \"faggots like\",\n","    \"faggot if you ever\"\n","  ],\n","  \"women\": [\n","    \"ugly dyke\",\n","    \"woman terrorist\",\n","    \"nigress\",\n","    \"bitch\",\n","    \"slut\",\n","    \"women\",\n","    \"sheeboon\",\n","    \"negress\",\n","    \"mud shark\",\n","    \"women threat\",\n","    \"you a lame bitch\",\n","    \"your a cunt\",\n","    \"white bitch\",\n","    \"niggeress\",\n","    \"hoe\",\n","    \"dykes\",\n","    \"niggress\",\n","    \"sheboon\",\n","    \"feminazi\"\n","  ],\n","  \"blacks\": [\n","    \"pavement ape\",\n","    \"the niggers\",\n","    \"negress\",\n","    \"porch monkey\",\n","    \"that nigger\",\n","    \"this nigger\",\n","    \"sheboon\",\n","    \"all niggers\",\n","    \"eurafrica\",\n","    \"shut up nigger\",\n","    \"picaninny\",\n","    \"african attack\",\n","    \"spearchucker\",\n","    \"how many niggers\",\n","    \"nigger\",\n","    \"africa\",\n","    \"niggers are in my\",\n","    \"dindu nuffin\",\n","    \"stupid nigger\",\n","    \"moolie\",\n","    \"niggers\",\n","    \"bluegum\",\n","    \"nigger ass\",\n","    \"you niggers\",\n","    \"fucking nigger\",\n","    \"nigger music\",\n","    \"niggress\",\n","    \"you a nigger\",\n","    \"many niggers are\",\n","    \"nigress\",\n","    \"blacks\",\n","    \"teenaper\",\n","    \"sheeboon\",\n","    \"dumb nigger\",\n","    \"niggeress\",\n","    \"pickaninny\",\n","    \"nigga\"\n","  ],\n","  \"muslim\": [\n","    \"muslim immigrant\",\n","    \"islam\",\n","    \"mudslime\",\n","    \"mooslem\",\n","    \"muslim refugee\",\n","    \"musslime\",\n","    \"shitlam\",\n","    \"muslim invasion\",\n","    \"moslime\",\n","    \"mooslamic\",\n","    \"muzzie\",\n","    \"allah akbar\",\n","    \"mooslime\",\n","    \"musloid\",\n","    \"mudslimes\",\n","    \"muslim\",\n","    \"muslimes\",\n","    \"moslum\",\n","    \"mussie\",\n","    \"muzrat\",\n","    \"muslim countries\",\n","    \"muzzy\",\n","    \"moslim\",\n","    \"jihadi\",\n","    \"muslim country\",\n","    \"moslem\",\n","    \"muzzrat\",\n","    \"mooslim\"\n","  ],\n","  \"arabs\": [\n","    \"towel head\",\n","    \"goatfucker\",\n","    \"arabs\",\n","    \"goathumper\",\n","    \"raghead\",\n","    \"rag head\",\n","    \"goathumping\",\n","    \"towelhead\",\n","    \"camel jockey\",\n","    \"sandnigger\",\n","    \"camel fucker\",\n","    \"sand nigger\"\n","  ],\n","  \"generic\": [\n","    \"to rape\",\n","    \"raped and\",\n","    \"shithole country\",\n","    \"get raped\",\n","    \"raped\",\n","    \"is a fucking\",\n","    \"shit skin\",\n","    \"raped by\",\n","    \"hate you\",\n","    \"fake empowerment\",\n","    \"abusive women\",\n","    \"fuck you too\",\n","    \"violence\",\n","    \"wit a lame nigga\",\n","    \"they all look\",\n","    \"alllivesmatter\",\n","    \"shithole countries\",\n","    \"fucking hate\",\n","    \"trailer trash\",\n","    \"kill all\",\n","    \"terrorist threat\",\n","    \"harassment\",\n","    \"kill yourself\",\n","    \"shitskin\",\n","    \"okay to be white\",\n","    \"fucking hate you\"\n","  ],\n","  \"white\": [\n","    \"full of white\",\n","    \"white trash\",\n","    \"white devil\",\n","    \"white\",\n","    \"are all white\",\n","    \"white boy\",\n","    \"white ass\",\n","    \"white bitch\",\n","    \"hillbilly\",\n","    \"whigger\",\n","    \"white christian\",\n","    \"white person\",\n","    \"all white\",\n","    \"white nigger\",\n","    \"redneck\",\n","    \"white honky\",\n","    \"wigger\",\n","    \"them white\"\n","  ],\n","  \"economy\": [\n","    \"ghetto\"\n","  ],\n","  \"immigrant\": [\n","    \"illegal immigrants\",\n","    \"immigrant not welcome\",\n","    \"immigrant terror\",\n","    \"mexcrement\",\n","    \"go back to where you come from\",\n","    \"muslim refugee\",\n","    \"illegal aliens\",\n","    \"refugee\",\n","    \"protect from immigrants\",\n","    \"negro\",\n","    \"refugees\",\n","    \"immigrant\",\n","    \"refugee invasion\",\n","    \"go back to where they come from\",\n","    \"refugees impact\",\n","    \"bring ebola\",\n","    \"immigrants\",\n","    \"illegal alien\",\n","    \"immigrant invasion\",\n","    \"bring disease\"\n","  ],\n","  \"mental\": [\n","    \"retard\",\n","    \"mongoloid\",\n","    \"retarded\"\n","  ],\n","  \"asians\": [\n","    \"asians\",\n","    \"ching chong\",\n","    \"chinaman\"\n","  ]\n","}"],"metadata":{"id":"2r5wSgQxqNC6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading CreateDebate dataset"],"metadata":{"id":"oo8Lh02lqc_a"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"Kt0p5a9WqUtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append(v)\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append(v)\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"WJMTOtYqsv7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analyzing dataset for slur words across ad hominem score"],"metadata":{"id":"siDsVN1LJkEJ"}},{"cell_type":"code","source":["ah_score = dict()\n","slur_count = dict()\n","\n","# addressing order: \n","#   ah_score: \n","#       key: category -> author\n","#       value: list of score of each comment written\n","#\n","#   slur_count:\n","#       key: category -> slur_group -> author \n","#       value: list of slur count of each comment written"],"metadata":{"id":"fhVr_wNoujF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in categories_selected:\n","    ah_score[cat] = dict()\n","    slur_count[cat] = dict()\n","\n","    for slur_group in SLUR_WORDS.keys():\n","        slur_count[cat][slur_group] = dict()\n","\n","    for comment in tqdm(comments[cat]): \n","        author = comment['author']\n","        if author not in ah_score[cat]:\n","            ah_score[cat][author] = list()\n","            for slur_group in SLUR_WORDS.keys():\n","                slur_count[cat][slur_group][author] = list()\n","        ah_score[cat][author].append(1 - comment['score'])\n","        cleaned_comment_text = clean_text(comment['body'])\n","        for slur_group, slur_words in SLUR_WORDS.items():\n","            current_count = sum([cleaned_comment_text.count(slur_word) for slur_word in slur_words])\n","            slur_count[cat][slur_group][author].append(current_count)"],"metadata":{"id":"Dfem4yAU0BkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Median ah score per category per author\n","#   key: category -> author\n","#   value: median ah score\n","ah_score_median = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_median[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_median[category][author] = np.median(ah_scores)"],"metadata":{"id":"-_2wtIqK3HQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["HIGH_AH_THRESHOLD = 0.8\n","MODERATE_AH_THRESHOLD = 0.2\n","\n","high_ah_group = dict()\n","moderate_ah_group = dict()\n","\n","# key: category -> slur_group\n","# value: list of slur_count"],"metadata":{"id":"j2Rr6Rxb9fD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for category in categories_selected:\n","    high_ah_group[category] = dict()\n","    moderate_ah_group[category] = dict()\n","\n","    for slur_group in SLUR_WORDS.keys():\n","        high_ah_group[category][slur_group] = list()\n","        moderate_ah_group[category][slur_group] = list()\n","\n","    for author, median_ah_score in ah_score_median[category].items():\n","        if median_ah_score < MODERATE_AH_THRESHOLD:\n","            continue\n","        elif median_ah_score > HIGH_AH_THRESHOLD:\n","            for slur_group in SLUR_WORDS.keys():\n","                high_ah_group[category][slur_group].append(np.sum(slur_count[category][slur_group][author]) / len(ah_score[category][author]))\n","        else:\n","            for slur_group in SLUR_WORDS.keys():\n","                moderate_ah_group[category][slur_group].append(np.sum(slur_count[category][slur_group][author]) / len(ah_score[category][author]))"],"metadata":{"id":"_0OQRI6E91v4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How many users considered for each case\n","\n","report = ' ' * 20 + ', '\n","for category in categories_selected:\n","    report += f'{category:>20}, '\n","for slur_group in SLUR_WORDS.keys():\n","    report += '\\n' + f'{slur_group:20}, '\n","    for category in categories_selected:\n","        report += f'{len(high_ah_group[category][slur_group]):>10}/{len(moderate_ah_group[category][slur_group]):<9}, '\n","print(report)"],"metadata":{"id":"7YiNTvPLfv-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plotting utilities"],"metadata":{"id":"aK0PNhy6Jsi_"}},{"cell_type":"code","source":["def plot_category(category, func=np.average):\n","    x = list(SLUR_WORDS.keys())\n","    y1 = []\n","    y2 = []\n","\n","    for slur_group in x:\n","        y1.append(func(high_ah_group[category][slur_group]))\n","        y2.append(func(moderate_ah_group[category][slur_group]))\n","    ticks = np.arange(len(x))\n","    width = 0.35\n","\n","    fig, ax = plt.subplots(figsize=(10, 5))\n","    subplot1 = ax.bar(ticks - width/2, y1, width, label='High', tick_label=x)\n","    subplot2 = ax.bar(ticks + width/2, y2, width, label='Moderate', tick_label=x)\n","    ax.set_ylabel('Scores')\n","    ax.set_title(f'Slur word distribution for \\'{category}\\' forum')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()\n","\n","def plot_slurs(slur_group, func=np.average):\n","    x = categories_selected\n","    y1 = []\n","    y2 = []\n","\n","    for category in x:\n","        y1.append(func(high_ah_group[category][slur_group]))\n","        y2.append(func(moderate_ah_group[category][slur_group]))\n","    ticks = np.arange(len(x))\n","    width = 0.35\n","\n","    fig, ax = plt.subplots(figsize=(10, 5))\n","    subplot1 = ax.bar(ticks - width/2, y1, width, label='High', tick_label=x)\n","    subplot2 = ax.bar(ticks + width/2, y2, width, label='Moderate', tick_label=x)\n","    ax.set_ylabel('Scores')\n","    ax.set_title(f'Slur word distribution for \\'{slur_group}\\' slurs')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"JI6KBLdR_H2L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"PoUk7feNJxSC"}},{"cell_type":"markdown","source":["## For given category"],"metadata":{"id":"8p3LcdHOZ6ZM"}},{"cell_type":"code","source":["plot_category('politics2')"],"metadata":{"id":"RUFrlhPiJ5u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_category('religion')"],"metadata":{"id":"FX9gssCSaCaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_category('world')"],"metadata":{"id":"gxfGA6Vzje73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_category('science')"],"metadata":{"id":"X50Dci1AjkzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_category('law')"],"metadata":{"id":"ruOaAfVwjrU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_category('technology')"],"metadata":{"id":"GIV2eW7kjzIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## For given slur group"],"metadata":{"id":"zXI31N6Qj9dn"}},{"cell_type":"code","source":["plot_slurs('jews')"],"metadata":{"id":"UCGQXQqrkAMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('homosexual')"],"metadata":{"id":"uUVMedJTkJIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('women')"],"metadata":{"id":"sRMPo2rqkPUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('blacks')"],"metadata":{"id":"TblGpl09kSdj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('muslim')"],"metadata":{"id":"dP0nYUynkWNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('arabs')"],"metadata":{"id":"vsU8F2YhkaZ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('generic')"],"metadata":{"id":"CTsyIw8-kfrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('white')"],"metadata":{"id":"zhxhtFf5kjvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('economy')"],"metadata":{"id":"PypYa__SkpS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('immigrant')"],"metadata":{"id":"qDP0wZR4ktAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('mental')"],"metadata":{"id":"o_uq-iUQkwaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slurs('asians')"],"metadata":{"id":"5W8BiJHlk0_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"xDYDZfJNJ4f_"},"execution_count":null,"outputs":[]}]}