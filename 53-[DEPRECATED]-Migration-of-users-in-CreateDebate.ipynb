{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["hKRbc7zAdOki","EIUKkFGodSaq","DTAx2HYBsIuO","PBRFdtANn36a","BpR59NK1dlPe","Ymceyvx4dEsv"],"authorship_tag":"ABX9TyNDF92rpwmbDUCTyAxhKoP2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup environment"],"metadata":{"id":"NRtunBMyBUHs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9cHhpnCBQ6H"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"oyXaH4oBBh2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install shifterator"],"metadata":{"id":"JoheETjVjTAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","import shifterator as sh\n","import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"mpCkMDLzBtRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"yzueJ3J7BzIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load CreateDebate dataset"],"metadata":{"id":"LD2YYBAoCBCU"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"yjaUTWLJB7CN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append(v)\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append(v)\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"s1VznzpNCK9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_tstring(tstring):\n","    \"\"\"\n","    Parses comment's time to an integer to enable\n","    comparison between comments based on their time of posting\n","    \"\"\"\n","    if tstring == 'Not Available':\n","        raise ValueError('Invalid posting time for parse_tstring')\n","    tstring = tstring.replace('T', '-').replace(':', '-').replace('+', '-').split('-')\n","    return int(''.join(tstring[:-2]))"],"metadata":{"id":"OTrzxAW0wyl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading AH score\n","\n","with open('/content/gdrive/MyDrive/Temp/47-ah-score.pkl', 'rb') as fp:\n","    ah_score = pickle.load(fp)\n","\n","# `ah_score` is a dictionary that contains the ah score of the comments written\n","# by all the users\n","\n","# key: category -> user\n","# value: list of ah_score for given user for given category\n","\n","# value > 0.5 --> ad hominem\n","# value < 0.5 --> non ad hominem"],"metadata":{"id":"YNlPt2J6CSFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading CreateDebate profile characteristics into dataframe\n","df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)\n","\n","# Extract useful characteristics\n","reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"pG5HaZ-uC63S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def profile_characteristics_stats(user_subset):\n","    \"\"\"\n","    Returns average and standard deviation of characteristics for given subset\n","    of users\n","    \"\"\"\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        try:\n","            rewards_.append(reward_points_map[user])\n","        except:pass\n","        try:\n","            efficiency_.append(efficiency_map[user])\n","        except:pass\n","        try:\n","            n_allies.append(allies_map[user])\n","        except:pass\n","        try:\n","            n_enemies.append(enemies_map[user])\n","        except:pass\n","        try:\n","            n_hostiles.append(hostiles_map[user])\n","        except:pass\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"wwMPqNAnC83H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Median ah score per category per author\n","#   key: category -> author\n","#   value: median ah score\n","\n","ah_score_median = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_median[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_median[category][author] = np.max(ah_scores)"],"metadata":{"id":"Eljptv3NEqNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count = dict()\n","# key: category -> author\n","# value: number of comments written by author in the given forum\n","\n","for category in categories_selected:\n","    comment_count[category] = dict()\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        try:\n","            comment_count[category][author] += 1\n","        except KeyError:\n","            comment_count[category][author] = 1"],"metadata":{"id":"XtbxHUvhF0Oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"OVgU3pMvF5OU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count_user = dict()\n","# key: user -> category -> year (in string)\n","# value: number of comments posted by the user for that category in the given year\n","\n","for user in user_list:\n","    comment_count_user[user] = dict()\n","    for category in categories_selected:\n","        comment_count_user[user][category] = dict()\n","        for year in range(2008, 2022):\n","            syear = str(year)\n","            comment_count_user[user][category][syear] = 0\n","\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        if comment['time'] == 'Not Available':\n","            continue\n","        year = comment['time'][:4]\n","        assert(int(year) < 2022 and int(year) >= 2008)\n","        comment_count_user[ comment['author'] ][ category ][ year ] += 1 "],"metadata":{"id":"Tk3od7lGF81O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_post_time = dict()\n","# key: category -> user\n","# value: post time of the first comment by given user in the given category\n","#        It is an integer as returned by parse_tstring routine\n","\n","for category in categories_selected:\n","    first_post_time[category] = dict()\n","\n","    for comment in comments[category]: \n","        if comment['time'] == 'Not Available':\n","            continue\n","        author = comment['author']\n","        try:\n","            first_post_time[category][author] = min(first_post_time[category][author], parse_tstring(comment['time']))\n","        except KeyError:\n","            first_post_time[category][author] = parse_tstring(comment['time'])"],"metadata":{"id":"MTeBZUrlzFz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_migrated_users(category1, category2):\n","    \"\"\"\n","    Returns a list of <user_name, year_o, year_m> tuple\n","\n","    year_o: Year in which user first posted in category1\n","    year_m: Year in which user first posted in category2\n","\n","    The users in the returned list should have their first post in CreateDebate\n","    in category1\n","    \"\"\"\n","    resultant_list = []\n","\n","    def get_nz_idx(arr):\n","        idx = -1\n","        for i in range(len(arr)):\n","            if arr[i] != 0:\n","                idx = i\n","                break\n","        return idx\n","\n","    for user in user_list:\n","        count_1_ = comment_count[category1].get(user, 0)\n","        count_2_ = comment_count[category2].get(user, 0)\n","\n","        count_1    = [0 for _ in range(2008, 2022)]\n","        count_2    = [0 for _ in range(2008, 2022)]\n","        count_wo_1 = [0 for _ in range(2008, 2022)]\n","\n","        # `count_1` stores number of comments posted by this user in category1\n","        # `count_2` stores number of comments posted by this user in category2\n","        # `count_wo_1` stores number of comments posted by this user in \n","        #   CreateDebate but not in category1\n","\n","        for year in range(2008, 2022):\n","            count_1[year - 2008] += comment_count_user[user][category1][str(year)]\n","            count_2[year - 2008] += comment_count_user[user][category2][str(year)]\n","\n","            for category in categories_selected:\n","                if category == category1:\n","                    continue\n","                count_wo_1[year - 2008] += comment_count_user[user][category][str(year)]\n","\n","        # Do we need to consider this user or not?\n","        idx_nz_cat_1    = get_nz_idx(count_1)      # index of first non-zero entry in count_1\n","        idx_nz_cat_2    = get_nz_idx(count_2)      # index of first non-zero entry in count_2\n","        idx_nz_cat_wo_1 = get_nz_idx(count_wo_1)   # index of first non-zero entry in count_wo_1\n","\n","        # Condition to consider an user:\n","        # 1.    idx_nz_cat_1 != -1 (user has posted atleast one comment in category1)\n","        # 2.    idx_nz_cat_2 != -1 (user has posted atleast one comment in category2)\n","        # 3.    idx_nz_cat_1 < idx_nz_cat_wo_1 (the first post of the user should be in category1)\n","\n","        if count_1_ != 0 and count_2_ != 0: # idx_nz_cat_1 != -1 and idx_nz_cat_2 != -1 and idx_nz_cat_1 < idx_nz_cat_wo_1:\n","            # Consider this user and append it to the list\n","            resultant_list.append((user, idx_nz_cat_1 + 2008, idx_nz_cat_2 + 2008))\n","\n","    return resultant_list"],"metadata":{"id":"fRRkNuxZGBlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_migrated_users(migration_list, category1, category2):\n","    \"\"\"\n","    Partitions the users into 4 categories: \n","        AH-AH\n","        AH-NonAH\n","        NonAH-AH\n","        NonAH-NonAH\n","    \n","    migration_list should be obtained using get_migrated_users method\n","    \"\"\"\n","\n","    ah_ah_list = []\n","    ah_nonah_list = []\n","    nonah_ah_list = []\n","    nonah_nonah_list = []\n","\n","    for entry in migration_list:\n","        median_score_1 = ah_score_median[category1][entry[0]]\n","        median_score_2 = ah_score_median[category2][entry[0]]\n","\n","        if median_score_1 > 0.5 and median_score_2 > 0.5:\n","            ah_ah_list.append(entry)\n","\n","        elif median_score_1 > 0.5 and median_score_2 < 0.5:\n","            ah_nonah_list.append(entry)\n","        \n","        elif median_score_1 < 0.5 and median_score_2 > 0.5:\n","            nonah_ah_list.append(entry)\n","\n","        elif median_score_1 < 0.5 and median_score_2 < 0.5:\n","            nonah_nonah_list.append(entry)\n","        \n","        else:\n","            print(entry)\n","\n","    return ah_ah_list, ah_nonah_list, nonah_ah_list, nonah_nonah_list "],"metadata":{"id":"wj_AqOLONz_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_origin_year(ah_ah, ah_nonah, nonah_ah, nonah_nonah):\n","    \"\"\"\n","    Plot origin year\n","    \"\"\"\n","    cnt_ah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_ah_nonah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_nonah = [0 for _ in range(2008, 2022)]\n","\n","    labels = [_ for _ in range(2008, 2022)]\n","\n","    for user, year_o, year_m in ah_ah:\n","        cnt_ah_ah[year_o - 2008] += 1\n","    for user, year_o, year_m in ah_nonah:\n","        cnt_ah_nonah[year_o - 2008] += 1\n","    for user, year_o, year_m in nonah_ah:\n","        cnt_nonah_ah[year_o - 2008] += 1\n","    for user, year_o, year_m in nonah_nonah:\n","        cnt_nonah_nonah[year_o - 2008] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width - width / 2, cnt_ah_ah, width, label='AH -> AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks - width / 2, cnt_ah_nonah, width, label='AH -> NonAH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width / 2, cnt_nonah_ah, width, label='NonAH -> AH', tick_label=labels)\n","    subplot4 = ax.bar(ticks + width + width / 2, cnt_nonah_nonah, width, label='NonAH -> NonAH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Year of joining forum 1')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"GbojyAvjQq-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_migration_year(ah_ah, ah_nonah, nonah_ah, nonah_nonah):\n","    \"\"\"\n","    Plot migration year\n","    \"\"\"\n","    cnt_ah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_ah_nonah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_nonah = [0 for _ in range(2008, 2022)]\n","\n","    labels = [_ for _ in range(2008, 2022)]\n","\n","    for user, year_o, year_m in ah_ah:\n","        cnt_ah_ah[year_m - 2008] += 1\n","    for user, year_o, year_m in ah_nonah:\n","        cnt_ah_nonah[year_m - 2008] += 1\n","    for user, year_o, year_m in nonah_ah:\n","        cnt_nonah_ah[year_m - 2008] += 1\n","    for user, year_o, year_m in nonah_nonah:\n","        cnt_nonah_nonah[year_m - 2008] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width - width / 2, cnt_ah_ah, width, label='AH -> AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks - width / 2, cnt_ah_nonah, width, label='AH -> NonAH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width / 2, cnt_nonah_ah, width, label='NonAH -> AH', tick_label=labels)\n","    subplot4 = ax.bar(ticks + width + width / 2, cnt_nonah_nonah, width, label='NonAH -> NonAH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Year of joining forum 2')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"Fbeaq3fZcloi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_delta_year(ah_ah, ah_nonah, nonah_ah, nonah_nonah):\n","    \"\"\"\n","    Plot migration year - origin year\n","    \"\"\"\n","    cnt_ah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_ah_nonah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_ah = [0 for _ in range(2008, 2022)]\n","    cnt_nonah_nonah = [0 for _ in range(2008, 2022)]\n","\n","    labels = [_ for _ in range(14)]\n","\n","    for user, year_o, year_m in ah_ah:\n","        cnt_ah_ah[year_m - year_o] += 1\n","    for user, year_o, year_m in ah_nonah:\n","        cnt_ah_nonah[year_m - year_o] += 1\n","    for user, year_o, year_m in nonah_ah:\n","        cnt_nonah_ah[year_m - year_o] += 1\n","    for user, year_o, year_m in nonah_nonah:\n","        cnt_nonah_nonah[year_m - year_o] += 1\n","\n","    ticks = np.arange(len(labels))\n","    width = 0.20\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width - width / 2, cnt_ah_ah, width, label='AH -> AH', tick_label=labels)\n","    subplot2 = ax.bar(ticks - width / 2, cnt_ah_nonah, width, label='AH -> NonAH', tick_label=labels)\n","    subplot3 = ax.bar(ticks + width / 2, cnt_nonah_ah, width, label='NonAH -> AH', tick_label=labels)\n","    subplot4 = ax.bar(ticks + width + width / 2, cnt_nonah_nonah, width, label='NonAH -> NonAH', tick_label=labels)\n","\n","    ax.set_ylabel('#users')\n","    ax.set_xlabel('Time difference in years')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(labels, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"0eGRUmYBcwd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reader_addr = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/threads.log'\n","reader = open(reader_addr, 'rb')\n","threads = []\n","try:\n","    while True:\n","        e = pickle.load(reader)\n","        threads.append(e)\n","except:\n","    reader.close()"],"metadata":{"id":"7ZHsiICOm5Sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_graph(user_subset, n1 = 0, n2 = 0):\n","    \"\"\"Builds support graph and dispute graph from hyper-parameters n1 and n2\n","    inputs\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    output\n","    (author_map : dict, reverse_map : list, author_count : int, support_graph : nx.DiGraph, support_matrix: list, dispute_graph : nxDiGraph, dispute_matrix : list)\n","    \"\"\"\n","\n","    athr = dict()\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    athr = dict()\n","\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","        cur_author = cmntMap[cid].author\n","\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    author_map = dict()\n","    reverse_map = [\"\"] * len(A)\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","\n","    support_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","    dispute_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        cur_pol = cmntMap[cid].polarity\n","        \n","        if cur_author in author_map and cur_pol != 'Not Available':\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                nxt_pol = cmntMap[key].polarity\n","                if nxt_author in author_map and nxt_pol != 'Not Available':\n","                    nxt_author_id = author_map[nxt_author]\n","                    if cur_pol == nxt_pol:\n","                        support_matrix[nxt_author_id][cur_author_id] += 1\n","                    else:\n","                        dispute_matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","\n","    support_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if support_matrix[i][j] != 0:\n","                support_graph.add_weighted_edges_from([(i, j, support_matrix[i][j])])\n","\n","    dispute_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if dispute_matrix[i][j] != 0:\n","                dispute_graph.add_weighted_edges_from([(i, j, dispute_matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, support_graph, support_matrix, dispute_graph, dispute_matrix)"],"metadata":{"id":"Lxyxtc_-npPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_map, user_reverse_map, user_count, support_graph, support_matrix, dispute_graph, dispute_matrix = build_graph(user_list)"],"metadata":{"id":"7LGnApYKqpV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["support_centrality_dict = nx.algorithms.centrality.degree_centrality(support_graph)\n","dispute_centrality_dict = nx.algorithms.centrality.degree_centrality(dispute_graph)"],"metadata":{"id":"GaG45TpZrZcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_centrality_stats(user_subset):\n","    s_c = []\n","    d_c = []\n","\n","    for user in user_subset:\n","        try:\n","            s_c.append(support_centrality_dict[user_map[user]])\n","        except:\n","            pass\n","        try:\n","            d_c.append(dispute_centrality_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(s_c), np.std(s_c), np.average(d_c), np.std(d_c)"],"metadata":{"id":"Wh1-9Cb3rgGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_wordcloud(user_name, categories, time_1, time_2):\n","    texts = []\n","\n","    for category in categories:\n","        for comment in comments[category]:\n","            if comment['author'] != user_name:\n","                continue\n","            if comment['time'] == 'Not Available':\n","                continue\n","            time_ = comment['time'][:10]\n","            if time_ < time_1 or time_ >= time_2:\n","                continue\n","            cleaned_comment_body = ' '.join(clean_text(comment['body']))\n","            texts.append(cleaned_comment_body)\n","\n","    texts = ' '.join(texts)\n","    word_cloud = wordcloud.WordCloud(stopwords=STOP_WORDS,\n","                                     collocations=False).generate(texts)\n","\n","    plt.imshow(word_cloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","# plot_wordcloud('excon', ['politics2'], '2018-01-01', '2019-01-01')"],"metadata":{"id":"eCjszYBFceYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_list = []\n","\n","for year in range(2008, 2022):\n","    for month in range(1, 10):\n","        time_list.append(f'{year}-0{month}-01')\n","    for month in range(10, 13):\n","        time_list.append(f'{year}-{month}-01')"],"metadata":{"id":"7QxFKb9hci20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_time_idx(time_):\n","    for i, x in enumerate(time_list):\n","        if x == time_:\n","            return i\n","    return -1"],"metadata":{"id":"fEPcdlOgclYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Jensen Shannon Divergence\n","\n","def plot_js_div(username, categories1, categories2):\n","    text1 = []\n","    text2 = []\n","\n","    for category in categories1:\n","        for comment in comments[category]:\n","            user = comment['author']\n","            if user != username:\n","                continue\n","            text1.append(comment['body'])\n","    \n","    for category in categories2:\n","        for comment in comments[category]:\n","            user = comment['author']\n","            if user != username:\n","                continue\n","            text2.append(comment['body'])\n","    \n","    text1 = ' '.join(text1)\n","    text2 = ' '.join(text2)\n","    word_tokens_1 = clean_text(text1)\n","    word_tokens_2 = clean_text(text2)\n","\n","    dict1 = dict()\n","    dict2 = dict()\n","\n","    for token in word_tokens_1:\n","        try:\n","            dict1[token] += 1\n","        except KeyError: \n","            dict1[token] = 1\n","    \n","    for token in word_tokens_2: \n","        try: \n","            dict2[token] += 1\n","        except KeyError: \n","            dict2[token] = 1\n","        \n","    sh_instance = sh.JSDivergenceShift(type2freq_1=dict1,\n","                                       type2freq_2=dict2,\n","                                       weight_1=0.5,\n","                                       weight_2=0.5,\n","                                       base=2,\n","                                       alpha=1)\n","    \n","    sh_instance.get_shift_graph(title='Jensen-Shannon Divergence Shifts')"],"metadata":{"id":"3uvY2BuIjq9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"hKRbc7zAdOki"}},{"cell_type":"code","source":["category1 = 'politics2'\n","category2 = 'religion'\n","\n","############################\n","category1_ = 'politics2'"],"metadata":{"id":"S3wEqZpEdMyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["migration_list = get_migrated_users(category1, category2)\n","#migration_list_ = get_migrated_users(category1_, category2)"],"metadata":{"id":"_ax90L4RgVhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["partitions = partition_migrated_users(migration_list, category1, category2)\n","#partitions_ = partition_migrated_users(migration_list_, category1_, category2)"],"metadata":{"id":"q08ef6BSdljx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for i in range(4):\n","#     partitions[i].extend(partitions_[i])"],"metadata":{"id":"pvrXf3Yt4UVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","    print(len(partitions[i]))"],"metadata":{"id":"WDr45IOyhq4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_origin_year(*partitions)"],"metadata":{"id":"InCQp0Tkec02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_migration_year(*partitions)"],"metadata":{"id":"Ty-4U6INf2oN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_delta_year(*partitions)"],"metadata":{"id":"9nGrUSsei_zC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_ah_list, ah_nonah_list, nonah_ah_list, nonah_nonah_list = partitions"],"metadata":{"id":"pLEr8fP9jMJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_list = ah_ah_list + ah_nonah_list \n","\n","nonah_list = nonah_ah_list + nonah_nonah_list"],"metadata":{"id":"-jIo7fgeqemU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Wordcloud Generation"],"metadata":{"id":"EIUKkFGodSaq"}},{"cell_type":"markdown","source":["### NonAH - AH wordcloud"],"metadata":{"id":"DTAx2HYBsIuO"}},{"cell_type":"code","source":["for x in nonah_ah_list:\n","    print(x)\n","    user = x[0]\n","    print(comment_count['politics2'].get(user, 0) + comment_count['religion'].get(user, 0))\n","    print(comment_count['world'][user])"],"metadata":{"id":"SfjZeM53sOnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Brylos',\n","                    categories=['religion', 'politics2'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"1BTdtqz8uRed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Brylos',\n","                    categories=['world'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"tn66j87-tff2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_js_div('Brylos', ['religion', 'politics2'], ['world'])"],"metadata":{"id":"eIwjwVDHsVzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AH - NonAH wordcloud"],"metadata":{"id":"PBRFdtANn36a"}},{"cell_type":"code","source":["for x in ah_nonah_list:\n","    print(x)\n","    user = x[0]\n","    print(comment_count['politics2'].get(user, 0) + comment_count['religion'].get(user, 0))\n","    print(comment_count['world'][user])"],"metadata":{"id":"-Jzteqy3n3W7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Cinder000',\n","                    categories=['religion', 'politics2'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"4fi1s-3ArXUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='Cinder000',\n","                    categories=['world'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"biTGfZBeqE_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_js_div('Cinder000', ['religion', 'politics2'], ['world'])"],"metadata":{"id":"E-3ZrwjooKS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AH-AH wordcloud"],"metadata":{"id":"BpR59NK1dlPe"}},{"cell_type":"code","source":["for x in ah_ah_list:print(x[0], comment_count['religion'].get(x[0], 0) + comment_count['politics2'].get(x[0], 0), comment_count['world'][x[0]])"],"metadata":{"id":"5M9OryiOdSGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_js_div('BrontoLite', ['religion', 'politics2'], ['world'])"],"metadata":{"id":"LmSoxj5UmIPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='BrontoLite',\n","                    categories=['religion', 'politics2'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"ll4uboNfd83g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = find_time_idx('2008-01-01')\n","while (idx < len(time_list)):\n","    try:\n","        plot_wordcloud(user_name='BrontoLite',\n","                    categories=['world'],\n","                    time_1=time_list[idx],\n","                    time_2=time_list[idx + 1])\n","        print(time_list[idx])\n","        print()\n","    except:\n","        pass\n","    finally:\n","        idx += 1"],"metadata":{"id":"41OxlPIxhSAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Average and Std. Deviation of characteristics across different groups"],"metadata":{"id":"Ymceyvx4dEsv"}},{"cell_type":"code","source":["def get_users(migration_list):\n","    users = []\n","    for user, year_o, year_m in migration_list:\n","        users.append(user)\n","    return users"],"metadata":{"id":"JZUdy_0PkVlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_ah_users = get_users(ah_ah_list)\n","ah_nonah_users = get_users(ah_nonah_list)\n","nonah_ah_users = get_users(nonah_ah_list)\n","nonah_nonah_users = get_users(nonah_nonah_list)\n","\n","ah_users = get_users(ah_list)\n","nonah_users = get_users(nonah_list)"],"metadata":{"id":"ue83-lsqkpY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ah -> ah\n","avgs, stds = profile_characteristics_stats(ah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"IIv5GaCWq04D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ah -> ah\n","avgs, stds = profile_characteristics_stats(nonah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"8N29ncJ7rr-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ah -> ah\n","avgs, stds = profile_characteristics_stats(ah_ah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"oGG_3a3-k3nD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ah -> nonah\n","avgs, stds = profile_characteristics_stats(ah_nonah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"G5Ti75sulNM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nonah -> ah\n","avgs, stds = profile_characteristics_stats(nonah_ah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"XeBYoBtAmIuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nonah -> nonah\n","avgs, stds = profile_characteristics_stats(nonah_nonah_users)\n","print(avgs)\n","print(stds)"],"metadata":{"id":"UGxYC_xKmrk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(ah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(ah_users))"],"metadata":{"id":"pvP45FaTsFmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(nonah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(nonah_users))"],"metadata":{"id":"oygwThWSsp2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(ah_ah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(ah_ah_users))"],"metadata":{"id":"jvtj3OH1n3gc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(ah_nonah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(ah_nonah_users))"],"metadata":{"id":"-XUQCP0BohZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(nonah_ah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(nonah_ah_users))"],"metadata":{"id":"gYwG8EZgtHnz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_1, _2, _3, support_graph_, _4, dispute_graph_, _6 = build_graph(nonah_nonah_users)\n","\n","try:\n","    support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","except:\n","    support_graph_r = -1\n","\n","try:\n","    dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","except:\n","    dispute_graph_r = -1\n","\n","print('Support graph reciprocity', support_graph_r)\n","print('Dispute graph reciprocity', dispute_graph_r)\n","\n","print(get_centrality_stats(nonah_nonah_users))"],"metadata":{"id":"tur8jtKDt-Yq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ijRzcnz3uKa7"},"execution_count":null,"outputs":[]}]}