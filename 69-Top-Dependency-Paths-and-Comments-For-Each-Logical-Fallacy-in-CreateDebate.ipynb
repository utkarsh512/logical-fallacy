{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/oHW43EAKfO9qO1aeobrg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Objective\n","\n","- In this notebook, we will be using the top five logical fallacy (on the basis of `%comment`).\n","\n","- For each logical fallacy, we will analyze the top five dependency paths. For each dependency path, we will see top 5 actual words and 2 comments.\n","\n","- We will perform this study on two user group\n","    - Entire CreateDebate users\n","    - Top 10/100 users (on the basis of coreness value from *core-periphery* study) for *Perspective* debates.\n","\n","- The goal is to see whether the top dependecies change for the two user groups."],"metadata":{"id":"JNrn03SejEOh"}},{"cell_type":"markdown","source":["# Notebook Setup"],"metadata":{"id":"pnYQq2hxkHxY"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5w-2NFI6h6-r","executionInfo":{"status":"ok","timestamp":1678133805274,"user_tz":-330,"elapsed":28517,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"ff817ec4-ca82-4859-b7db-61b7f25d3b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Mount Google drive to Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github for handling\n","# CreateDebate dataset\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQ1MWxepkOfc","executionInfo":{"status":"ok","timestamp":1678133821151,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"6165f3cc-b0a5-456d-e4b3-216be4bcbcaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CreateDebateScraper'...\n","remote: Enumerating objects: 176, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 176 (delta 5), reused 4 (delta 4), pack-reused 170\u001b[K\n","Receiving objects: 100% (176/176), 207.95 KiB | 3.85 MiB/s, done.\n","Resolving deltas: 100% (61/61), done.\n","/content/CreateDebateScraper/src/nested\n"]}]},{"cell_type":"code","source":["# Install `cpnet` library for core-periphery analysis\n","!pip install cpnet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H14_BkkIkfcS","executionInfo":{"status":"ok","timestamp":1678133930925,"user_tz":-330,"elapsed":4115,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"6f8daae6-dabb-478f-f12d-a0b725231c29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting cpnet\n","  Downloading cpnet-0.0.21-py3-none-any.whl (30 kB)\n","Collecting simanneal>=0.4.2\n","  Downloading simanneal-0.5.0-py2.py3-none-any.whl (5.6 kB)\n","Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from cpnet) (1.10.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (3.0)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (1.3.5)\n","Requirement already satisfied: numba>=0.50.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (0.56.4)\n","Requirement already satisfied: plotly>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (5.5.0)\n","Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (1.2.0)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from cpnet) (1.22.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (0.11.2)\n","Requirement already satisfied: tqdm>=4.49.0 in /usr/local/lib/python3.8/dist-packages (from cpnet) (4.64.1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.50.0->cpnet) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.50.0->cpnet) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.50.0->cpnet) (6.0.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.0->cpnet) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.0->cpnet) (2022.7.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.10.0->cpnet) (8.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.10.0->cpnet) (1.15.0)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn>=0.11.0->cpnet) (3.5.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (23.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (4.38.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (8.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn>=0.11.0->cpnet) (0.11.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.50.0->cpnet) (3.15.0)\n","Installing collected packages: simanneal, cpnet\n","Successfully installed cpnet-0.0.21 simanneal-0.5.0\n"]}]},{"cell_type":"code","source":["# Import libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","from   matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","from thread import (Comment,\n","                    Thread)\n","\n","from collections import deque\n","from copy import deepcopy\n","import pickle\n","import json\n","from tqdm import tqdm\n","from pprint import pprint\n","from functools import lru_cache\n","\n","import networkx as nx\n","import spacy\n","import cpnet\n","import nltk "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gQrWngxk5Vj","executionInfo":{"status":"ok","timestamp":1678133972948,"user_tz":-330,"elapsed":16188,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"1c12baca-b4e9-4d98-c36b-4577695eeece"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}]},{"cell_type":"code","source":["# Setup for plotting\n","sns.set(style='darkgrid')\n","matplotlib.rcParams['figure.dpi'] = 120\n","matplotlib.rcParams['font.size'] = 18\n","matplotlib.rcParams['figure.figsize'] = (18, 5)\n","\n","# Setup for nltk\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","nltk.download('wordnet') # For lemmatizers\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","\n","# Setup for spacy\n","!python -m spacy download en_core_web_sm\n","scapy_nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lZS-z7ylAnq","executionInfo":{"status":"ok","timestamp":1678134017495,"user_tz":-330,"elapsed":17507,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"40796800-f249-4d6e-a52b-46dfcf9f0f4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","2023-03-06 20:20:05.655851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-06 20:20:05.655969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-06 20:20:05.655993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-03-06 20:20:07.557708: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.4)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"markdown","source":["# Load CreateDebate Dataset"],"metadata":{"id":"EoHvlqJ3lZqV"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"CwaiUzWHlLKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xSmVRrmljvi","executionInfo":{"status":"ok","timestamp":1678134129514,"user_tz":-330,"elapsed":18312,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"ce9add4c-0cc2-4975-b263-5dd0a6d5ec5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:17<00:00,  2.99s/it]\n"]}]},{"cell_type":"code","source":["# Get all usernames in a list\n","user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"sTXrbMO2lmYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utilities for Core-Periphery Analysis"],"metadata":{"id":"ydA8Q0xtmx-j"}},{"cell_type":"code","source":["def build_graph(user_subset, threads, n1 = 0, n2 = 0):\n","    \"\"\"\n","    Builds user network graph from hyper-parameters n1 and n2\n","    \n","    Inputs\n","    ------\n","    :param user_subset: list of users to consider, rest will be ignored\n","    :param threads: list containing `Thread`s of comment\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    Output\n","    ------\n","    (\n","        author_map: dict,\n","        reverse_map: list,\n","        author_count: int, \n","        graph: nx.DiGraph,\n","        matrix: list\n","    )\n","    \"\"\"\n","\n","    # Uses globally defined `threads` variable to construct this dictionary.\n","    # You may choose which categories to be included while building `threads`\n","\n","    # key  : author name\n","    # value: count of level-1 comments\n","    athr = dict()\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","    \n","    # Filter those authors who satisfy the contraint on number of level-1 comments\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    # Now use `athr` for storing count of direct replies\n","    # key  : author name\n","    # value: count of direct replies received\n","    athr = dict()\n","\n","    # Depth-first search utility to get number of direct replies for each author\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    # Traverse thread-tree to get number of direct replies for each author\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","    \n","    # Filter authors who now satify both the contrainsts on count of \n","    # - level-1 comments\n","    # - direct replies\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    # key  : author name\n","    # value: corresponing node number in the support/dispute network\n","    author_map = dict()\n","\n","    # To get author name for node number\n","    reverse_map = [\"\" for _ in range(len(A))]\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","    \n","    # Weighted adjacency matrices for user network\n","    # Weight for directed edge b/w Node A and Node B corresponsds to the number\n","    # of times Node A directly-replied Node B.\n","    matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    # Depth-first search utility to build the adjacency matrices for graph.\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        \n","        if cur_author in author_map:\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                if nxt_author in author_map:\n","                    nxt_author_id = author_map[nxt_author]\n","                    matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","        \n","    # Create NetworkX graphs from the adjacency matrices.\n","    # We need nx graphs in order to get various network stats provided in nx\n","    # library.\n","    graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if matrix[i][j] != 0:\n","                graph.add_weighted_edges_from([(i, j, matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, graph, matrix)"],"metadata":{"id":"omUKEDGKlv-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_coreness_dict(graph):\n","    \"\"\"Get coreness value for each nodes as dictionary. We use Boyd algorithm.\n","    :param graph: `nx.DiGraph` object\n","\n","    For more info about the algorithm used, see\n","    https://github.com/skojaku/core-periphery-detection\n","    \"\"\"\n","    algorithm = cpnet.MINRES()\n","    algorithm.detect(graph)\n","    coreness = algorithm.get_coreness()\n","    return coreness        "],"metadata":{"id":"sTivObKemgWj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utilities for Dependency Parsing"],"metadata":{"id":"A3oelxVjnBzu"}},{"cell_type":"code","source":["def load_empath_dictionary():\n","    \"\"\"\n","    Returns a dict[str, list] object where keys are categories and values are \n","    associated words for that category\n","    \"\"\"\n","    empath_dict = dict()\n","    with open('/content/gdrive/MyDrive/DL/empath/dictionary.tsv', 'r') as f:\n","        for line in f:\n","            cols = line.strip().split(\"\\t\")\n","            name = cols[0]\n","            terms = cols[1:]\n","            empath_dict[name] = list()\n","            for t in set(terms):\n","                empath_dict[name].append(t)\n","    return empath_dict"],"metadata":{"id":"WEeSRwx-mpgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empath_dict = load_empath_dictionary()"],"metadata":{"id":"qss2U24wnJKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_count = list()\n","for v in empath_dict.values():\n","    tokens_count.append(len(v))\n","print(f'Average token count {np.average(tokens_count)}, Std. dev {np.std(tokens_count)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glMb-BGlnLqc","executionInfo":{"status":"ok","timestamp":1678134536322,"user_tz":-330,"elapsed":13,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"730cddb8-e472-4583-bc26-debd97c3b59a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average token count 83.29381443298969, Std. dev 28.771070501829353\n"]}]},{"cell_type":"code","source":["# SOTA slur word dictionary (from Punyajoy)\n","with open('/content/gdrive/MyDrive/DL/slurwords/slur_dictionary.json') as f:\n","    slur_words_dict = json.load(f)"],"metadata":{"id":"MQjfanrznOQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hate-targets broad categories\n","# Paper: \"A Measurement Study of Hate Speech in Social Media\", Mainack Mondal\n","with open('/content/gdrive/MyDrive/DL/empath/hate_categories.json') as f:\n","    hate_targets_dict = json.load(f)\n","pprint(hate_targets_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6LdL2UInXex","executionInfo":{"status":"ok","timestamp":1678134598668,"user_tz":-330,"elapsed":486,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"d307933f-73b4-4e16-f8cf-b4b753d2e377"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Behavior': ['negative_emotion',\n","              'timidity',\n","              'disappointment',\n","              'animal',\n","              'smell',\n","              'anger',\n","              'torment',\n","              'shame',\n","              'lust',\n","              'sadness',\n","              'rage',\n","              'dominant_personality',\n","              'violence',\n","              'childish',\n","              'pet',\n","              'irritability',\n","              'fear',\n","              'sexual',\n","              'ridicule',\n","              'wealthy',\n","              'weakness',\n","              'nervousness',\n","              'envy',\n","              'aggression',\n","              'hate'],\n"," 'Class': ['economy', 'poor', 'stealing'],\n"," 'Crime': ['prison', 'crime', 'terrorism'],\n"," 'Disablity': ['mental'],\n"," 'Ethnicity': ['immigrant', 'arabs', 'asians'],\n"," 'Gender': ['women', 'feminine'],\n"," 'Physical': ['monster',\n","              'ugliness',\n","              'youth',\n","              'appearance',\n","              'disgust',\n","              'hygiene',\n","              'medical_emergency',\n","              'health'],\n"," 'Politics': ['politics', 'war', 'government'],\n"," 'Race': ['blacks', 'white'],\n"," 'Religion': ['jews', 'muslim'],\n"," 'Sexual orientation': ['homosexual']}\n"]}]},{"cell_type":"code","source":["hate_targets_raw = dict()\n","# key: hate_targets\n","# value: list of raw tokens associated with that target\n","\n","for k, v in hate_targets_dict.items():\n","    hate_targets_raw[k] = list()\n","    for token_type in v:\n","        if token_type in slur_words_dict:\n","            hate_targets_raw[k].extend(slur_words_dict[token_type])\n","        if token_type in empath_dict:\n","            hate_targets_raw[k].extend(empath_dict[token_type])"],"metadata":{"id":"FDb9OCMtnddd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","replace_underscores_with_whitespaces = lambda z: ' '.join(z.split('_'))\n","\n","hate_targets = dict()\n","# key: hate_targets\n","# value: list of processed tokens associated with that target\n","\n","for k, v in hate_targets_raw.items():\n","    temp = list(map(lemmatizer.lemmatize, v))\n","    hate_targets[k] = set(map(replace_underscores_with_whitespaces, temp))\n","\n","# pprint(hate_targets)"],"metadata":{"id":"36XA0S_BnmwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dependency_graph(doc):\n","    \"\"\"Create dependency graph of tokens using scapy\"\"\"\n","    dependency_edges = list() # (parent, child, relationship)\n","    id_to_text = dict()\n","    id_to_token = dict()\n","    root = None\n","    node_count = 0\n","\n","    for token in doc:\n","        node_count += 1\n","        parent = token.head.i\n","        child = token.i\n","        relationship = token.dep_\n","        id_to_text[child] = lemmatizer.lemmatize(token.text)\n","        id_to_token[child] = token\n","        if relationship == 'ROOT':\n","            root = child\n","            continue\n","        dependency_edges.append((parent, child, relationship))\n","\n","    dependency_graph = dict()\n","    for i in range(node_count): \n","        dependency_graph[i] = list()\n","    for p, c, r in dependency_edges:\n","        dependency_graph[p].append((c, r))\n","        dependency_graph[c].append((p, r))\n","    \n","    return dependency_graph, id_to_text, id_to_token, root"],"metadata":{"id":"OaUaYtV2npZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_personal_pronoun_ids(id_to_token):\n","    \"\"\"Index Generator: Generates ids which are indices of personal pronouns\"\"\"\n","    for k, v in id_to_token.items():\n","        if v.tag_ == 'PRP': # Personal pronoun tag in scapy\n","            yield k"],"metadata":{"id":"gCl59WSFnwXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pronoun_ids(id_to_token):\n","    \"\"\"Generates ids which are indices of pronouns\"\"\"\n","    for k, v in id_to_token.items():\n","        if v.pos_ == 'PRON': # Pronoun tag in scapy\n","            yield k"],"metadata":{"id":"tXj8FbPvn2kr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_ids(id_to_text, trigger_type):\n","    \"\"\"Generates ids which are indices of triggers\n","    :param id_to_text: id_to_text returned by create_dependency_graph\n","    :type id_to_text: dict\n","    :param trigger_type: What type of triggers?\n","    :type trigger_type: str\n","    \"\"\"\n","    for k, v in id_to_text.items():\n","        if v in hate_targets[trigger_type]:\n","            yield k"],"metadata":{"id":"qFlGW2bxn58w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def breadth_first_search(dependency_graph, source):\n","    \"\"\"Performs breadth first search\n","    :param dependency_graph: Dependency graph returned by create_dependency_graph\n","    :type dependency_graph: dict\n","    :param source: Source node ID\n","    :type source: int\n","    \"\"\"\n","    q = deque()\n","    used = set()\n","    d = dict() # distance of nodes from source\n","    p = dict() # parent in bfs\n","    r = dict() # relation observed\n","\n","    q.append(source)\n","    used.add(source)\n","    p[source] = -1\n","    d[source] = 0\n","\n","    while len(q):\n","        v = q.popleft()\n","        for u, rel in dependency_graph[v]:\n","            if u in used:\n","                continue\n","            used.add(u)\n","            q.append(u)\n","            d[u] = d[v] + 1\n","            p[u] = v\n","            r[u] = rel\n","\n","    return d, p, r"],"metadata":{"id":"hNJjlq-Bn9uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_path_from_bfs(source, dest, dist_dict, parent_dict, relation_dict):\n","    \"\"\"Generate path from source to dest. Path will contain relationships \n","    encountered in bfs.\n","    \"\"\"\n","    assert dist_dict[source] == 0\n","    assert dest in dist_dict \n","\n","    indices_list = list() # to store indices along the path\n","\n","    path = list()\n","    cur = dest\n","    while cur != source:\n","        path.append(relation_dict[cur])\n","        indices_list.append(cur)\n","        cur = parent_dict[cur]\n","    indices_list.append(cur)\n","    \n","    path_string = ' -> '.join(path)\n","    return path_string, indices_list"],"metadata":{"id":"zWPdXgHDoAUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_count(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes count of ad hominem triggers associated with indices generated\n","    by index_generator\n","    \"\"\"\n","\n","    trigger_count = dict()\n","    for trigger_type in hate_targets.keys():\n","        trigger_count[trigger_type] = 0\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in hate_targets.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        trigger_count[trigger_type] += 1\n","    \n","    return trigger_count"],"metadata":{"id":"DrqoeSFBoDYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_count_by_username_ctx(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes count of ad hominem triggers associated with indices generated\n","    by index_generator\n","    \"\"\"\n","\n","    trigger_count = dict()\n","    for trigger_type in hate_targets.keys():\n","        trigger_count[trigger_type] = dict()\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size,\n","                          as_tuples=True)\n","\n","    for doc, context in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in hate_targets.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        if context['username'] in trigger_count[trigger_type]:\n","                            trigger_count[trigger_type][context['username']] += 1\n","                        else:\n","                            trigger_count[trigger_type][context['username']] = 1\n","    \n","    return trigger_count"],"metadata":{"id":"TP6sCpvkoLvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_count_by_path(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes no. of occurence of dependency paths for given texts and returns it\n","    :param texts: list of comment body (text)\n","    :param index_generator: `get_personal_pronoun_ids` or `get_pronoun_ids`\n","    :param n_process: No. of processes spawned for processing, refer to pipe utility in spacy\n","    :param batch_size: Batch size while processing, refer to pipe utility in spacy\n","    \"\"\"\n","\n","    trigger_count = dict()\n","    # key: dependency path\n","    # value: no. of occurence of given dependency path in `texts`\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in hate_targets.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        path, _ = generate_path_from_bfs(index, trigger_id, dist,\n","                                                         parent, relation)\n","                        if path in trigger_count:\n","                            trigger_count[path] += 1\n","                        else:\n","                            trigger_count[path] = 1\n","    \n","    return trigger_count"],"metadata":{"id":"iwwG8JqSoRrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_words_along_paths(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"For each dependency path encountered, it will store the actual words\n","    which exist as we traverse the path\n","    :param texts: list of comment body (text)\n","    :param index_generator: `get_personal_pronoun_ids` or `get_pronoun_ids`\n","    :param n_process: No. of processes spawned for processing, refer to pipe utility in spacy\n","    :param batch_size: Batch size while processing, refer to pipe utility in spacy\n","    \"\"\"\n","    word_path = dict()\n","    # key: dependency path\n","    # value: list containing the actual words instead of relationship along the \n","    #        dependency path\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in hate_targets.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        path, word_indices = \\\n","                            generate_path_from_bfs(index, trigger_id, dist,\n","                                                   parent, relation)\n","                        words = list()\n","                        for word_index in word_indices:\n","                            words.append(id_to_text[word_index])\n","                        words_string = ' -> '.join(words)\n","\n","                        if path not in word_path:\n","                            word_path[path] = list()\n","                        word_path[path].append(words_string)             \n","    \n","    return word_path"],"metadata":{"id":"954mC_GWomw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_comments_by_path(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"For each dependency path encountered, it will store the actual words\n","    which exist as we traverse the path\n","\n","    :param texts: list of comment body (text)\n","    :param index_generator: `get_personal_pronoun_ids` or `get_pronoun_ids`\n","    :param n_process: No. of processes spawned for processing, refer to pipe utility in spacy\n","    :param batch_size: Batch size while processing, refer to pipe utility in spacy\n","    \"\"\"\n","    comment_list = list()\n","\n","    comment_ids = dict()\n","    # key: dependency path\n","    # value: dict of <comment_pos, occurence of given dependency path in the given comment>\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        comment_list.append(doc.text)\n","        curr_comment_pos = len(comment_list) - 1\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in hate_targets.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        path, word_indices = \\\n","                            generate_path_from_bfs(index, trigger_id, dist,\n","                                                   parent, relation)\n","                        words = list()\n","                        for word_index in word_indices:\n","                            words.append(id_to_text[word_index])\n","                        words_string = ' -> '.join(words)\n","                        if path not in comment_ids:\n","                            comment_ids[path] = dict()\n","                        if curr_comment_pos not in comment_ids[path]:\n","                            comment_ids[path][curr_comment_pos] = list()\n","                        comment_ids[path][curr_comment_pos].append(words_string)\n","    \n","    return comment_ids, comment_list"],"metadata":{"id":"ku4LyqEiughT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Computation for all CreateDebate users"],"metadata":{"id":"RcN_Y3u-y-W6"}},{"cell_type":"code","source":["for_against_debates = dict()\n","perspective_debates = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates[cat] = list()\n","    perspective_debates[cat] = list()\n","\n","    for comment in comments[cat]:\n","        if comment['polarity'] == 'Not Available':\n","            perspective_debates[cat].append(deepcopy(comment))\n","        else:\n","            for_against_debates[cat].append(deepcopy(comment))"],"metadata":{"id":"IbA-GKJUyDaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For now, only Politics users are considered!\n","for_against_user_set = set()\n","perspective_user_set = set()\n","\n","for comment in for_against_debates['politics2']:\n","    for_against_user_set.add(comment['author'])\n","\n","for comment in perspective_debates['politics2']:\n","    perspective_user_set.add(comment['author'])\n","\n","print(f'{len(for_against_user_set)} & {len(perspective_user_set)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOQs39tozGOi","executionInfo":{"status":"ok","timestamp":1678137667081,"user_tz":-330,"elapsed":7,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"73786edc-69d1-47cf-fe9c-764f42150f9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6761 & 2002\n"]}]},{"cell_type":"code","source":["# Encoding labels used while classification.\n","# Refer to notebook#63.\n","label_map = {\n","    'faulty generalization': 0,\n","    'false causality': 1,\n","    'circular reasoning': 2, \n","    'ad populum': 3,\n","    'ad hominem': 4,\n","    'fallacy of logic': 5,\n","    'appeal to emotion': 6,\n","    'false dilemma': 7,\n","    'equivocation': 8,\n","    'fallacy of extension': 9,\n","    'fallacy of relevance': 10,\n","    'fallacy of credibility': 11,\n","    'intentional': 12,\n","}\n","\n","inverse_label_map = dict()\n","for k, v in label_map.items():\n","    inverse_label_map[v] = k"],"metadata":{"id":"lGyx-hVtzKlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_obj(file_path):\n","    \"\"\"Load a pickled object from given path\n","    :param file_path: Path to the pickle file of the object\n","    :type file_path: string\n","    \"\"\"\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)"],"metadata":{"id":"TnLsRRfczPNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_obj(obj, file_path):\n","    \"\"\"Save an object to given path via pickling\n","    :param obj: Object to pickle\n","    :param file_path: Path for pickling\n","    :type file_path: string\n","    \"\"\"\n","    with open(file_path, 'wb') as f:\n","        return pickle.dump(obj, f)"],"metadata":{"id":"udn6pP53zT7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load labels and scores obtained during classification of for-against and\n","# perspective debates into the logical fallacies\n","for_against_labels_and_scores = \\\n","  load_obj('/content/gdrive/MyDrive/Temp/63-for_against_labels_and_scores.pkl')\n","perspective_labels_and_scores = \\\n","  load_obj('/content/gdrive/MyDrive/Temp/63-perspective_labels_and_scores.pkl')"],"metadata":{"id":"w6uoLkwSzV1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_logical = dict()\n","perspective_logical = dict()\n","# key: logical fallacy class\n","# value: list of comments \n","\n","for k in label_map.keys():\n","    for_against_logical[k] = list()\n","    perspective_logical[k] = list()\n","\n","for comment, labels_and_scores in zip(for_against_debates['politics2'], for_against_labels_and_scores):\n","    label = int(labels_and_scores[0]['label'].lstrip('LABEL_'))\n","    for_against_logical[inverse_label_map[label]].append(comment)\n","\n","for comment, labels_and_scores in zip(perspective_debates['politics2'], perspective_labels_and_scores):\n","    label = int(labels_and_scores[0]['label'].lstrip('LABEL_'))\n","    perspective_logical[inverse_label_map[label]].append(comment)"],"metadata":{"id":"Drj9jbNMzZ8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@lru_cache(maxsize=16)\n","def get_user_subset_for_against(cls):\n","    user_subset = set()\n","    for comment in for_against_logical[cls]:\n","        user_subset.add(comment['author'])\n","    return frozenset(user_subset)\n","\n","@lru_cache(maxsize=16)\n","def get_user_subset_perspective(cls):\n","    user_subset = set()\n","    for comment in perspective_logical[cls]:\n","        user_subset.add(comment['author'])\n","    return frozenset(user_subset)"],"metadata":{"id":"dMMdTJBXzkOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes_selected = ('fallacy of relevance', \n","                    'faulty generalization', \n","                    'ad hominem', \n","                    'intentional',\n","                    'appeal to emotion')"],"metadata":{"id":"8BfH52fKznlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_texts = dict()\n","perspective_texts = dict()\n","\n","for k in classes_selected:\n","    for_against_texts[k] = [comment['body'].lower() for comment in for_against_logical[k]]\n","    perspective_texts[k] = [comment['body'].lower() for comment in perspective_logical[k]]"],"metadata":{"id":"JQJ5JYlgz047"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trigger count by path\n","\n","trigger_count_by_path_for_against = dict()\n","trigger_count_by_path_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    trigger_count_by_path_for_against[k] = get_trigger_count_by_path(for_against_texts[k], get_personal_pronoun_ids)\n","    trigger_count_by_path_perspective[k] = get_trigger_count_by_path(perspective_texts[k], get_personal_pronoun_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dpq1fTdg0Dze","executionInfo":{"status":"ok","timestamp":1678140219802,"user_tz":-330,"elapsed":1323221,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"3f4130ef-666d-4756-ba65-b0b88a3cd306"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15549/15549 [04:52<00:00, 53.14it/s] \n","100%|██████████| 5099/5099 [01:17<00:00, 65.50it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 17420/17420 [05:17<00:00, 54.81it/s] \n","100%|██████████| 5508/5508 [01:25<00:00, 64.12it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13054/13054 [03:27<00:00, 62.95it/s] \n","100%|██████████| 4974/4974 [01:11<00:00, 69.62it/s] \n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11516/11516 [02:35<00:00, 74.10it/s] \n","100%|██████████| 4478/4478 [00:46<00:00, 95.94it/s] \n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4006/4006 [00:50<00:00, 79.11it/s] \n","100%|██████████| 1335/1335 [00:17<00:00, 77.58it/s] \n"]}]},{"cell_type":"code","source":["save_obj(trigger_count_by_path_for_against, '/content/gdrive/MyDrive/Temp/69-trigger_count_by_path_for_against.pkl')\n","save_obj(trigger_count_by_path_perspective, '/content/gdrive/MyDrive/Temp/69-trigger_count_by_path_perspective.pkl')"],"metadata":{"id":"AdbNPGzU1FmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Words along paths\n","\n","words_along_paths_for_against = dict()\n","words_along_paths_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    words_along_paths_for_against[k] = get_words_along_paths(for_against_texts[k], get_personal_pronoun_ids)\n","    words_along_paths_perspective[k] = get_words_along_paths(perspective_texts[k], get_personal_pronoun_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lV0Y8Rd9vH0","executionInfo":{"status":"ok","timestamp":1678141965646,"user_tz":-330,"elapsed":1351233,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"4f12eba0-4ef3-400b-faed-985943d7a336"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15549/15549 [05:03<00:00, 51.19it/s] \n","  0%|          | 0/5099 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n","    send_bytes(obj)\n","  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 405, in _send_bytes\n","    self._send(buf)\n","  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","100%|██████████| 5099/5099 [01:19<00:00, 63.93it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 17420/17420 [05:18<00:00, 54.69it/s] \n","100%|██████████| 5508/5508 [01:26<00:00, 63.61it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13054/13054 [03:30<00:00, 61.98it/s] \n","100%|██████████| 4974/4974 [01:08<00:00, 72.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11516/11516 [02:34<00:00, 74.69it/s] \n","100%|██████████| 4478/4478 [00:54<00:00, 81.83it/s] \n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4006/4006 [00:55<00:00, 72.52it/s] \n","100%|██████████| 1335/1335 [00:18<00:00, 72.32it/s] \n"]}]},{"cell_type":"code","source":["save_obj(words_along_paths_for_against, '/content/gdrive/MyDrive/Temp/69-words_along_paths_for_against.pkl')\n","save_obj(words_along_paths_perspective, '/content/gdrive/MyDrive/Temp/69-words_along_paths_perspective.pkl')"],"metadata":{"id":"FEXjy5Qn-aGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comments by path\n","\n","comment_ids_for_against = dict()\n","comment_list_for_against = dict()\n","comment_ids_perspective = dict()\n","comment_list_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    comment_ids_for_against[k], comment_list_for_against[k] = get_comments_by_path(for_against_texts[k], get_personal_pronoun_ids)\n","    comment_ids_perspective[k], comment_list_perspective[k] = get_comments_by_path(perspective_texts[k], get_personal_pronoun_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRv88DncEey0","executionInfo":{"status":"ok","timestamp":1678160468835,"user_tz":-330,"elapsed":1350911,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"981764d4-70e6-4d0b-8214-bc1d7dbec5cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15549/15549 [05:08<00:00, 50.33it/s]\n","100%|██████████| 5099/5099 [01:20<00:00, 63.58it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 17420/17420 [05:18<00:00, 54.76it/s] \n","100%|██████████| 5508/5508 [01:26<00:00, 63.49it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13054/13054 [03:37<00:00, 59.97it/s] \n","100%|██████████| 4974/4974 [01:12<00:00, 68.33it/s] \n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11516/11516 [02:28<00:00, 77.69it/s] \n","100%|██████████| 4478/4478 [00:50<00:00, 89.31it/s] \n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4006/4006 [00:49<00:00, 80.47it/s] \n","100%|██████████| 1335/1335 [00:17<00:00, 74.45it/s] \n"]}]},{"cell_type":"code","source":["save_obj(comment_ids_for_against, '/content/gdrive/MyDrive/Temp/69-comment_ids_for_against.pkl')\n","save_obj(comment_list_for_against, '/content/gdrive/MyDrive/Temp/69-comment_list_for_against.pkl')\n","save_obj(comment_ids_perspective, '/content/gdrive/MyDrive/Temp/69-comment_ids_perspective.pkl')\n","save_obj(comment_list_perspective, '/content/gdrive/MyDrive/Temp/69-comment_list_perspective.pkl')"],"metadata":{"id":"0e_wjQjaFKv0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Computation for Top 20 Perspective users\n","\n","- on the basis of coreness value from core-periphery study"],"metadata":{"id":"Yxf1Dy_rL6zk"}},{"cell_type":"code","source":["def get_perspective_threads():\n","    \"\"\"Returns a list of threads posted as Perspective debates\"\"\"\n","    threads = []\n","\n","    for category in categories_selected:\n","        reader_addr = f'/content/gdrive/MyDrive/DL/CreateDebate/{category}/threads.log'\n","        reader = open(reader_addr, 'rb')\n","        try:\n","            while True:\n","                e = pickle.load(reader)\n","                for_against_counter = 0\n","                perspective_counter = 0\n","                for k, v in e.comments.items():\n","                    if v.polarity == 'Not Available':\n","                        perspective_counter += 1\n","                    else:\n","                        for_against_counter += 1\n","                if perspective_counter > for_against_counter:\n","                    threads.append(e)\n","        except:\n","            reader.close()\n","    \n","    return threads"],"metadata":{"id":"MERA8RenLHnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perspective_threads = get_perspective_threads()"],"metadata":{"id":"Zuc3P4VONPaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_author_map, P_reverse_map, P_author_count, P_graph, P_matrix = build_graph(user_list, perspective_threads)"],"metadata":{"id":"GE0G3TfFNUGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_coreness = get_coreness_dict(P_graph)"],"metadata":{"id":"M45Lr7LfNXzj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_coreness_list = list()\n","for k, v in P_coreness.items():\n","    P_coreness_list.append((k, v))\n","P_coreness_list = sorted(P_coreness_list, key=lambda z: z[1], reverse=True)"],"metadata":{"id":"b-UnNYrZN_FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_top_users = list()\n","\n","for i in range(20): # select top-20 users on coreness value\n","    P_top_users.append(P_reverse_map[P_coreness_list[i][0]])"],"metadata":{"id":"G4nAPl6yOCPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_top_users_set = set(P_top_users)"],"metadata":{"id":"luOKUMw5POxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["P_comments_for_against = dict()\n","P_comments_perspective = dict()\n","\n","for k in classes_selected:\n","    P_comments_for_against[k] = list()\n","    P_comments_perspective[k] = list()\n","\n","    for comment in for_against_logical[k]:\n","        if comment['author'] in P_top_users_set:\n","            P_comments_for_against[k].append(deepcopy(comment))\n","\n","    for comment in perspective_logical[k]: \n","        if comment['author'] in P_top_users_set:\n","            P_comments_perspective[k].append(deepcopy(comment))"],"metadata":{"id":"Z8AGLDR4PkOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k in classes_selected:\n","    print(k, len(P_comments_for_against[k]), len(P_comments_perspective[k]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CmS-kcXP8mF","executionInfo":{"status":"ok","timestamp":1678146417836,"user_tz":-330,"elapsed":9,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"b904ee0b-6a60-440d-c112-8033839b7e7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance 2995 2348\n","faulty generalization 3209 2300\n","ad hominem 3630 2490\n","intentional 3108 2461\n","appeal to emotion 804 572\n"]}]},{"cell_type":"code","source":["P_texts_for_against = dict()\n","P_texts_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    P_texts_for_against[k] = [comment['body'].lower() for comment in P_comments_for_against[k]]\n","    P_texts_perspective[k] = [comment['body'].lower() for comment in P_comments_perspective[k]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNy8oWn_QC9n","executionInfo":{"status":"ok","timestamp":1678146512350,"user_tz":-330,"elapsed":573,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"76beaf76-0519-4f57-df46-f058191ff955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n","faulty generalization\n","ad hominem\n","intentional\n","appeal to emotion\n"]}]},{"cell_type":"code","source":["# Trigger count by path\n","P_trigger_count_by_path_for_against = dict()\n","P_trigger_count_by_path_perspective = dict()\n","\n","for k in classes_selected: \n","    print(k)\n","    P_trigger_count_by_path_for_against[k] = get_trigger_count_by_path(P_texts_for_against[k], get_personal_pronoun_ids)\n","    P_trigger_count_by_path_perspective[k] = get_trigger_count_by_path(P_texts_perspective[k], get_personal_pronoun_ids)\n","\n","save_obj(P_trigger_count_by_path_for_against, '/content/gdrive/MyDrive/Temp/69-P_trigger_count_by_path_for_against.pkl')\n","save_obj(P_trigger_count_by_path_perspective, '/content/gdrive/MyDrive/Temp/69-P_trigger_count_by_path_perspective.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdFK0KaaQjQe","executionInfo":{"status":"ok","timestamp":1678147016328,"user_tz":-330,"elapsed":324989,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"db61ca23-8079-4f87-af8e-2f89e0f35b4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2995/2995 [00:53<00:00, 55.71it/s]\n","100%|██████████| 2348/2348 [00:29<00:00, 78.40it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3209/3209 [00:55<00:00, 57.99it/s] \n","100%|██████████| 2300/2300 [00:35<00:00, 64.73it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3630/3630 [00:47<00:00, 75.92it/s] \n","100%|██████████| 2490/2490 [00:27<00:00, 90.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3108/3108 [00:33<00:00, 93.38it/s] \n","100%|██████████| 2461/2461 [00:21<00:00, 113.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 804/804 [00:11<00:00, 68.68it/s] \n","100%|██████████| 572/572 [00:07<00:00, 75.57it/s] \n"]}]},{"cell_type":"code","source":["# Words along paths\n","P_words_along_paths_for_against = dict()\n","P_words_along_paths_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    P_words_along_paths_for_against[k] = get_words_along_paths(P_texts_for_against[k], get_personal_pronoun_ids)\n","    P_words_along_paths_perspective[k] = get_words_along_paths(P_texts_perspective[k], get_personal_pronoun_ids)\n","\n","save_obj(P_words_along_paths_for_against, '/content/gdrive/MyDrive/Temp/69-P_words_along_paths_for_against.pkl')\n","save_obj(P_words_along_paths_perspective, '/content/gdrive/MyDrive/Temp/69-P_words_along_paths_perspective.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCfLbqQIRnbE","executionInfo":{"status":"ok","timestamp":1678147467777,"user_tz":-330,"elapsed":319552,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"15bfc093-ec4a-4b1f-bcd2-302a00f4e243"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2995/2995 [00:50<00:00, 59.13it/s] \n","100%|██████████| 2348/2348 [00:25<00:00, 90.83it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3209/3209 [00:54<00:00, 58.90it/s] \n","100%|██████████| 2300/2300 [00:32<00:00, 70.35it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3630/3630 [00:54<00:00, 67.10it/s] \n","100%|██████████| 2490/2490 [00:26<00:00, 92.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3108/3108 [00:35<00:00, 88.28it/s] \n","100%|██████████| 2461/2461 [00:20<00:00, 121.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 804/804 [00:10<00:00, 74.83it/s] \n","100%|██████████| 572/572 [00:08<00:00, 70.41it/s] \n"]}]},{"cell_type":"code","source":["# Comments by path\n","P_comment_ids_for_against = dict()\n","P_comment_list_for_against = dict()\n","P_comment_ids_perspective = dict()\n","P_comment_list_perspective = dict()\n","\n","for k in classes_selected:\n","    print(k)\n","    P_comment_ids_for_against[k], P_comment_list_for_against[k] = get_comments_by_path(P_texts_for_against[k], get_personal_pronoun_ids)\n","    P_comment_ids_perspective[k], P_comment_list_perspective[k] = get_comments_by_path(P_texts_perspective[k], get_personal_pronoun_ids)\n","\n","save_obj(P_comment_ids_for_against, '/content/gdrive/MyDrive/Temp/69-P_comment_ids_for_against.pkl')\n","save_obj(P_comment_list_for_against, '/content/gdrive/MyDrive/Temp/69-P_comment_list_for_against.pkl')\n","save_obj(P_comment_ids_perspective, '/content/gdrive/MyDrive/Temp/69-P_comment_ids_perspective.pkl')\n","save_obj(P_comment_list_perspective, '/content/gdrive/MyDrive/Temp/69-P_comment_list_perspective.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMvwfI8SR-L9","executionInfo":{"status":"ok","timestamp":1678161674351,"user_tz":-330,"elapsed":324305,"user":{"displayName":"Utkarsh Patel","userId":"14292413845157007490"}},"outputId":"11b2374e-b71d-4235-9d2b-44d9346bdaec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fallacy of relevance\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2995/2995 [00:55<00:00, 54.44it/s]\n","100%|██████████| 2348/2348 [00:24<00:00, 94.41it/s] \n"]},{"output_type":"stream","name":"stdout","text":["faulty generalization\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3209/3209 [00:54<00:00, 58.86it/s] \n","100%|██████████| 2300/2300 [00:33<00:00, 68.35it/s] \n"]},{"output_type":"stream","name":"stdout","text":["ad hominem\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3630/3630 [00:49<00:00, 72.63it/s] \n","100%|██████████| 2490/2490 [00:28<00:00, 85.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["intentional\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3108/3108 [00:33<00:00, 92.16it/s] \n","100%|██████████| 2461/2461 [00:19<00:00, 128.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["appeal to emotion\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 804/804 [00:11<00:00, 67.55it/s] \n","100%|██████████| 572/572 [00:11<00:00, 50.81it/s]\n"]}]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"lT1ao82MTm7S"}},{"cell_type":"code","source":["Types = ('for-against-all', 'perspective-all', 'for-against-top', 'perspective-top')\n","\n","TriggerCount = (\n","    trigger_count_by_path_for_against,\n","    trigger_count_by_path_perspective,\n","    P_trigger_count_by_path_for_against,\n","    P_trigger_count_by_path_perspective\n",")\n","\n","WordsAlongPaths = (\n","    words_along_paths_for_against,\n","    words_along_paths_perspective,\n","    P_words_along_paths_for_against,\n","    P_words_along_paths_perspective,\n",")\n","\n","CommentIds = (\n","    comment_ids_for_against,\n","    comment_ids_perspective,\n","    P_comment_ids_for_against,\n","    P_comment_ids_perspective\n",")\n","\n","CommentList = (\n","    comment_list_for_against,\n","    comment_list_perspective,\n","    P_comment_list_for_against,\n","    P_comment_list_perspective\n",")"],"metadata":{"id":"hlCM2p1aS9CM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = dict()\n","\n","for i, Type in enumerate(Types):\n","    data[Type] = dict()\n","\n","    for cls in classes_selected:\n","        data[Type][cls] = dict()\n","\n","        # Get top 5 dependency paths\n","        dependency_path_list = list()\n","        for k, v in TriggerCount[i][cls].items():\n","            dependency_path_list.append((k, v))\n","        dependency_path_list = sorted(dependency_path_list, key=lambda z: z[1], reverse=True)[:5]\n","\n","        for dependency_path, _ in dependency_path_list:\n","            detailed_depenency_path = ' -> '.join(map(spacy.explain, dependency_path.split(' -> ')))\n","            data[Type][cls][detailed_depenency_path] = list()\n","\n","            # Get most used actual words for this dependency\n","            words_dict = dict()\n","            for words in WordsAlongPaths[i][cls][dependency_path]:\n","                if words not in words_dict:\n","                    words_dict[words] = 0\n","                words_dict[words] += 1\n","            words_list = list()\n","            for k, v in words_dict.items():\n","                words_list.append((k, v))\n","            words_list = sorted(words_list, key=lambda z: z[1], reverse=True)[:5]\n","\n","            for words, _ in words_list:\n","                data[Type][cls][detailed_depenency_path].append(words)"],"metadata":{"id":"6AxZTgn61TVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./words.json', 'w') as f:\n","    json.dump(data, f, indent=4)"],"metadata":{"id":"uxVcn3Nb6x1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reduce_array(a):\n","    d = {}\n","    for x in a:\n","        if x not in d:\n","            d[x] = 0\n","        d[x] += 1\n","    lst = []\n","    for k, v in d.items():\n","        lst.append((k, v))\n","    lst = sorted(lst, key=lambda z: z[1], reverse=True)\n","    s = []\n","    for k, v in lst:\n","        s.append(f'{k} (x{v}), ')\n","    return ''.join(s)"],"metadata":{"id":"lOoRSLRGSPv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = dict()\n","\n","for i, Type in enumerate(Types):\n","    data[Type] = dict()\n","\n","    for cls in classes_selected:\n","        data[Type][cls] = dict()\n","\n","        # Get top 5 dependency paths\n","        dependency_path_list = list()\n","        for k, v in TriggerCount[i][cls].items():\n","            dependency_path_list.append((k, v))\n","        dependency_path_list = sorted(dependency_path_list, key=lambda z: z[1], reverse=True)[:5]\n","\n","        for dependency_path, _ in dependency_path_list:\n","            detailed_depenency_path = ' -> '.join(map(spacy.explain, dependency_path.split(' -> ')))\n","            data[Type][cls][detailed_depenency_path] = list()\n","\n","            # Get 2 comments\n","            comment_ids_list = list()\n","            for k, v in CommentIds[i][cls][dependency_path].items():\n","                comment_ids_list.append((k, len(v)))\n","            comment_ids_list = sorted(comment_ids_list, key=lambda z: z[1], reverse=True)\n","\n","            filtered_cids = list()\n","            for cid, _ in comment_ids_list:\n","                cur_comment_text = CommentList[i][cls][cid]\n","                if (len(cur_comment_text.split()) > 200):\n","                    continue\n","                filtered_cids.append(cid)\n","\n","\n","            for cid in filtered_cids[:2]:\n","                comment_body = CommentList[i][cls][cid]\n","                comment_triggers = reduce_array(CommentIds[i][cls][dependency_path][cid])\n","                data[Type][cls][detailed_depenency_path].append(dict(comment=comment_body, path=comment_triggers))"],"metadata":{"id":"Wdc_lQtO7O4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./comments.json', 'w') as f:\n","    json.dump(data, f, indent=4)"],"metadata":{"id":"W3JCSDBJ-nOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CX30pqC3_hms"},"execution_count":null,"outputs":[]}]}