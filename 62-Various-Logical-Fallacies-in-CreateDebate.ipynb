{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP9BkMVeUowg+IVasp/AeGf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0NhcECYxVI0e"},"outputs":[],"source":["# Mount Google drive to Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested"],"metadata":{"id":"Z4kT-XeYWlw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"gWuUBvlhWvnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset used in Logical Fallacy Detection (Zhijing Jin et al.)\n","# We will use this dataset to train BERT for detecting different\n","# categories of logical fallacy.\n","!curl https://raw.githubusercontent.com/causalNLP/logical-fallacy/main/data/edu_all.csv -o fallacies.csv"],"metadata":{"id":"Fdww1PROW1el"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   collections              import namedtuple\n","from   copy                     import deepcopy\n","# import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","# import spacy\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","nltk.download('wordnet') # For lemmatizers\n","nltk.download('omw-1.4')\n","\n","import matplotlib\n","from   nltk.stem                import WordNetLemmatizer\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","\n","import torch\n","from transformers import (\n","    BertTokenizer as Tokenizer,\n","    BertForSequenceClassification as Model,\n","    pipeline\n",") \n","\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","\n","import seaborn as sns\n","\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"CqGWnKNBXWSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's see the fallacies.csv\n","\n","df = pd.read_csv('fallacies.csv')"],"metadata":{"id":"p_7Tfaa-Xto3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"UQug4m9fYIYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of class\n","\n","labels = list(df['updated_label'].unique())\n","for label in labels:\n","    print(label)"],"metadata":{"id":"IFBIqTc8YJg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(df['updated_label'].unique())\n","label_stats = list()\n","for label in labels:\n","    frac = len(df[df[\"updated_label\"] == label]) / len(df)\n","    label_stats.append((label, frac))"],"metadata":{"id":"OOJpKGyuZPbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label, frac in sorted(label_stats, reverse=True, key=lambda z: z[1]):\n","    print(f'Label: {label:>24}, {frac * 100:.2f}%')"],"metadata":{"id":"-yVLzuIjZSaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding labels as integer\n","label_map = {\n","    'faulty generalization': 0,\n","    'false causality': 1,\n","    'circular reasoning': 2, \n","    'ad populum': 3,\n","    'ad hominem': 4,\n","    'fallacy of logic': 5,\n","    'appeal to emotion': 6,\n","    'false dilemma': 7,\n","    'equivocation': 8,\n","    'fallacy of extension': 9,\n","    'fallacy of relevance': 10,\n","    'fallacy of credibility': 11,\n","    'intentional': 12,\n","}\n","\n","inverse_label_map = dict()\n","for k, v in label_map.items():\n","    inverse_label_map[v] = k"],"metadata":{"id":"8q8828nqf32d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create training set\n","texts, labels = list(df['source_article']), [z for z in map(lambda x: label_map[x], list(df['updated_label']))]"],"metadata":{"id":"XO9ovwGRaD-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts[0], labels[0]"],"metadata":{"id":"DK1U2caabrr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"y3spUdfnbuK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encodings = tokenizer(texts, max_length=64, truncation=True, padding=\"max_length\")"],"metadata":{"id":"8HSCnMaYcr0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","dataset = CustomDataset(encodings, labels)"],"metadata":{"id":"PfBcOVi5dkZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)"],"metadata":{"id":"Eo4fmZehdwLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model.from_pretrained('bert-base-uncased',\n","                              num_labels=len(label_map))\n","model.to(device)\n","model.train()"],"metadata":{"id":"80soMWuwd85P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = DataLoader(dataset, batch_size=64, shuffle=True)\n","optim = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"hdHhkuDWeImL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(5):\n","    for batch in tqdm(loader):\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels_ = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels_)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()"],"metadata":{"id":"QTiSiFMBeZhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating inference pipeline\n","pipe = pipeline(task='text-classification',\n","                model=model,\n","                tokenizer=tokenizer,\n","                device=0)"],"metadata":{"id":"iGgJKvjOetvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"kxhoTViuqcru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"6RAQTx9LqwsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_debates = dict()\n","perspective_debates = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates[cat] = list()\n","    perspective_debates[cat] = list()\n","\n","    for comment in comments[cat]:\n","        if comment['polarity'] == 'Not Available':\n","            perspective_debates[cat].append(deepcopy(comment))\n","        else:\n","            for_against_debates[cat].append(deepcopy(comment))"],"metadata":{"id":"PSBxtqMSq5cH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_politics_texts = [comment['body'] for comment in \\\n","                              for_against_debates['politics2']]\n","perspective_politics_texts = [comment['body'] for comment in \\\n","                              perspective_debates['politics2']]"],"metadata":{"id":"dnrZDI3ArEwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ListDataset(torch.utils.data.Dataset):\n","    def __init__(self, text_list):\n","        self._list = text_list\n","\n","    def __len__(self):\n","        return len(self._list)\n","\n","    def __getitem__(self, i):\n","        return self._list[i]"],"metadata":{"id":"ZH2aiB-drsCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_dataset = ListDataset(for_against_politics_texts)\n","perspective_dataset = ListDataset(perspective_politics_texts)"],"metadata":{"id":"NZtKWnRQrv1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_labels_and_scores(list_dataset):\n","    labels_and_scores = list()\n","    for out in tqdm(pipe(list_dataset, batch_size=64, max_length=64, truncation=True), total=len(list_dataset)):\n","        labels_and_scores.append(out)\n","    return labels_and_scores"],"metadata":{"id":"kR4DeH-isCpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_labels_and_scores = get_labels_and_scores(for_against_dataset)"],"metadata":{"id":"G86ZNuTfszyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perspective_labels_and_scores = get_labels_and_scores(perspective_dataset)"],"metadata":{"id":"A4IiIkRFs6Xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_labeled = {\n","    'texts': list(),\n","    'labels': list(),\n","    'scores': list(),\n","}\n","\n","perspective_labeled = {\n","    'texts': list(),\n","    'labels': list(),\n","    'scores': list(),\n","}"],"metadata":{"id":"hl56huSCv2yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, label_and_score in zip(for_against_politics_texts, for_against_labels_and_scores):\n","    label = inverse_label_map[int(label_and_score['label'][6:])] # Remove LABEL_ prefix\n","    score = label_and_score['score']\n","    for_against_labeled['texts'].append(text)\n","    for_against_labeled['labels'].append(label)\n","    for_against_labeled['scores'].append(score)"],"metadata":{"id":"ZUwyXjSHwIeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, label_and_score in zip(perspective_politics_texts, perspective_labels_and_scores):\n","    label = inverse_label_map[int(label_and_score['label'][6:])] # Remove LABEL_ prefix\n","    score = label_and_score['score']\n","    perspective_labeled['texts'].append(text)\n","    perspective_labeled['labels'].append(label)\n","    perspective_labeled['scores'].append(score)"],"metadata":{"id":"OVEbBUsIxYWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.histplot(data=perspective_labeled, x=\"scores\", hue='labels', multiple='stack')"],"metadata":{"id":"4ppOzrFoyH6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_class_dist_number(data1, data2):\n","    labels = [k for k in label_map.keys()]\n","\n","    def get_class_freq(data):\n","        freq = dict()\n","        for label in labels:\n","            freq[label] = 0\n","        for label in data['labels']:\n","            freq[label] += 1\n","        return freq\n","\n","    freq1 = get_class_freq(data1)\n","    freq2 = get_class_freq(data2)\n","\n","    x = labels\n","    y1 = [freq1[label] for label in label_map]\n","    y2 = [freq2[label] for label in label_map]\n","\n","    ticks = np.arange(len(x))\n","    width = 0.30\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width / 2, y1, width, label='for-against', tick_label=x)\n","    subplot2 = ax.bar(ticks + width / 2, y2, width, label='perspective', tick_label=x)\n","\n","    ax.set_ylabel('# Comment')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"11AH-PNk10Bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_class_dist_frac(data1, data2):\n","    labels = [k for k in label_map.keys()]\n","\n","    def get_class_freq(data):\n","        freq = dict()\n","        for label in labels:\n","            freq[label] = 0\n","        for label in data['labels']:\n","            freq[label] += 1\n","        return freq\n","\n","    def normalize(data):\n","        total = 0\n","        for v in data.values():\n","            total += v\n","        for k in labels:\n","            data[k] /= total\n","        return data\n","\n","    freq1 = normalize(get_class_freq(data1))\n","    freq2 = normalize(get_class_freq(data2))\n","\n","    x = labels\n","    y1 = [freq1[label] for label in label_map]\n","    y2 = [freq2[label] for label in label_map]\n","\n","    ticks = np.arange(len(x))\n","    width = 0.30\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width / 2, y1, width, label='for-against', tick_label=x)\n","    subplot2 = ax.bar(ticks + width / 2, y2, width, label='perspective', tick_label=x)\n","\n","    ax.set_ylabel('% Comment')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"Kvy6mB5t6qn9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_class_dist_frac(for_against_labeled, perspective_labeled)"],"metadata":{"id":"BzZKoiFt43wE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_ordered = dict()\n","perspective_ordered = dict()\n","\n","for label in label_map.keys():\n","    for_against_ordered[label] = list()\n","    perspective_ordered[label] = list()"],"metadata":{"id":"0eibj4Cb5AOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, label, score in zip(for_against_labeled['texts'],\n","                              for_against_labeled['labels'],\n","                              for_against_labeled['scores']):\n","    for_against_ordered[label].append((text, score))"],"metadata":{"id":"L_Ii9snKDZpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, label, score in zip(perspective_labeled['texts'],\n","                              perspective_labeled['labels'],\n","                              perspective_labeled['scores']):\n","    perspective_ordered[label].append((text, score))"],"metadata":{"id":"2Ub1qo1ZDxtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in label_map.keys():\n","    for_against_ordered[label] = sorted(for_against_ordered[label],\n","                                        reverse=True,\n","                                        key=lambda z: z[1])\n","    perspective_ordered[label] = sorted(perspective_ordered[label],\n","                                        reverse=True,\n","                                        key=lambda z: z[1])"],"metadata":{"id":"F3GJm9nuD5_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_map = {\n","    'faulty generalization': 0,\n","    'false causality': 1,\n","    'circular reasoning': 2, \n","    'ad populum': 3,\n","    'ad hominem': 4,\n","    'fallacy of logic': 5,\n","    'appeal to emotion': 6,\n","    'false dilemma': 7,\n","    'equivocation': 8,\n","    'fallacy of extension': 9,\n","    'fallacy of relevance': 10,\n","    'fallacy of credibility': 11,\n","    'intentional': 12,\n","}"],"metadata":{"id":"aTevrzZwEiFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current = 'equivocation'\n","\n","for text, score in for_against_ordered[current][:20]:\n","    print(f'[{score:.2f}]: {text}')\n","    print()"],"metadata":{"id":"WiMuTdpjELHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, score in perspective_ordered[current][:20]:\n","    print(f'[{score:.2f}]: {text}')\n","    print()"],"metadata":{"id":"Z1hD9A2wE2e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q3xswYUpGUs6"},"execution_count":null,"outputs":[]}]}