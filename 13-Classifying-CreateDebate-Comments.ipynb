{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"13-Classifying-CreateDebate-Comments.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNrWlsWH8y+n4DiqLeavI/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2PIpNQsLBUp6"},"source":["# Classifying Create Debate comments\n","* __Objective__: Training BERT (tensorflow) model on annotated Change My View dataset, and using it to compute confidence score of comments in Create Debate corpus and also using Perspective API to get scores for various attributes like toxicity, insult, etc.\n","* __File Management__: Using Google Drive\n","* __Runtime Type__: GPU\n","* __Note__: Run this notebook only for a new version of Create Debate corpus, data of previous versions are already saved in the Google Drive"]},{"cell_type":"markdown","metadata":{"id":"prlm2AFfbTxf"},"source":["## Mounting Google Drive and cloning CreateDebate-Scraper API"]},{"cell_type":"code","metadata":{"id":"2JqW2hjzv_CN"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mrq8sOtgw_Lc"},"source":["!git clone https://github.com/utkarsh512/CreateDebateScraper.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWAUGAgY1q8E"},"source":["%cd CreateDebateScraper/src/nested/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_26W5tj1vmc"},"source":["from thread import Thread, Comment\n","import pickle\n","import networkx as nx\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CZl-WW4vVzF"},"source":["## Training BERT TF model on Change-My-View dataset"]},{"cell_type":"code","metadata":{"id":"TDq4DY6fvhN0"},"source":["!pip install -q tensorflow-text\n","!pip install -q tf-models-official"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwBhdGZpvpKV"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optmizer\n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhkmF6nBv2lV"},"source":["# Allow memory growth for the GPU\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nU91aw-wv8dl"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtAtWuYfwF2F"},"source":["import os\n","\n","if tf.test.is_gpu_available():\n","  strategy = tf.distribute.MirroredStrategy()\n","  print('Using GPU')\n","else:\n","  raise ValueError('Running on CPU is not recomended.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UlwxWcwwUNN"},"source":["os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n","tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","train_dir = '/content/gdrive/MyDrive/DL/dataset/bert/train'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMfg_qW4wl3N"},"source":["AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 32\n","seed = 42\n","\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    seed=seed)\n","\n","class_names = raw_train_ds.class_names\n","train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Owa7kP4wsJ9"},"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpnwaNxsw6GE"},"source":["with strategy.scope():\n","    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","    metrics = tf.metrics.BinaryAccuracy()\n","    epochs = 3\n","    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","    num_train_steps = steps_per_epoch * epochs\n","    num_warmup_steps = int(0.1*num_train_steps)\n","\n","    init_lr = 3e-5\n","    optimizer = optimization.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')\n","    classifier_model = build_classifier_model()\n","    classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","    print(f'Training model with {tfhub_handle_encoder}')\n","    history = classifier_model.fit(x=train_ds, epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSaGNv_11f4l"},"source":["## Classifying CreateDebate Comments"]},{"cell_type":"code","metadata":{"id":"iUFyVfZMGCI6"},"source":["TOPIC = 'world' # change this to access different datasets\n","\n","#Extracting thread objects\n","\n","reader_addr = f'/content/gdrive/MyDrive/DL/CreateDebate/{TOPIC}/threads.log'\n","reader = open(reader_addr, 'rb')\n","threads = []\n","e = Thread()\n","try:\n","    while True:\n","        e = pickle.load(reader)\n","        threads.append(e)\n","except:\n","    reader.close()\n","\n","authors = dict()\n","\n","tot_comment_cnt = 0\n","\n","for thread in threads:\n","    for key in thread.comments.keys():\n","        tot_comment_cnt += 1\n","        comment = thread.comments[key]\n","        cur_text = comment.body\n","        cur_author = comment.author\n","        try:\n","            authors[cur_author].append(cur_text)\n","        except:\n","            authors[cur_author] = list()\n","            authors[cur_author].append(cur_text)\n","\n","ah_comment_cnt = dict()\n","none_comment_cnt = dict()\n","comment_score = dict()\n","\n","comments_with_score = list()\n","\n","for author in authors.keys():\n","    ah_comment_cnt[author] = 0\n","    none_comment_cnt[author] = 0\n","    comment_score[author] = list()\n","\n","# Classifying comments\n","\n","cur_author_cnt = 0\n","cur_comment_cnt = 0\n","tot_author_cnt = len(authors.keys())\n","\n","with tqdm(total=tot_comment_cnt) as pbar:\n","    for author in authors.keys():\n","        cur_author_cnt += 1\n","        for i in range(len(authors[author])):\n","            cur_comment_cnt += 1\n","            text = [authors[author][i]]\n","            result = tf.sigmoid(classifier_model(tf.constant(text)))\n","            score = float(result[0][0])\n","            comments_with_score.append((score, text))\n","            comment_score[author].append(score)\n","            if score < 0.5:\n","                ah_comment_cnt[author] += 1\n","            else:\n","                none_comment_cnt[author] += 1\n","            pbar.update(1)\n","\n","# Saving the results in the drive\n","\n","dir = f'/content/gdrive/MyDrive/DL/CreateDebate/{TOPIC}/'\n","\n","with open(dir + 'ah_comment_cnt.log', 'wb') as f:\n","    pickle.dump(ah_comment_cnt, f)\n","\n","with open(dir + 'none_comment_cnt.log', 'wb') as f:\n","    pickle.dump(none_comment_cnt, f)\n","\n","with open(dir + 'comment_score.log', 'wb') as f:\n","    pickle.dump(comment_score, f)\n","\n","with open(dir + 'comments_with_score.log', 'wb') as f:\n","    pickle.dump(comments_with_score, f)\n","\n","with open(dir + 'authors.log', 'wb') as f:\n","    pickle.dump(authors, f)"],"execution_count":null,"outputs":[]}]}