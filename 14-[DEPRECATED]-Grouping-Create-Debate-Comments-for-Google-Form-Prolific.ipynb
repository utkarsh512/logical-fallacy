{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"14-Grouping-Create-Debate-Comments-for-Google-Form-Prolific.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOghwW7lZVpkFCvOCML3eT5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Cw5u9MpHFCFu"},"source":["# Grouping Create Debate Comments for Google Form Generation for Annotation Task on Prolific\n","* __Objective__: Grouping the comments of Create Debate corpus to make balanced set of 20 comments (10 ad-hominem + 10 none), with small, medium and large comments being in 50:30:20 ratio respectively.\n","* __File Management__: Using Google Drive\n","* __Runtime Type__: CPU"]},{"cell_type":"markdown","metadata":{"id":"r33jg3TKF7HC"},"source":["## Mounting Google Drive and loading data"]},{"cell_type":"code","metadata":{"id":"TN1DfM3eBQm1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYeUCnbafcDf"},"source":["!git clone https://github.com/utkarsh512/CreateDebate-Scraper.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpe8uUb2fgsQ"},"source":["%cd CreateDebate-Scraper/src/nested/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slvFXBJyGNbZ"},"source":["import numpy as np\n","import re\n","import pickle\n","from thread import Thread, Comment"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vm6-v7VGlxw"},"source":["dir = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/{}.log'\n","comments_with_score = list()\n","\n","with open(dir.format('comments_with_score'), 'rb') as f:\n","    comments_with_score = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ctklTd7fsBK"},"source":["reader_addr = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/threads.log'\n","reader = open(reader_addr, 'rb')\n","threads = []\n","e = Thread()\n","try:\n","    while True:\n","        e = pickle.load(reader)\n","        threads.append(e)\n","except:\n","    reader.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qxqo8UAngOsi"},"source":["authors = dict()\n","\n","tot_comment_cnt = 0\n","\n","idx = -1\n","\n","for thread in threads:\n","    idx += 1\n","    for key in thread.comments.keys():\n","        tot_comment_cnt += 1\n","        comment = thread.comments[key]\n","        cur_text = comment.body\n","        cur_author = comment.author\n","        url = thread.url\n","        try:\n","            authors[cur_author].append((cur_text, url, idx))\n","        except:\n","            authors[cur_author] = list()\n","            authors[cur_author].append((cur_text, url, idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwaJMAF4h1VN"},"source":["cur_author_cnt = 0\n","cur_comment_cnt = 0\n","tot_author_cnt = len(authors.keys())\n","\n","comments_with_url = list()\n","\n","for author in authors.keys():\n","    cur_author_cnt += 1\n","    for i in range(len(authors[author])):\n","        cur_comment_cnt += 1\n","        text = [authors[author][i][0]]\n","        url = authors[author][i][1]\n","        idx = authors[author][i][2]\n","        comments_with_url.append((url, text, idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgCz5dbCg0dx"},"source":["idx = np.random.randint(len(comments_with_score))\n","print(comments_with_score[idx][1][0])\n","print(comments_with_url[idx][1][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWO-_uP6iv8z"},"source":["v = list()\n","for i in range(len(comments_with_score)):\n","    score = comments_with_score[i][0]\n","    text = comments_with_score[i][1]\n","    url = comments_with_url[i][0]\n","    idx = comments_with_url[i][2]\n","    v.append((score, text, url, idx))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1CILxBhG_n4"},"source":["## Extracting top 1000 ad-hominem and none comments"]},{"cell_type":"code","metadata":{"id":"TCW_cDwSG8ST"},"source":["v = sorted(v)\n","top_ah_comments = []\n","top_none_comments = []\n","\n","for i in range(1000):\n","    top_ah_comments.append(v[i])\n","    top_none_comments.append(v[-(i + 1)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyBjECXJHN1H"},"source":["# Random shuffle of top comments\n","np.random.RandomState(seed=42).shuffle(top_ah_comments)\n","np.random.RandomState(seed=42).shuffle(top_none_comments)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a91Mbq-hHg_c"},"source":["## Dividing the comments into small, medium and large categories"]},{"cell_type":"code","metadata":{"id":"_skn-kAzHc27"},"source":["small_ah_comments = []\n","medium_ah_comments = []\n","large_ah_comments = []\n","\n","small_none_comments = []\n","medium_none_comments = []\n","large_none_comments = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5iF4W_YHw5b"},"source":["for x in top_ah_comments:\n","    c = len(x[1][0].strip().split())\n","    if c < 38:\n","        small_ah_comments.append(x)\n","    elif c >= 80:\n","        large_ah_comments.append(x)\n","    else:\n","        medium_ah_comments.append(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVlA9Se9HzTx"},"source":["for x in top_none_comments:\n","    c = len(x[1][0].strip().split())\n","    if c < 34:\n","        small_none_comments.append(x)\n","    elif c >= 72:\n","        large_none_comments.append(x)\n","    else:\n","        medium_none_comments.append(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb7U-72qH2ig"},"source":["total_groups_possible = int(len(small_ah_comments) / 5)\n","total_groups_possible = min(total_groups_possible, int(len(small_none_comments) / 5))\n","total_groups_possible = min(total_groups_possible, int(len(large_ah_comments) / 2))\n","total_groups_possible = min(total_groups_possible, int(len(large_none_comments) / 2))\n","total_groups_possible = min(total_groups_possible, int(len(medium_ah_comments) / 3))\n","total_groups_possible = min(total_groups_possible, int(len(medium_none_comments) / 3))\n","print(total_groups_possible)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gj69xwp1IC1l"},"source":["## Making balanced groups with 20 comments each"]},{"cell_type":"code","metadata":{"id":"ypOI_i1sH7cf"},"source":["groups = []\n","\n","small_ah_count = 0\n","small_none_count = 0\n","medium_ah_count = 0\n","medium_none_count = 0\n","large_ah_count = 0\n","large_none_count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRzK56AEIMe7"},"source":["for i in range(96):\n","    group = []\n","    for j in range(5):\n","        group.append(small_ah_comments[small_ah_count])\n","        small_ah_count += 1\n","        group.append(small_none_comments[small_none_count])\n","        small_none_count += 1\n","    for j in range(3):\n","        group.append(medium_ah_comments[medium_ah_count])\n","        medium_ah_count += 1\n","        group.append(medium_none_comments[medium_none_count])\n","        medium_none_count += 1\n","    for j in range(2):\n","        group.append(large_ah_comments[large_ah_count])\n","        large_ah_count += 1\n","        group.append(large_none_comments[large_none_count])\n","        large_none_count += 1  \n","    np.random.RandomState(seed=42).shuffle(group)\n","    groups.append(group)\n","np.random.RandomState(seed=42).shuffle(groups)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26LnCSD7IQxT"},"source":["delimiter = '@#@#@'\n","delimiter2 = '##$$##@@'\n","\n","# Each group is balanced class-wise as well as length-wise and has 20 comments\n","\n","low, high = 0, 1  # index of groups which will be used in the form\n","\n","addr = 'https://utkarsh512.github.io/pages/staticPages/comment{}.txt'\n","\n","ctr = 0\n","\n","with open('/content/gdrive/MyDrive/DL/CreateDebate/Politics/CommentsForGoogleForm.txt', 'w', encoding='utf-8') as f:\n","    content = []\n","    for i in range(low, high):\n","        content_ = []\n","        for x in groups[i]:\n","            content_.append(f'{x[1][0].strip()}{delimiter2}{addr.format(ctr)}')\n","            ctr += 1\n","        content_ = delimiter.join([x for x in content_])\n","        content.append(content_)\n","    content = delimiter.join([x for x in content])\n","    content = re.sub(\"\\s+\", \" \", content)\n","    f.write(content)\n","    print(content)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9FnyGahaIlp"},"source":["# Constructing static webpages for comments for context\n","\n","addr = '/content/gdrive/MyDrive/DL/CreateDebate/Politics/staticPages/comment{}.txt'\n","\n","ctr = 0\n","\n","for i in range(low, high):\n","    for x in groups[i]:\n","        with open(addr.format(ctr), 'w', encoding='utf-8') as f:\n","            f.write(str(threads[x[3]]))\n","            ctr += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzO-O7kQJeQp"},"source":["## Uploading these comments on Google Form\n","* Download this `.txt` file and upload it on the [website](https://github.com/utkarsh512/utkarsh512.github.io/tree/master/docs/dataset).\n","* Then, visit [this](https://github.com/utkarsh512/Hate-Speech/tree/main/utils/autoform)."]},{"cell_type":"code","metadata":{"id":"svwpMfl4Kn9t"},"source":["punyajoysaha1998@gmail.com \n","mithun.rcciit@gmail.com"],"execution_count":null,"outputs":[]}]}