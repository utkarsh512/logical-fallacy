{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"35-Ad-Hominem-Detection-Twitter-Data.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMN2bMshQkQ8P1w6lzHZweX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["__Objective__: To train TensorFlow BERT model on `Change My View` dataset, and classify the tweets as `ad hominem` and `not ad hominem`.\n","\n","__Runtime__: GPU"],"metadata":{"id":"tGjjcQqH_FKe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaBA8Oj2-5ND"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","source":["# BERT training"],"metadata":{"id":"lsWWKWUR_27o"}},{"cell_type":"code","source":["!pip install -q tensorflow-text\n","!pip install -q tf-models-official"],"metadata":{"id":"fWzgiPn__hkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optmizer\n","\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"fvj2fjTf_-Mi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Allow memory growth for the GPU\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"metadata":{"id":"Oo-CQqfNAJDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","if tf.test.is_gpu_available():\n","  strategy = tf.distribute.MirroredStrategy()\n","  print('Using GPU')\n","else:\n","  raise ValueError('Running on CPU is not recomended.')"],"metadata":{"id":"0ZFpC15UAOW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n","tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","train_dir = '/content/gdrive/MyDrive/DL/dataset/bert/train'"],"metadata":{"id":"QS0r37fAARIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 32\n","seed = 42\n","\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    seed=seed)\n","\n","class_names = raw_train_ds.class_names\n","train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"5N8EGDpnAVUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_classifier_model():\n","    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","    encoder_inputs = preprocessing_layer(text_input)\n","    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","    outputs = encoder(encoder_inputs)\n","    net = outputs['pooled_output']\n","    net = tf.keras.layers.Dropout(0.1)(net)\n","    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","    return tf.keras.Model(text_input, net)"],"metadata":{"id":"LfUwkAtuAaf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with strategy.scope():\n","    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","    metrics = tf.metrics.BinaryAccuracy()\n","    epochs = 3\n","    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","    num_train_steps = steps_per_epoch * epochs\n","    num_warmup_steps = int(0.1*num_train_steps)\n","\n","    init_lr = 3e-5\n","    optimizer = optimization.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')\n","    classifier_model = build_classifier_model()\n","    classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","    print(f'Training model with {tfhub_handle_encoder}')\n","    history = classifier_model.fit(x=train_ds, epochs=epochs)"],"metadata":{"id":"Gqvikkv4AiiB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classifying tweets"],"metadata":{"id":"kR5nH5m1H_YX"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np \n","from tqdm import tqdm "],"metadata":{"id":"ZssYGCXZInv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_base_addr = '/content/gdrive/MyDrive/DL/Twitter/raw/{}.csv'          # path to the raw tweets which have been just scraped\n","filt_base_addr = '/content/gdrive/MyDrive/DL/Twitter/classified/{}.csv'  # path where classfied tweets CSV will be stored\n","pages = ['nytimes', 'npr', 'breitbart', 'foxnews']                       # pages we are considering"],"metadata":{"id":"6b_hGsDSAles"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dict()\n","for e in pages:\n","    df[e] = pd.read_csv(raw_base_addr.format(e))"],"metadata":{"id":"x30UPbqpJAuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# return the classification score for given text\n","# note that `text` must be pre-processed before score-computation\n","# score lies between 0 [ad hominem] and 1 [non ad hominem]\n","\n","def score(text):\n","    text = [text]\n","    result = tf.sigmoid(classifier_model(tf.constant(text)))\n","    score = float(result[0][0])\n","    return score"],"metadata":{"id":"kNaxno2_JaVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# computing scores for all the tweets\n","# might take time!\n","\n","for e in tqdm(pages):\n","    df[e]['score'] = df[e]['pptweet'].apply(score)"],"metadata":{"id":"pEb96GpLJ25E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# assign labels to the tweets\n","\n","def get_label(s):\n","    return 'AH' if s < 0.5 else 'None'\n","\n","for e in tqdm(pages):\n","    df[e]['label'] = df[e]['score'].apply(get_label)"],"metadata":{"id":"q8_SNQDtKOPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the classified tweets as CSV\n","\n","for e in pages:\n","    df[e].to_csv(filt_base_addr.format(e), index=False)"],"metadata":{"id":"bSQTG2d8P9Si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"EEw0V3qXQdXd"},"execution_count":null,"outputs":[]}]}