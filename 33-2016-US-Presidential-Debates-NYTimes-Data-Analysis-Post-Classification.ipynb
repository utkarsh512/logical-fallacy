{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"33-2016-US-Presidential-Debates-NYTimes-Data-Analysis-Post-Classification.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPeavZ3rbtYfqEi46SyMB1f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PTgzAST2HVrF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","source":["## Training PyTorch BERT model for heat map generation"],"metadata":{"id":"3l6wdsIvMFkD"}},{"cell_type":"code","source":["import os\n","\n","def read_split(dir):\n","    texts = []\n","    labels = []\n","    with open(dir, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()[1:]\n","        for line in lines:\n","            line = line.strip().split(\",\")\n","            assert(len(line) == 2)\n","            label = line[0].strip()\n","            text = line[1].strip()\n","            texts.append(text)\n","            if label == 'AH':\n","                labels.append(1)\n","            else:\n","                labels.append(0)\n","    return texts, labels"],"metadata":{"id":"FJ0li08DL6hG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_texts, train_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/train.csv')\n","test_texts, test_labels = read_split('/content/gdrive/MyDrive/DL/dataset/pytorch/test.csv')"],"metadata":{"id":"u3L16oqcMMkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","from transformers import BertTokenizer, BertForSequenceClassification"],"metadata":{"id":"P053OHsGMR_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","max_seq_length = 64\n","train_encodings = tokenizer(train_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")\n","test_encodings = tokenizer(test_texts, truncation=True, max_length=max_seq_length, padding=\"max_length\")"],"metadata":{"id":"KpzwP0D_MblS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"],"metadata":{"id":"QjivIs68Moky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from transformers import AdamW"],"metadata":{"id":"WjW3Rj8vMuLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"7uCNpx0NM0fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)\n","model.train()"],"metadata":{"id":"gxiptJc6M4mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","optim = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"if4UOO0QNCmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","for epoch in range(3):\n","    for batch in tqdm(train_loader):\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()"],"metadata":{"id":"eTxs2HzWNHGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","!pip install datasets\n","from datasets import load_metric"],"metadata":{"id":"8Ghsr6X9OXfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric= load_metric(\"accuracy\")\n","model.eval()\n","eval_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","for batch in eval_loader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"],"metadata":{"id":"RNBNm_vrOebW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './utkbert/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"metadata":{"id":"xjrXiovxOkyB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading FB Data"],"metadata":{"id":"ggdt6hUKOtFj"}},{"cell_type":"code","source":["import pandas as pd "],"metadata":{"id":"FQtQcLu1H02u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reading the file containing the classified comments\n","df = pd.read_csv('/content/gdrive/MyDrive/DL/Facebook/fbscraper/nytimes/2016/2016c.csv')"],"metadata":{"id":"NSk3tYyDH9OP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"XTJRm0maICF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filtering the comments who have confidence score in range (0.4, 0.6)\n","reqdf = df[df['score'] > 0.4]\n","reqdf = reqdf[reqdf['score'] < 0.6]"],"metadata":{"id":"YvpOfm6CIDlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reqdf"],"metadata":{"id":"9_ked0BoImCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# percentage of comment in 0.4-0.6 range \n","\n","len(reqdf) / len(df) * 100"],"metadata":{"id":"kk2iRgrgIvJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = list(reqdf['processedText'])\n","scores = list(reqdf['score'])"],"metadata":{"id":"hrn8LTNzI9fK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# separating adhominem comments from the rest\n","adHominem = list()\n","none = list()\n","\n","for text, score in zip(texts, scores):\n","    if score < 0.5:\n","        adHominem.append(text)\n","    else:\n","        none.append(text)"],"metadata":{"id":"jhYPAuVCJxAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(adHominem), len(none)"],"metadata":{"id":"mNdc2vTCJypt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","import re"],"metadata":{"id":"mvpLd73HKi9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_version = 'utkbert'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"metadata":{"id":"iNGyojjbPHD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INTENSITY = 70\n","\n","def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"metadata":{"id":"qxUiSHnlPJnz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return clean_apos(W, A)\n","\n","def clean_apos(w, a):\n","    W = []\n","    A = []\n","    ctr = 0\n","    while ctr != len(w):\n","        if w[ctr] == '\\'':\n","            W[-1] += w[ctr] + w[ctr + 1]\n","            A[-1] = min(INTENSITY, A[-1] + a[ctr] + a[ctr + 1])\n","            ctr += 2\n","        else:\n","            W.append(w[ctr])\n","            A.append(a[ctr])\n","            ctr += 1\n","    return W, A"],"metadata":{"id":"Q2XF843_PSBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def top_three_tokens(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    for w in xx:\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                scores[j] = INTENSITY\n","    return words, scores"],"metadata":{"id":"HzyosEjsPV7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_word(word_list):\n","  new_word_list = []\n","  for word in word_list:\n","    for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\",  \"{\", \"}\"]:\n","      if latex_sensitive in word:\n","        word = word.replace(latex_sensitive, '\\\\'+latex_sensitive)\n","    new_word_list.append(word)\n","  return new_word_list"],"metadata":{"id":"bM6s0KL8PZT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","def sanitize(text):\n","    text = text.lower()\n","    text = re.sub(\"\\s+\", \" \", text)  # converting space-like character to single white space\n","    text = re.sub(\"\\u2018\", '\\'', text)    # encoding apostrophe to X\n","    text = re.sub(\"\\u2019\", '\\'', text)    # encoding apostrophe to X\n","    xx = ''\n","    for x in text:\n","        if x in string.punctuation and x != '\\'':\n","            xx += ' '\n","        xx += x\n","    text = xx\n","    text = text.split()\n","    new_text = []\n","    for x in text:\n","        ok = False\n","        for y in x:\n","            ok = ok or y.isalnum()\n","        if ok:\n","            for c in string.punctuation:\n","                x = x.strip(c)\n","            new_text.append(x)\n","    return ' '.join(clean_word(new_text))"],"metadata":{"id":"wdtydMm_Pbdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sanitize(\"'Lol, who ain't you bro??'\")"],"metadata":{"id":"uoexaVFYPemg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["header = r'''\\documentclass[10pt,a4paper]{article}\n","\\usepackage[left=1.00cm, right=1.00cm, top=1.00cm, bottom=2.00cm]{geometry}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}''' + '\\n\\n'\n","\n","footer = r'''\\end{CJK*}\n","\\end{document}'''\n","\n","def heatmap(word_list, attention_list, label_list, latex_file, title, batch_size=20, color='blue'):\n","    '''Routine to generate attention heatmaps for given texts\n","    ---------------------------------------------------------\n","    Input:\n","    :param word_list: array of texts\n","    :param attention_list: array of attention scores for each text\n","    :param label_list: label for each text\n","    :param latex_file: name of the latex file\n","    :param title: title of latex file\n","    :param batch_size: Number of comments in each batch\n","    '''\n","    with open(latex_file, 'w', encoding='utf-8') as f:\n","        f.write(header)\n","        f.write('\\\\section{%s}\\n\\n' % title)\n","\n","        n_examples = len(word_list)\n","        n_batches = n_examples // batch_size\n","\n","        for i in range(n_batches):\n","            batch_word_list = word_list[i * batch_size: (i + 1) * batch_size]\n","            batch_attention_list = attention_list[i * batch_size: (i + 1) * batch_size]\n","            batch_label_list = label_list[i * batch_size: (i + 1) * batch_size]\n","            f.write('\\\\subsection{Batch %d}\\n\\n' % (i + 1))\n","            for j in range(batch_size):\n","                f.write('\\\\subsubsection{Comment %d - %s}\\n\\n' % (j + 1, batch_label_list[j]))\n","                sentence = batch_word_list[j]\n","                score = batch_attention_list[j]\n","                assert len(sentence) == len(score)\n","                f.write('\\\\noindent')\n","                for k in range(len(sentence)):\n","                    f.write('\\\\colorbox{%s!%s}{' % (color, score[k]) + '\\\\strut ' + sentence[k] + '} ')\n","                f.write('\\n\\n')\n","\n","        f.write(footer)"],"metadata":{"id":"fhUQgG7zPhAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random100AdHominem = adHominem[:100]\n","random100None = none[:100]"],"metadata":{"id":"NyRTkX2XPnIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vTexts = list()\n","vScores = list()\n","\n","for text in tqdm(random100None):\n","    sent = sanitize(text)\n","    try:\n","        texts_, scores_ = top_three_tokens(sent)\n","        vTexts.append(texts_)\n","        vScores.append(scores_)\n","    except:\n","        pass"],"metadata":{"id":"0-fCvygRP470"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vTexts = vTexts[:50]\n","vScores = vScores[:50]"],"metadata":{"id":"yiqtQGn0Qiyw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"buC9w-cuRqS3"}},{"cell_type":"code","source":["heatmap(vTexts, vScores, ['Not ad hominem'] * len(vTexts), 'none.tex', 'Non ad hominem comments having low confidence score', color='orange')"],"metadata":{"id":"jnlHZbQvQ4FF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"R2bjLeNKRW33"},"execution_count":null,"outputs":[]}]}