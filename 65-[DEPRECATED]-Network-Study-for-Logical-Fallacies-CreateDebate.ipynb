{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNW8QSqriwyGOLiULY6qAz7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Experimental Setup"],"metadata":{"id":"GkUa4FGRP4gj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hb3116e6PPtp"},"outputs":[],"source":["# Mount Google drive to Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"ix2egePoPxuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib\n","from   matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","from thread import (Comment,\n","                    Thread)\n","\n","from copy import deepcopy\n","import pickle\n","from tqdm import tqdm\n","from pprint import pprint\n","\n","import networkx as nx"],"metadata":{"id":"aMN8QmpjQHtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup for plotting\n","sns.set(style='darkgrid')\n","matplotlib.rcParams['figure.dpi'] = 120\n","matplotlib.rcParams['font.size'] = 18\n","matplotlib.rcParams['figure.figsize'] = (12, 5)"],"metadata":{"id":"ZZrUx1-pQfOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"qtaVXEYDRXBN"}},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"oECZRQE5RJC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"EWWgjb-RRddg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading CreateDebate profile characteristics into dataframe\n","df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)\n","\n","# Extract useful characteristics\n","reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"AEH3ju_1RfMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def profile_characteristics_stats(user_subset):\n","    \"\"\"\n","    Returns average and standard deviation of profile characteristics for \n","    given subset of users.\n","\n","    :param user_subset: Iterable containing usernames\n","\n","    >>> avgs, stds = profile_characterisitics_stat(user_subset)\n","    >>> rewards_avg, efficiency_avg, n_allies_avg, n_enemies_avg, n_hostiles_avg = avgs\n","    >>> rewards_std, efficiency_std, n_allies_std, n_enemies_std, n_hostiles_std = stds\n","\n","    Note that profile characteristics for some users might not be present in our\n","    dataset as some users might have deleted their account when we scraped the\n","    forum to obtain these characteristics.\n","    \"\"\"\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        try:\n","            rewards_.append(reward_points_map[user])\n","        except:pass\n","        try:\n","            efficiency_.append(efficiency_map[user])\n","        except:pass\n","        try:\n","            n_allies.append(allies_map[user])\n","        except:pass\n","        try:\n","            n_enemies.append(enemies_map[user])\n","        except:pass\n","        try:\n","            n_hostiles.append(hostiles_map[user])\n","        except:pass\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"zOxD8MvhRsvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"BAZH3ctQR0Hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a list of all comment thread representative to build user network graph\n","\n","threads = []\n","\n","for category in categories_selected:\n","    reader_addr = f'/content/gdrive/MyDrive/DL/CreateDebate/{category}/threads.log'\n","    reader = open(reader_addr, 'rb')\n","    try:\n","        while True:\n","            e = pickle.load(reader)\n","            threads.append(e)\n","    except:\n","        reader.close()"],"metadata":{"id":"raQsFCjWR1w2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_graph(user_subset, n1 = 0, n2 = 0):\n","    \"\"\"\n","    Builds user network graph from hyper-parameters n1 and n2\n","    \n","    Inputs\n","    ------\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    Output\n","    ------\n","    (\n","        author_map: dict,\n","        reverse_map: list,\n","        author_count: int, \n","        graph: nx.DiGraph,\n","        matrix: list\n","    )\n","    \"\"\"\n","\n","    # Uses globally defined `threads` variable to construct this dictionary.\n","    # You may choose which categories to be included while building `threads`\n","\n","    # key  : author name\n","    # value: count of level-1 comments\n","    athr = dict()\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","    \n","    # Filter those authors who satisfy the contraint on number of level-1 comments\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    # Now use `athr` for storing count of direct replies\n","    # key  : author name\n","    # value: count of direct replies received\n","    athr = dict()\n","\n","    # Depth-first search utility to get number of direct replies for each author\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    # Traverse thread-tree to get number of direct replies for each author\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","    \n","    # Filter authors who now satify both the contrainsts on count of \n","    # - level-1 comments\n","    # - direct replies\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    # key  : author name\n","    # value: corresponing node number in the support/dispute network\n","    author_map = dict()\n","\n","    # To get author name for node number\n","    reverse_map = [\"\" for _ in range(len(A))]\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","    \n","    # Weighted adjacency matrices for user network\n","    # Weight for directed edge b/w Node A and Node B corresponsds to the number\n","    # of times Node A directly-replied Node B.\n","    matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    # Depth-first search utility to build the adjacency matrices for graph.\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        \n","        if cur_author in author_map:\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                if nxt_author in author_map:\n","                    nxt_author_id = author_map[nxt_author]\n","                    matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","        \n","    # Create NetworkX graphs from the adjacency matrices.\n","    # We need nx graphs in order to get various network stats provided in nx\n","    # library.\n","    graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if matrix[i][j] != 0:\n","                graph.add_weighted_edges_from([(i, j, matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, graph, matrix)"],"metadata":{"id":"D_sEJcoVR_V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct global user network for entire CreateDebate corpus\n","user_map, user_reverse_map, user_count, Graph, Matrix = build_graph(user_list)"],"metadata":{"id":"EB_5VnblSCC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_reciprocity_stats(user_subset):\n","    \"\"\"\n","    Returns reciprocity for given subset of users in local network\n","\n","    >>> r = get_reciprocity_stats(user_subset)\n","    \"\"\"\n","    _, _, _, Graph_, _ = build_graph(user_subset)\n","\n","    try:\n","        r = nx.algorithms.reciprocity(Graph_)\n","    except:\n","        r = None\n","\n","    return r"],"metadata":{"id":"v0W6ZAEOSF9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing centrality value for each node from global network.\n","# This will be used for computing stats for user subset.\n","centrality_dict = nx.algorithms.centrality.degree_centrality(Graph)"],"metadata":{"id":"2lWm29BBSQpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_centrality_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of degree centrality for given user \n","    subset in the global network.\n","\n","    >>> c_avg, c_std = get_centrality_stats(user_subset)\n","    \"\"\"\n","    c = []\n","\n","    for user in user_subset:\n","        try:\n","            c.append(centrality_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(c), np.std(c)"],"metadata":{"id":"xS_PKKEnSS73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing clustering coeffieient for each node from global network. \n","# This will be used for computing stats for user subset.\n","clustering_dict = nx.algorithms.cluster.clustering(Graph)"],"metadata":{"id":"n1VwEEwdSVri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_clustering_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of clustering coefficient for given user \n","    subset in the global network.\n","\n","    >>> c_avg, c_std = get_clustering_stats(user_subset)\n","    \"\"\"\n","    c = []\n","\n","    for user in user_subset:\n","        try:\n","            c.append(clustering_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(c), np.std(c)"],"metadata":{"id":"adLNTGDESXTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_stats(user_subset):\n","    n                          = len(user_subset)\n","    r                          = get_reciprocity_stats(user_subset) \n","    deg_avg, deg_std           = get_centrality_stats(user_subset)\n","    clu_avg, clu_std           = get_clustering_stats(user_subset)\n","    user_chr_avg, user_chr_std = profile_characteristics_stats(user_subset) \n","\n","    print('Size: %d' % n)\n","    print('Graph reciprocity: %.2f' % r)\n","\n","    print('Graph degree centrality: %.5f ± %.5f' % (deg_avg, deg_std))\n","\n","    print('Graph clustering coeff: %.2f ± %.2f' % (clu_avg, clu_std))\n","\n","    print('Reward points: %.2f ± %.2f' % (user_chr_avg[0], user_chr_std[0]))\n","    print('Efficiency   : %.2f ± %.2f' % (user_chr_avg[1], user_chr_std[1]))\n","    print('# Allies     : %.2f ± %.2f' % (user_chr_avg[2], user_chr_std[2]))\n","    print('# Enemies    : %.2f ± %.2f' % (user_chr_avg[3], user_chr_std[3]))\n","    print('# Hostiles   : %.2f ± %.2f' % (user_chr_avg[4], user_chr_std[4]))"],"metadata":{"id":"XQlWLgrKShdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Network study"],"metadata":{"id":"Jn8U71LvWz9m"}},{"cell_type":"code","source":["for_against_debates = dict()\n","perspective_debates = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates[cat] = list()\n","    perspective_debates[cat] = list()\n","\n","    for comment in comments[cat]:\n","        if comment['polarity'] == 'Not Available':\n","            perspective_debates[cat].append(deepcopy(comment))\n","        else:\n","            for_against_debates[cat].append(deepcopy(comment))"],"metadata":{"id":"NA2iFbOZSla2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For now, only Politics users are considered!\n","for_against_user_set = set()\n","perspective_user_set = set()\n","\n","for comment in for_against_debates['politics2']:\n","    for_against_user_set.add(comment['author'])\n","\n","for comment in perspective_debates['politics2']:\n","    perspective_user_set.add(comment['author'])\n","\n","print(f'{len(for_against_user_set)} & {len(perspective_user_set)}')"],"metadata":{"id":"eBKnA2PRW5FY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding labels used while classification.\n","# Refer to notebook#63.\n","label_map = {\n","    'faulty generalization': 0,\n","    'false causality': 1,\n","    'circular reasoning': 2, \n","    'ad populum': 3,\n","    'ad hominem': 4,\n","    'fallacy of logic': 5,\n","    'appeal to emotion': 6,\n","    'false dilemma': 7,\n","    'equivocation': 8,\n","    'fallacy of extension': 9,\n","    'fallacy of relevance': 10,\n","    'fallacy of credibility': 11,\n","    'intentional': 12,\n","}\n","\n","inverse_label_map = dict()\n","for k, v in label_map.items():\n","    inverse_label_map[v] = k"],"metadata":{"id":"HWSRU_r_S9Qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_obj(file_path):\n","    \"\"\"Load a pickled object from given path\n","    :param file_path: Path to the pickle file of the object\n","    :type file_path: string\n","    \"\"\"\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)"],"metadata":{"id":"6I8IWo5WTDiC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load labels and scores obtained during classification \n","for_against_labels_and_scores = \\\n","  load_obj('/content/gdrive/MyDrive/Temp/63-for_against_labels_and_scores.pkl')\n","perspective_labels_and_scores = \\\n","  load_obj('/content/gdrive/MyDrive/Temp/63-perspective_labels_and_scores.pkl')"],"metadata":{"id":"KJ0_0sfzTFux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_logical = dict()\n","perspective_logical = dict()\n","# key: logical fallacy class\n","# value: list of comments \n","\n","for k in label_map.keys():\n","    for_against_logical[k] = list()\n","    perspective_logical[k] = list()"],"metadata":{"id":"4vQeNX2_TH2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for comment, labels_and_scores in zip(for_against_debates['politics2'], for_against_labels_and_scores):\n","    label = int(labels_and_scores[0]['label'].lstrip('LABEL_'))\n","    for_against_logical[inverse_label_map[label]].append(comment)"],"metadata":{"id":"KoCC1jaSTx3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for comment, labels_and_scores in zip(perspective_debates['politics2'], perspective_labels_and_scores):\n","    label = int(labels_and_scores[0]['label'].lstrip('LABEL_'))\n","    perspective_logical[inverse_label_map[label]].append(comment)"],"metadata":{"id":"a5wjexxoUcjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_user_subset_for_against(cls, flag=0):\n","    user_subset = set()\n","    for comment in for_against_logical[cls]:\n","        user_subset.add(comment['author'])\n","    if flag:\n","        user_subset = for_against_user_set - user_subset\n","    return user_subset\n","\n","def get_user_subset_perspective(cls, flag=0):\n","    user_subset = set()\n","    for comment in perspective_logical[cls]:\n","        user_subset.add(comment['author'])\n","    if flag:\n","        user_subset = perspective_user_set - user_subset\n","    return user_subset"],"metadata":{"id":"aRtDWof7VR-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes_selected = ('faulty generalization', 'ad hominem', 'fallacy of logic', 'intentional')"],"metadata":{"id":"qtDdj2vZUyev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_logical_cls = 'ad hominem'"],"metadata":{"id":"Lz_4Av4RV-xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = get_user_subset_for_against(cls=current_logical_cls, flag=0)\n","B = get_user_subset_for_against(cls=current_logical_cls, flag=1)\n","C = get_user_subset_perspective(cls=current_logical_cls, flag=0)\n","D = get_user_subset_perspective(cls=current_logical_cls, flag=1)\n","\n","print(len(A), len(B), len(C), len(D))"],"metadata":{"id":"XOImtVw6Ye7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(D)"],"metadata":{"id":"iHGYqRf1Y8To"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HHwsN3__ZBnF"},"execution_count":null,"outputs":[]}]}