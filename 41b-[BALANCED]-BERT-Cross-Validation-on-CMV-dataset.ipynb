{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"41b-[BALANCED]-BERT-Cross-Validation-on-CMV-dataset.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM1VjyDf3HOtDJbQWw5yDgU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["__Objective__: Computing cross-validation score of CMV dataset using BERT\n","\n","__Runtime__: GPU"],"metadata":{"id":"lbkoKUwqkkob"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-B8jYpqkRZy"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"7iuWBeeukyM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers \n","!pip install datasets"],"metadata":{"id":"Vr_Xw5Ceo7Xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm \n","import pickle as pkl\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from datasets import load_metric"],"metadata":{"id":"KI2ZyD7Uk11q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading CMV dataset and creating folds"],"metadata":{"id":"HThuxvXelb6Q"}},{"cell_type":"code","source":["def read_dataset(dir):\n","    \"\"\"Reading texts and labels from dataset\"\"\"\n","    texts_labels = []\n","    with open(dir, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()[1:]\n","        for line in lines:\n","            line = line.strip().split(\",\")\n","            assert(len(line) == 2)\n","            label = line[0].strip()\n","            text = line[1].strip()\n","            if label == 'AH':\n","                texts_labels.append((text, 1))\n","            else:\n","                texts_labels.append((text, 0))\n","    return texts_labels"],"metadata":{"id":"TcFb23SOlSnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = read_dataset('/content/gdrive/MyDrive/DL/dataset/pytorch/train.csv')\n","dataset.extend(read_dataset('/content/gdrive/MyDrive/DL/dataset/pytorch/test.csv'))"],"metadata":{"id":"K2bkiLyRmF9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataset)"],"metadata":{"id":"9T2dQfclmb1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_and_merge(dataset):\n","    ah = []\n","    none = [] \n","    for ctext, clabel in dataset:\n","        if (clabel):\n","            ah.append((ctext, clabel)) \n","        else:\n","            none.append((ctext, clabel)) \n","    i = 0 \n","    j = 0 \n","    new_dataset = []\n","    while (i < len(ah) and j < len(none)):\n","        new_dataset.append(ah[i]) \n","        new_dataset.append(none[j])\n","        i += 1\n","        j += 1\n","    while (i < len(ah)):\n","        new_dataset.append(ah[i]) \n","        i += 1 \n","    while (j < len(none)): \n","        new_dataset.append(none[j]) \n","        j += 1 \n","    return new_dataset"],"metadata":{"id":"2F8NuJcv7xL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# shuffling the dataset \n","dataset = split_and_merge(dataset)\n","\n","# creating folds \n","n_folds = 10\n","fold_length = len(dataset) // n_folds\n","folds = [dataset[i * fold_length: (i + 1) * fold_length] for i in range(n_folds)]"],"metadata":{"id":"jiaILrSPmdpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating tokenizer to get encodings\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n","max_seq_length = 64"],"metadata":{"id":"w7ksqj62oWHT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folds_text = [] \n","folds_label = [] \n","for i in range(n_folds):\n","    texts = []\n","    labels = [] \n","    for ftext, flabel in folds[i]:\n","        texts.append(ftext)\n","        labels.append(flabel)\n","    folds_text.append(texts)\n","    folds_label.append(labels) "],"metadata":{"id":"Ma1GN0Rxp9MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generating encodings\n","folds_encoding = [] \n","for i in range(n_folds):\n","    folds_encoding.append(tokenizer(folds_text[i], truncation=True, max_length=max_seq_length, padding=\"max_length\"))"],"metadata":{"id":"jmvjI4myoXcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"8mKWitUEpydF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = []\n","test_dataset = [] \n","\n","for i in tqdm(range(n_folds)):\n","    cur_text = [] \n","    cur_label = [] \n","    for j in range(n_folds):\n","        if i == j:\n","            continue \n","        cur_text.extend(folds_text[j]) \n","        cur_label.extend(folds_label[j]) \n","    cur_encoding = tokenizer(cur_text, truncation=True, max_length=max_seq_length, padding=\"max_length\")\n","    train_dataset.append(CustomDataset(cur_encoding, cur_label)) \n","    test_dataset.append(CustomDataset(folds_encoding[i], folds_label[i]))"],"metadata":{"id":"FvGmLOgTrEZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"IlLC2LoNsFoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_acc = [] \n","bert_f1 = []"],"metadata":{"id":"3LCunD8Q-Mia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from copy import deepcopy"],"metadata":{"id":"9pWq_y2HCA-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(fold_id):\n","    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","    model.to(device)\n","    model.train()\n","\n","    train_loader = DataLoader(train_dataset[fold_id], batch_size=64, shuffle=True)\n","    optim = AdamW(model.parameters(), lr=5e-5)\n","\n","    for epoch in range(3):\n","        for batch in tqdm(train_loader):\n","            optim.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs[0]\n","            loss.backward()\n","            optim.step()\n","\n","    acc = load_metric(\"accuracy\")\n","    f1 = load_metric(\"f1\")\n","    model.eval()\n","    eval_loader = DataLoader(test_dataset[fold_id], batch_size=64, shuffle=False)\n","    for batch in eval_loader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n","        f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n","    \n","    acc_ = deepcopy(acc.compute())\n","    f1_ = deepcopy(f1.compute())\n","    bert_acc.append(acc_)\n","    bert_f1.append(f1_)\n","    print(acc_)\n","    print(f1_)"],"metadata":{"id":"6H9tKnx_scU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","    evaluate(i)\n","    print(f'\\n\\n\\n{\"-\" * 100}\\n\\n\\n')"],"metadata":{"id":"HbckD59l1aWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lst_acc = [] \n","lst_f1 = [] \n","\n","for i in range(10):\n","    lst_acc.append(bert_acc[i]['accuracy'])\n","    lst_f1.append(bert_f1[i]['f1'])"],"metadata":{"id":"V5S-ylJGYGX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df = pd.DataFrame({'Accuracy': lst_acc, 'F1': lst_f1})"],"metadata":{"id":"hYeSdP4AveTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"8t4ufWT_HmC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"Nc_tQ8E2HmvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.boxplot()"],"metadata":{"id":"D3ZbvDXnHrhh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VooOQSJEYgaY"},"execution_count":null,"outputs":[]}]}