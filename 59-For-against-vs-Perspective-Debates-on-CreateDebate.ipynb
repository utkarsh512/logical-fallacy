{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM0AXRMlg4pCPkpcWmchBcx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"kSM9O5nVwFaN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEIz0zz1o_J8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"V7EYaP2apoJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"OmPAnM-9pz69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cpnet"],"metadata":{"id":"c26FaPnkp2zA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","from   transformers             import BertModel, BertTokenizer\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"OKcYBXeQp_kL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom routine to clean texts scraped from Web.\n","# It removes hyperlinks, punctuation marks (except apostrophe)\n","\n","tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"t1Hklyf_qGfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"5jktup5qqTJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"4ZBeSw5BqWP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_score_comments = dict()\n","\n","for cat in categories_selected:\n","    for comment in comments[cat]:\n","        ah_score_comments[comment['id']] = 1 - comment['score']"],"metadata":{"id":"0QuzXEKyqbc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_tstring(tstring):\n","    \"\"\"\n","    Parses comment's time to an integer to enable\n","    comparison between comments based on their time of posting\n","    \"\"\"\n","    if tstring == 'Not Available':\n","        raise ValueError('Invalid posting time for parse_tstring')\n","    tstring = tstring.replace('T', '-').replace(':', '-').replace('+', '-').split('-')\n","    return int(''.join(tstring[:-2]))"],"metadata":{"id":"MUVlmrOYqh0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading AH score\n","\n","with open('/content/gdrive/MyDrive/Temp/47-ah-score.pkl', 'rb') as fp:\n","    ah_score = pickle.load(fp)\n","\n","# `ah_score` is a dictionary that contains the ah score of the comments written\n","# by all the users\n","\n","# key: category -> user\n","# value: list of ah_score for given user for given category\n","\n","# value > 0.5 --> ad hominem\n","# value < 0.5 --> non ad hominem"],"metadata":{"id":"3k84vDNdqlCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading CreateDebate profile characteristics into dataframe\n","df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)\n","\n","# Extract useful characteristics\n","reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"IWyfk1Z7qobY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def profile_characteristics_stats(user_subset):\n","    \"\"\"\n","    Returns average and standard deviation of profile characteristics for \n","    given subset of users.\n","\n","    :param user_subset: Iterable containing usernames\n","\n","    >>> avgs, stds = profile_characterisitics_stat(user_subset)\n","    >>> rewards_avg, efficiency_avg, n_allies_avg, n_enemies_avg, n_hostiles_avg = avgs\n","    >>> rewards_std, efficiency_std, n_allies_std, n_enemies_std, n_hostiles_std = stds\n","\n","    Note that profile characteristics for some users might not be present in our\n","    dataset as some users might have deleted their account when we scraped the\n","    forum to obtain these characteristics.\n","    \"\"\"\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        try:\n","            rewards_.append(reward_points_map[user])\n","        except:pass\n","        try:\n","            efficiency_.append(efficiency_map[user])\n","        except:pass\n","        try:\n","            n_allies.append(allies_map[user])\n","        except:pass\n","        try:\n","            n_enemies.append(enemies_map[user])\n","        except:pass\n","        try:\n","            n_hostiles.append(hostiles_map[user])\n","        except:pass\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"p3LIMAe9qvIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Maximum ah score per category per author\n","#   key: category -> author\n","#   value: maximum ah score\n","\n","ah_score_max = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_max[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_max[category][author] = np.max(ah_scores)"],"metadata":{"id":"q0-XXgMpqxTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count = dict()\n","# key: category -> author\n","# value: number of comments written by author in the given forum\n","\n","for category in categories_selected:\n","    comment_count[category] = dict()\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        try:\n","            comment_count[category][author] += 1\n","        except KeyError:\n","            comment_count[category][author] = 1"],"metadata":{"id":"5KAJbd_Lq0iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"Mak6OhZGq7Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_post_time = dict()\n","# key: category -> user\n","# value: post time of the first comment by given user in the given category\n","#        It is an integer as returned by parse_tstring routine\n","\n","for category in categories_selected:\n","    first_post_time[category] = dict()\n","\n","    for comment in comments[category]: \n","        if comment['time'] == 'Not Available':\n","            continue\n","        author = comment['author']\n","        try:\n","            first_post_time[category][author] = min(first_post_time[category][author], parse_tstring(comment['time']))\n","        except KeyError:\n","            first_post_time[category][author] = parse_tstring(comment['time'])"],"metadata":{"id":"x9x9uBapq-CP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_migrated_users(user_subset, categories_1, categories_2, categories_1_origin=True, require_migration=True):\n","    \"\"\"\n","    Returns a list of usernames who migrated from categories_1 to categories_2\n","\n","    If categories_1_origin is True, we will consider all other major categories\n","    to compute post_time_2, so as to ensure that first post by the user is in \n","    categories_1\n","\n","    If require_migration is True, post_time_1 < post_time_2 condition is relaxed\n","    \"\"\"\n","\n","    resultant_list = list()\n","\n","    for user in user_subset:\n","        post_time_1 = 20220101000000\n","        post_time_2 = 20220101000000\n","\n","        if not isinstance(categories_1, set):\n","            categories_1 = set(categories_1)\n","        if not isinstance(categories_2, set): \n","            categories_2 = set(categories_2)\n","        \n","        for category in categories_1:\n","            try:\n","                cur_post_time = first_post_time[category][user]\n","                post_time_1 = min(post_time_1, cur_post_time)\n","            except KeyError:\n","                pass\n","        \n","        for category in categories_2:\n","            try:\n","                cur_post_time = first_post_time[category][user]\n","                post_time_2 = min(post_time_2, cur_post_time) \n","            except KeyError:\n","                pass\n","\n","        if post_time_1 == 20220101000000 or post_time_2 == 20220101000000:\n","            continue\n","\n","        if categories_1_origin:\n","            for category in categories_selected:\n","                if not ((category in categories_1) or (category in categories_2)):\n","                    try:\n","                        cur_post_time = first_post_time[category][user]\n","                        post_time_2 = min(post_time_2, cur_post_time)\n","                    except KeyError:\n","                        pass\n","\n","        if post_time_1 < post_time_2 or not require_migration:\n","            resultant_list.append(user)\n","        \n","    return resultant_list"],"metadata":{"id":"kzEgT2UdrBe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_migrated_users(migration_list, categories_1, categories_2):\n","    \"\"\"\n","    Partitions the users into 4 categories: \n","        AH-AH\n","        AH-NonAH\n","        NonAH-AH\n","        NonAH-NonAH\n","\n","    Users are classified as AH in a given category if they post at least one \n","    ad hominem comment in that category\n","    \n","    Note: migration_list should be obtained using get_migrated_users method\n","    \"\"\"\n","\n","    ah_ah_list = []\n","    ah_nonah_list = []\n","    nonah_ah_list = []\n","    nonah_nonah_list = []\n","\n","    for user in migration_list:\n","        max_score_1 = 0\n","        max_score_2 = 0\n","        for category in categories_1:\n","            max_score_1 = max(max_score_1, ah_score_max[category].get(user, 0))\n","        for category in categories_2:\n","            max_score_2 = max(max_score_2, ah_score_max[category].get(user, 0))\n","\n","        if max_score_1 > 0.5 and max_score_2 > 0.5:\n","            ah_ah_list.append(user)\n","\n","        elif max_score_1 > 0.5 and max_score_2 < 0.5:\n","            ah_nonah_list.append(user)\n","        \n","        elif max_score_1 < 0.5 and max_score_2 > 0.5:\n","            nonah_ah_list.append(user)\n","\n","        elif max_score_1 < 0.5 and max_score_2 < 0.5:\n","            nonah_nonah_list.append(user)\n","        \n","        else:\n","            print(user)\n","\n","    return ah_ah_list, ah_nonah_list, nonah_ah_list, nonah_nonah_list "],"metadata":{"id":"ogK1gPKMrRZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a list of all comment thread representative to build user network graph\n","\n","threads = []\n","\n","for category in categories_selected:\n","    reader_addr = f'/content/gdrive/MyDrive/DL/CreateDebate/{category}/threads.log'\n","    reader = open(reader_addr, 'rb')\n","    try:\n","        while True:\n","            e = pickle.load(reader)\n","            threads.append(e)\n","    except:\n","        reader.close()"],"metadata":{"id":"sCnDexO3rVHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_graph(user_subset, n1 = 0, n2 = 0):\n","    \"\"\"\n","    Builds user network graph from hyper-parameters n1 and n2\n","    \n","    Inputs\n","    ------\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    Output\n","    ------\n","    (\n","        author_map: dict,\n","        reverse_map: list,\n","        author_count: int, \n","        graph: nx.DiGraph,\n","        matrix: list\n","    )\n","    \"\"\"\n","\n","    # Uses globally defined `threads` variable to construct this dictionary.\n","    # You may choose which categories to be included while building `threads`\n","\n","    # key  : author name\n","    # value: count of level-1 comments\n","    athr = dict()\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","    \n","    # Filter those authors who satisfy the contraint on number of level-1 comments\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    # Now use `athr` for storing count of direct replies\n","    # key  : author name\n","    # value: count of direct replies received\n","    athr = dict()\n","\n","    # Depth-first search utility to get number of direct replies for each author\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    # Traverse thread-tree to get number of direct replies for each author\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","    \n","    # Filter authors who now satify both the contrainsts on count of \n","    # - level-1 comments\n","    # - direct replies\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    # key  : author name\n","    # value: corresponing node number in the support/dispute network\n","    author_map = dict()\n","\n","    # To get author name for node number\n","    reverse_map = [\"\" for _ in range(len(A))]\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","    \n","    # Weighted adjacency matrices for user network\n","    # Weight for directed edge b/w Node A and Node B corresponsds to the number\n","    # of times Node A directly-replied Node B.\n","    matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    # Depth-first search utility to build the adjacency matrices for graph.\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        \n","        if cur_author in author_map:\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                if nxt_author in author_map:\n","                    nxt_author_id = author_map[nxt_author]\n","                    matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","        \n","    # Create NetworkX graphs from the adjacency matrices.\n","    # We need nx graphs in order to get various network stats provided in nx\n","    # library.\n","    graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if matrix[i][j] != 0:\n","                graph.add_weighted_edges_from([(i, j, matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, graph, matrix)"],"metadata":{"id":"MM9dUQ17ra2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct global user network for entire CreateDebate corpus\n","user_map, user_reverse_map, user_count, Graph, Matrix = build_graph(user_list)"],"metadata":{"id":"sfTUesqUsbfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_reciprocity_stats(user_subset):\n","    \"\"\"\n","    Returns reciprocity for given subset of users in local network\n","\n","    >>> r = get_reciprocity_stats(user_subset)\n","    \"\"\"\n","    _, _, _, Graph_, _ = build_graph(user_subset)\n","\n","    try:\n","        r = nx.algorithms.reciprocity(Graph_)\n","    except:\n","        r = None\n","\n","    return r"],"metadata":{"id":"LBcwjf5-slC-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing centrality value for each node from global network.\n","# This will be used for computing stats for user subset.\n","centrality_dict = nx.algorithms.centrality.degree_centrality(Graph)"],"metadata":{"id":"kJtfo7SEtnMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_centrality_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of degree centrality for given user \n","    subset in the global network.\n","\n","    >>> c_avg, c_std = get_centrality_stats(user_subset)\n","    \"\"\"\n","    c = []\n","\n","    for user in user_subset:\n","        try:\n","            c.append(centrality_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(c), np.std(c)"],"metadata":{"id":"lghLsqLWty51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing clustering coeffieient for each node from global network. \n","# This will be used for computing stats for user subset.\n","clustering_dict = nx.algorithms.cluster.clustering(Graph)"],"metadata":{"id":"UQs47HOouFk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_clustering_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of clustering coefficient for given user \n","    subset in the global network.\n","\n","    >>> c_avg, c_std = get_clustering_stats(user_subset)\n","    \"\"\"\n","    c = []\n","\n","    for user in user_subset:\n","        try:\n","            c.append(clustering_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(c), np.std(c)"],"metadata":{"id":"G4TYLvGUuP5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_dict(x):\n","    \"\"\"\n","    Normalize elements in given dictionary as\n","        element = (element - min_element) / (max_element - min_element)\n","    \"\"\"\n","    mini = min(x.values())\n","    maxa = max(x.values())\n","\n","    res = dict()\n","\n","    for k, v in x.items():\n","        res[k] = (v - mini) / (maxa - mini)\n","    return res"],"metadata":{"id":"hS8iMuiVu0GU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_array(x):\n","    \"\"\"\n","    Normalize elements in given array as\n","        element = (element - min_element) / (max_element - min_element)\n","    \"\"\"\n","    assert isinstance(x, (list, tuple)), \"Expected a list or tuple\"\n","    mini = min(x)\n","    maxa = max(x)\n","    res = []\n","    for e in x:\n","        res.append((e - mini) / (maxa - mini))\n","    return res"],"metadata":{"id":"7Dd0z76gvGXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_stats(user_subset):\n","    n                          = len(user_subset)\n","    r                          = get_reciprocity_stats(user_subset) \n","    deg_avg, deg_std           = get_centrality_stats(user_subset)\n","    clu_avg, clu_std           = get_clustering_stats(user_subset)\n","    user_chr_avg, user_chr_std = profile_characteristics_stats(user_subset) \n","\n","    print('Size: %d' % n)\n","    print('Graph reciprocity: %.2f' % r)\n","\n","    print('Graph degree centrality: %.5f ± %.5f' % (deg_avg, deg_std))\n","\n","    print('Graph clustering coeff: %.2f ± %.2f' % (clu_avg, clu_std))\n","\n","    print('Reward points: %.2f ± %.2f' % (user_chr_avg[0], user_chr_std[0]))\n","    print('Efficiency   : %.2f ± %.2f' % (user_chr_avg[1], user_chr_std[1]))\n","    print('# Allies     : %.2f ± %.2f' % (user_chr_avg[2], user_chr_std[2]))\n","    print('# Enemies    : %.2f ± %.2f' % (user_chr_avg[3], user_chr_std[3]))\n","    print('# Hostiles   : %.2f ± %.2f' % (user_chr_avg[4], user_chr_std[4]))"],"metadata":{"id":"uSRW3rozvIYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"rCIR58GewLGu"}},{"cell_type":"code","source":["# Partition the dataset into polar and non-polar comments\n","\n","polar_cids = set()\n","\n","comments_p = dict()\n","comments_np = dict()\n","\n","for x in categories_selected:\n","    comments_p[x] = list()\n","    comments_np[x] = list()\n","\n","    for comment in comments[x]:\n","        if comment['polarity'] == 'Not Available':\n","            comments_np[x].append(deepcopy(comment))\n","        else:\n","            comments_p[x].append(deepcopy(comment))"],"metadata":{"id":"FhylWS_TvUkH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x in categories_selected:\n","    print(f'{x} - {len(comments_p[x])} - {len(comments_np[x])}')"],"metadata":{"id":"dp26-V3SwWEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# User characterisitics\n","\n","user_list_p = dict()\n","user_list_np = dict()\n","\n","for cat in categories_selected:\n","    user_list_p[cat] = set()\n","    for comment in comments_p[cat]:\n","        user_list_p[cat].add(comment['author'])\n","\n","    user_list_np[cat] = set()\n","    for comment in comments_np[cat]:\n","        user_list_np[cat].add(comment['author'])\n"],"metadata":{"id":"ocj_HYw4waIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in categories_selected:\n","    print(f'{cat} - {len(user_list_p[cat])} - {len(user_list_np[cat])} - {len(user_list_p[cat] & user_list_np[cat])}')"],"metadata":{"id":"z9PYTXe4wfrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_user_list_p = dict()\n","nonah_user_list_p = dict()\n","\n","ah_user_list_np = dict()\n","nonah_user_list_np = dict()\n","\n","for cat in categories_selected:\n","    ah_user_list_p[cat] = set() \n","    nonah_user_list_p[cat] = set()\n","    for user in user_list_p[cat]:\n","        if ah_score_max[cat][user] > 0.5:\n","            ah_user_list_p[cat].add(user)\n","        else:\n","            nonah_user_list_p[cat].add(user)\n","    \n","    ah_user_list_np[cat] = set()\n","    nonah_user_list_np[cat] = set()\n","    for user in user_list_np[cat]:\n","        if ah_score_max[cat][user] > 0.5:\n","            ah_user_list_np[cat].add(user)\n","        else:\n","            nonah_user_list_np[cat].add(user)"],"metadata":{"id":"rUsSQ0LdwiKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity check for above block\n","\n","for cat in categories_selected:\n","    l1 = len(ah_user_list_p[cat])\n","    l2 = len(nonah_user_list_p[cat])\n","    l3 = len(ah_user_list_np[cat])\n","    l4 = len(nonah_user_list_np[cat])\n","    l5 = len(ah_user_list_p[cat] | nonah_user_list_p[cat])\n","    l6 = len(ah_user_list_np[cat] | nonah_user_list_np[cat])\n","    assert(l1 + l2 == l5)\n","    assert(l3 + l4 == l6)"],"metadata":{"id":"ik4UI9mCxola"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## User characteristics for topical forums"],"metadata":{"id":"lV9oJpD_BE1w"}},{"cell_type":"code","source":["# User-characteristics for different forums\n","\n","display_stats(nonah_user_list_np['technology'])"],"metadata":{"id":"OqAm_85eyPym"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Migration study"],"metadata":{"id":"AwxWPq49A_ZW"}},{"cell_type":"code","source":["categories_1 = ['politics2']\n","categories_2 = ['law']"],"metadata":{"id":"GboyjrVX0BUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["migration_list = get_migrated_users(user_list_np['politics2'], categories_1, categories_2, categories_1_origin=True, require_migration=True)"],"metadata":{"id":"TZiXC2tyA-H1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(migration_list)"],"metadata":{"id":"wNAUYMGUBjMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AA, AN, NA, NN = partition_migrated_users(migration_list, categories_1, categories_2)"],"metadata":{"id":"r615qdktBl40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('AH-AH: %d, AH-NONAH: %d, NONAH-AH: %d, NONAH-NONAH: %d' % (len(AA), len(AN), len(NA), len(NN)))"],"metadata":{"id":"PGdO04_gB0cz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = AA + AN\n","N = NA + NN"],"metadata":{"id":"_H07c_1sB3Nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(N)"],"metadata":{"id":"yHuEjlXVB6ag"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Top 1000 comments"],"metadata":{"id":"TMxdY4fgIDFX"}},{"cell_type":"code","source":["comments_p_ = list()\n","comments_np_ = list()\n","\n","for comment in comments_p['politics2']:\n","    comments_p_.append((comment['score'], comment['body']))\n","\n","for comment in comments_np['politics2']:\n","    comments_np_.append((comment['score'], comment['body']))"],"metadata":{"id":"mAVVONDTB8wU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments_p_ = sorted(comments_p_)\n","comments_np_ = sorted(comments_np_)"],"metadata":{"id":"pgW2OALPIzXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_ah_p_ = comments_p_[:1000]\n","top_ah_np_ = comments_np_[:1000]"],"metadata":{"id":"SB1uiToUJA2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_ah_p = list()\n","top_ah_np = list()\n","\n","for z in top_ah_p_:\n","    top_ah_p.append(' '.join(clean_text(z[1])))\n","\n","for z in top_ah_np_:\n","    top_ah_np.append(' '.join(clean_text(z[1])))"],"metadata":{"id":"ObKggAQNJVDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization"],"metadata":{"id":"HFWOGiX9J6iF"}},{"cell_type":"code","source":["class Visualizer:\n","    \"\"\"Wrapper for creating heatmaps for documents\"\"\"\n","    def __init__(self):\n","        self._header = r'''\\documentclass[10pt,a4paper]{article}\n","\\usepackage[left=1.00cm, right=1.00cm, top=1.00cm, bottom=2.00cm]{geometry}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}''' + '\\n\\n'\n","\n","        self._footer = r'''\\end{CJK*}\n","\\end{document}'''\n","\n","    def visualize(self,\n","                  word_list,\n","                  attention_list,\n","                  label_list,\n","                  latex_file,\n","                  title,\n","                  batch_size=20,\n","                  color='blue'):\n","        \"\"\"Routine to generate attention heatmaps for given texts\n","        ---------------------------------------------------------\n","        Input:\n","        :param word_list: list of texts (each text is a list of words)\n","        :param attention_list: scores for each word, dimension same as word_list\n","        :param label_list: label for each text\n","        :param latex_file: name of the latex file\n","        :param title: title of latex file\n","        :param batch_size: Number of comments in each batch\n","        :param color: color used for visualization, can be 'blue', 'red', 'green', etc.\n","        \"\"\"\n","        word_list_processed = []\n","        for x in word_list:\n","            word_list_processed.append(self._clean_word(x))\n","\n","        with open(latex_file, 'w', encoding='utf-8') as f:\n","            f.write(self._header)\n","            f.write('\\\\section{%s}\\n\\n' % title)\n","\n","            n_examples = len(word_list)\n","            n_batches = n_examples // batch_size\n","\n","            for i in range(n_batches):\n","                batch_word_list = word_list_processed[i * batch_size: (i + 1) * batch_size]\n","                batch_attention_list = attention_list[i * batch_size: (i + 1) * batch_size]\n","                batch_label_list = label_list[i * batch_size: (i + 1) * batch_size]\n","                f.write('\\\\subsection{Batch %d}\\n\\n' % (i + 1))\n","                for j in range(batch_size):\n","                    f.write('\\\\subsubsection{Comment %d - %s}\\n\\n' % (j + 1, batch_label_list[j]))\n","                    sentence = batch_word_list[j]\n","                    score = batch_attention_list[j]\n","                    assert len(sentence) == len(score)\n","                    f.write('\\\\noindent')\n","                    for k in range(len(sentence)):\n","                        f.write('\\\\colorbox{%s!%s}{' % (color, score[k]) + '\\\\strut ' + sentence[k] + '} ')\n","                    f.write('\\n\\n')\n","\n","            f.write(self._footer)\n","\n","    @staticmethod\n","    def _clean_word(word_list):\n","        new_word_list = []\n","        for word in word_list:\n","            for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\", \"{\", \"}\"]:\n","                if latex_sensitive in word:\n","                    word = word.replace(latex_sensitive, '\\\\' + latex_sensitive)\n","            new_word_list.append(word)\n","        return new_word_list"],"metadata":{"id":"lvQxlz8eJ4Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_version = '/content/gdrive/MyDrive/DL/cnerg-bert-adhominem'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"metadata":{"id":"s2Kf9syxKCnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INTENSITY = 70\n","\n","def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"metadata":{"id":"0LiBYFd_KFaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return clean_apos(W, A)\n","\n","def clean_apos(w, a):\n","    W = []\n","    A = []\n","    ctr = 0\n","    while ctr != len(w):\n","        if w[ctr] == '\\'':\n","            W[-1] += w[ctr] + w[ctr + 1]\n","            A[-1] = min(INTENSITY, A[-1] + a[ctr] + a[ctr + 1])\n","            ctr += 2\n","        else:\n","            W.append(w[ctr])\n","            A.append(a[ctr])\n","            ctr += 1\n","    return W, A"],"metadata":{"id":"U_oF9RjjKhMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def top_three_tokens(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    for w in xx:\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                scores[j] = INTENSITY\n","    return words, scores"],"metadata":{"id":"kfTdKgwWKjkM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["viz = Visualizer()\n","\n","def create_latex_file( do_polar=True):\n","    top_ah_comments = top_ah_p if do_polar else top_ah_np\n","    words_list = list()\n","    scores_list = list()\n","\n","    for comment in top_ah_comments:\n","        try:\n","            words, scores = top_three_tokens(comment)\n","        except:\n","            continue\n","        words_list.append(words)\n","        scores_list.append(scores)\n","    \n","    label = 'For-against' if do_polar else 'Perspective'\n","    labels_list = [label for _ in range(len(words_list))]\n","    \n","    viz.visualize(words_list, scores_list, labels_list,\n","                  latex_file='sample.tex',\n","                  title=f'Top ad hominem comments from {label} debates in Politics forum',\n","                  batch_size=len(words_list),\n","                  color='cyan')"],"metadata":{"id":"w6qQdVGfKmcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_latex_file(do_polar=False)"],"metadata":{"id":"oTemNYhfLNK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P7Jh8CjXLRXc"},"execution_count":null,"outputs":[]}]}