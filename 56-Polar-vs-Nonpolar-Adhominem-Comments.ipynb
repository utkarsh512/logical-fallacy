{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM+51cb9/bvQePPq9JrtRBW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"SfCIGoYLznEG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoVP8NzrzNVL"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"r3hkLIy6za96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"uvdVA29oBrv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","# import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","from   transformers             import BertModel, BertTokenizer\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"Wt4sHJlDzrdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom routine to clean texts scraped from Web.\n","# It removes hyperlinks, punctuation marks (except apostrophe)\n","\n","tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"zAShcRYiz6mH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"JRJJUBxm0BxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"YZb5EUY90Nnq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"e8x-eQzhiVAr"}},{"cell_type":"code","source":["# Partition the dataset into polar and non-polar comments\n","\n","polar_cids = set()\n","\n","comments_p = dict()\n","comments_np = dict()\n","\n","for x in categories_selected:\n","    comments_p[x] = list()\n","    comments_np[x] = list()\n","\n","    for comment in comments[x]:\n","        if comment['polarity'] == 'Not Available':\n","            comments_np[x].append(deepcopy(comment))\n","        else:\n","            comments_p[x].append(deepcopy(comment))"],"metadata":{"id":"gLhtQtea0U0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x in categories_selected:\n","    print(f'{x} - {len(comments_p[x])} - {len(comments_np[x])}')"],"metadata":{"id":"37qH7DNi1lvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# User characterisitics\n","\n","user_list_p = dict()\n","user_list_np = dict()\n","\n","for cat in categories_selected:\n","    user_list_p[cat] = set()\n","    for comment in comments_p[cat]:\n","        user_list_p[cat].add(comment['author'])\n","\n","    user_list_np[cat] = set()\n","    for comment in comments_np[cat]:\n","        user_list_np[cat].add(comment['author'])\n"],"metadata":{"id":"nv6yBcxN1yhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in categories_selected:\n","    print(f'{cat} - {len(user_list_p[cat])} - {len(user_list_np[cat])} - {len(user_list_p[cat] & user_list_np[cat])}')"],"metadata":{"id":"5ZeiLtym9tIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## User profile characteristics"],"metadata":{"id":"YhpEjN5zipM8"}},{"cell_type":"code","source":["# Loading CreateDebate profile characteristics into dataframe\n","df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)\n","\n","# Extract useful characteristics\n","reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"qqeoGevQ9-hc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def profile_characteristics_stats(user_subset):\n","    \"\"\"\n","    Returns average and standard deviation of profile characteristics for \n","    given subset of users.\n","\n","    :param user_subset: Iterable containing usernames\n","\n","    >>> avgs, stds = profile_characterisitics_stat(user_subset)\n","    >>> rewards_avg, efficiency_avg, n_allies_avg, n_enemies_avg, n_hostiles_avg = avgs\n","    >>> rewards_std, efficiency_std, n_allies_std, n_enemies_std, n_hostiles_std = stds\n","\n","    Note that profile characteristics for some users might not be present in our\n","    dataset as some users might have deleted their account when we scraped the\n","    forum to obtain these characteristics.\n","    \"\"\"\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        try:\n","            rewards_.append(reward_points_map[user])\n","        except:pass\n","        try:\n","            efficiency_.append(efficiency_map[user])\n","        except:pass\n","        try:\n","            n_allies.append(allies_map[user])\n","        except:pass\n","        try:\n","            n_enemies.append(enemies_map[user])\n","        except:pass\n","        try:\n","            n_hostiles.append(hostiles_map[user])\n","        except:pass\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"tJtiCjh7_-qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_stats(user_subset):\n","    user_chr_avg, user_chr_std = profile_characteristics_stats(user_subset) \n","    print('Reward points: %.2f ± %.2f' % (user_chr_avg[0], user_chr_std[0]))\n","    print('Efficiency   : %.2f ± %.2f' % (user_chr_avg[1], user_chr_std[1]))\n","    print('# Allies     : %.2f ± %.2f' % (user_chr_avg[2], user_chr_std[2]))\n","    print('# Enemies    : %.2f ± %.2f' % (user_chr_avg[3], user_chr_std[3]))\n","    print('# Hostiles   : %.2f ± %.2f' % (user_chr_avg[4], user_chr_std[4]))"],"metadata":{"id":"n35XAl4dADaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in categories_selected: \n","    print('\\n\\n-----------------------------\\n\\n')\n","    print(f'Category: {cat}\\n\\n')\n","    print('Users writing polar comments:\\n')\n","    display_stats(user_list_p[cat])\n","    print('\\n\\nUsers writing non-polar comments:\\n')\n","    display_stats(user_list_np[cat])"],"metadata":{"id":"CXmPwc0TAT-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Slur words analysis"],"metadata":{"id":"a9o0ltAuijTc"}},{"cell_type":"code","source":["polar_comments_full_text = dict()\n","nonpolar_comments_full_text = dict()\n","\n","for cat in tqdm(categories_selected):\n","    polar_comments_full_text[cat] = list()\n","    nonpolar_comments_full_text[cat] = list()\n","\n","    for comment in comments_p[cat]:\n","        polar_comments_full_text[cat].append(' '.join(clean_text(comment['body'].strip())))\n","    \n","    for comment in comments_np[cat]:\n","        nonpolar_comments_full_text[cat].append(' '.join(clean_text(comment['body'].strip())))\n","\n","    polar_comments_full_text[cat] = ' '.join(polar_comments_full_text[cat])\n","    nonpolar_comments_full_text[cat] = ' '.join(nonpolar_comments_full_text[cat])"],"metadata":{"id":"iBgUPBSzA3a_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SLUR_WORDS = {\n","  \"jews\": [\n","    \"jews\",\n","    \"oven dodger\",\n","    \"nazi\",\n","    \"dirty jew\",\n","    \"holocaust\",\n","    \"kikesucker\",\n","    \"hook nose\",\n","    \"kike\"\n","  ],\n","  \"homosexual\": [\n","    \"faggots usually\",\n","    \"fucking queer\",\n","    \"the biggest faggot\",\n","    \"dyke\",\n","    \"you fucking faggot\",\n","    \"hate faggots\",\n","    \"queer\",\n","    \"homosexual\",\n","    \"the faggots\",\n","    \"faggot\",\n","    \"faggots usually have\",\n","    \"gay\",\n","    \"faggots\",\n","    \"dykey\",\n","    \"ugly dyke\",\n","    \"faggots like you\",\n","    \"you a fag\",\n","    \"lesbian\",\n","    \"homo\",\n","    \"is a faggot\",\n","    \"like a faggot\",\n","    \"dykes\",\n","    \"faggots like\",\n","    \"faggot if you ever\"\n","  ],\n","  \"women\": [\n","    \"ugly dyke\",\n","    \"woman terrorist\",\n","    \"nigress\",\n","    \"bitch\",\n","    \"slut\",\n","    \"women\",\n","    \"sheeboon\",\n","    \"negress\",\n","    \"mud shark\",\n","    \"women threat\",\n","    \"you a lame bitch\",\n","    \"your a cunt\",\n","    \"white bitch\",\n","    \"niggeress\",\n","    \"hoe\",\n","    \"dykes\",\n","    \"niggress\",\n","    \"sheboon\",\n","    \"feminazi\"\n","  ],\n","  \"blacks\": [\n","    \"pavement ape\",\n","    \"the niggers\",\n","    \"negress\",\n","    \"porch monkey\",\n","    \"that nigger\",\n","    \"this nigger\",\n","    \"sheboon\",\n","    \"all niggers\",\n","    \"eurafrica\",\n","    \"shut up nigger\",\n","    \"picaninny\",\n","    \"african attack\",\n","    \"spearchucker\",\n","    \"how many niggers\",\n","    \"nigger\",\n","    \"africa\",\n","    \"niggers are in my\",\n","    \"dindu nuffin\",\n","    \"stupid nigger\",\n","    \"moolie\",\n","    \"niggers\",\n","    \"bluegum\",\n","    \"nigger ass\",\n","    \"you niggers\",\n","    \"fucking nigger\",\n","    \"nigger music\",\n","    \"niggress\",\n","    \"you a nigger\",\n","    \"many niggers are\",\n","    \"nigress\",\n","    \"blacks\",\n","    \"teenaper\",\n","    \"sheeboon\",\n","    \"dumb nigger\",\n","    \"niggeress\",\n","    \"pickaninny\",\n","    \"nigga\"\n","  ],\n","  \"muslim\": [\n","    \"muslim immigrant\",\n","    \"islam\",\n","    \"mudslime\",\n","    \"mooslem\",\n","    \"muslim refugee\",\n","    \"musslime\",\n","    \"shitlam\",\n","    \"muslim invasion\",\n","    \"moslime\",\n","    \"mooslamic\",\n","    \"muzzie\",\n","    \"allah akbar\",\n","    \"mooslime\",\n","    \"musloid\",\n","    \"mudslimes\",\n","    \"muslim\",\n","    \"muslimes\",\n","    \"moslum\",\n","    \"mussie\",\n","    \"muzrat\",\n","    \"muslim countries\",\n","    \"muzzy\",\n","    \"moslim\",\n","    \"jihadi\",\n","    \"muslim country\",\n","    \"moslem\",\n","    \"muzzrat\",\n","    \"mooslim\"\n","  ],\n","  \"arabs\": [\n","    \"towel head\",\n","    \"goatfucker\",\n","    \"arabs\",\n","    \"goathumper\",\n","    \"raghead\",\n","    \"rag head\",\n","    \"goathumping\",\n","    \"towelhead\",\n","    \"camel jockey\",\n","    \"sandnigger\",\n","    \"camel fucker\",\n","    \"sand nigger\"\n","  ],\n","  \"generic\": [\n","    \"to rape\",\n","    \"raped and\",\n","    \"shithole country\",\n","    \"get raped\",\n","    \"raped\",\n","    \"is a fucking\",\n","    \"shit skin\",\n","    \"raped by\",\n","    \"hate you\",\n","    \"fake empowerment\",\n","    \"abusive women\",\n","    \"fuck you too\",\n","    \"violence\",\n","    \"wit a lame nigga\",\n","    \"they all look\",\n","    \"alllivesmatter\",\n","    \"shithole countries\",\n","    \"fucking hate\",\n","    \"trailer trash\",\n","    \"kill all\",\n","    \"terrorist threat\",\n","    \"harassment\",\n","    \"kill yourself\",\n","    \"shitskin\",\n","    \"okay to be white\",\n","    \"fucking hate you\"\n","  ],\n","  \"white\": [\n","    \"full of white\",\n","    \"white trash\",\n","    \"white devil\",\n","    \"white\",\n","    \"are all white\",\n","    \"white boy\",\n","    \"white ass\",\n","    \"white bitch\",\n","    \"hillbilly\",\n","    \"whigger\",\n","    \"white christian\",\n","    \"white person\",\n","    \"all white\",\n","    \"white nigger\",\n","    \"redneck\",\n","    \"white honky\",\n","    \"wigger\",\n","    \"them white\"\n","  ],\n","  \"economy\": [\n","    \"ghetto\"\n","  ],\n","  \"immigrant\": [\n","    \"illegal immigrants\",\n","    \"immigrant not welcome\",\n","    \"immigrant terror\",\n","    \"mexcrement\",\n","    \"go back to where you come from\",\n","    \"muslim refugee\",\n","    \"illegal aliens\",\n","    \"refugee\",\n","    \"protect from immigrants\",\n","    \"negro\",\n","    \"refugees\",\n","    \"immigrant\",\n","    \"refugee invasion\",\n","    \"go back to where they come from\",\n","    \"refugees impact\",\n","    \"bring ebola\",\n","    \"immigrants\",\n","    \"illegal alien\",\n","    \"immigrant invasion\",\n","    \"bring disease\"\n","  ],\n","  \"mental\": [\n","    \"retard\",\n","    \"mongoloid\",\n","    \"retarded\"\n","  ],\n","  \"asians\": [\n","    \"asians\",\n","    \"ching chong\",\n","    \"chinaman\"\n","  ]\n","}"],"metadata":{"id":"Q3MA7MpWTR9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polar_slur_count = dict()\n","nonpolar_slur_count = dict()\n","\n","for cat in categories_selected:\n","    polar_slur_count[cat] = dict()\n","    nonpolar_slur_count[cat] = dict()\n","\n","    polar_token_count = len(polar_comments_full_text[cat].split())\n","    nonpolar_token_count = len(nonpolar_comments_full_text[cat].split())\n","\n","    for slur_group in SLUR_WORDS.keys():\n","        polar_slur_count[cat][slur_group] = 0\n","        nonpolar_slur_count[cat][slur_group] = 0\n","\n","        for slur_words in SLUR_WORDS[slur_group]:\n","            polar_slur_count[cat][slur_group] += polar_comments_full_text[cat].count(slur_words) * len(slur_words.split())\n","            nonpolar_slur_count[cat][slur_group] += nonpolar_comments_full_text[cat].count(slur_words) * len(slur_words.split())\n","        \n","        polar_slur_count[cat][slur_group] /= polar_token_count\n","        nonpolar_slur_count[cat][slur_group] /= nonpolar_token_count"],"metadata":{"id":"9jfmNzbuUEbJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_slur_words(cat, saveas):\n","    x = list(SLUR_WORDS.keys())\n","    y1 = [] # Polar\n","    y2 = [] # Non-polar\n","\n","    for slur_group in x:\n","        y1.append(polar_slur_count[cat][slur_group])\n","        y2.append(nonpolar_slur_count[cat][slur_group])\n","\n","    ticks = np.arange(len(x))\n","    width = 0.30\n","\n","    fig, ax = plt.subplots()\n","    fig.set_size_inches(15, 12)\n","    subplot1 = ax.bar(ticks - width / 2, y1, width, label='Polar', tick_label=x)\n","    subplot2 = ax.bar(ticks + width / 2, y2, width, label='Non-polar', tick_label=x)\n","\n","    ax.set_ylabel('Probability')\n","    ax.set_title(f'Probability of being a slur word')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.savefig(saveas)"],"metadata":{"id":"YoY5jjBtYaG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('politics2', '/content/gdrive/MyDrive/Temp/MTPImages/politics_slur_prob_1.pdf')"],"metadata":{"id":"jOVT0hTwZ3nd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('religion', '/content/gdrive/MyDrive/Temp/MTPImages/religion_slur_prob_1.pdf')"],"metadata":{"id":"jnt5NcoiZ6cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('world')"],"metadata":{"id":"Nb7tFOVMae_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('science')"],"metadata":{"id":"pnNCtSxuaqTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('law')"],"metadata":{"id":"un1YMxKOcCz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_slur_words('technology')"],"metadata":{"id":"L4HGID-ccWV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Triggers"],"metadata":{"id":"rD7LwwQPix5T"}},{"cell_type":"code","source":["comments_p['politics2'][0]"],"metadata":{"id":"6GaUs-iAciVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Get top 100 ad hominem comments from polar and non-polar content\n","# and visualize them using attention scores\n","\n","comment_score_p = dict()\n","comment_score_np = dict()\n","\n","for cat in categories_selected:\n","    comment_score_p[cat] = list()\n","    comment_score_np[cat] = list()\n","\n","    for comment in comments_p[cat]:\n","        comment_score_p[cat].append((comment['id'], 1 - comment['score']))\n","    \n","    for comment in comments_np[cat]: \n","        comment_score_np[cat].append((comment['id'], 1 - comment['score']))\n","    \n","    comment_score_p[cat] = sorted(comment_score_p[cat], key=lambda x: x[1], reverse=True)\n","    comment_score_np[cat] = sorted(comment_score_np[cat], key=lambda x: x[1], reverse=True)"],"metadata":{"id":"pk1_aGYojdW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_ah_comments_p = dict()\n","top_ah_comments_np = dict()\n","\n","for cat in categories_selected:\n","    top_ah_comments_p[cat] = list()\n","    top_ah_comments_np[cat] = list()\n","\n","    top_cid_p = list()\n","    top_cid_np = list()\n","\n","    for i in range(100):\n","        top_cid_p.append(comment_score_p[cat][i][0])\n","        top_cid_np.append(comment_score_np[cat][i][0])\n","    \n","    top_cid_p = set(top_cid_p)\n","    top_cid_np = set(top_cid_np)\n","\n","    for comment in comments_p[cat]: \n","        if comment['id'] in top_cid_p:\n","            top_ah_comments_p[cat].append(' '.join(clean_text(comment['body'])))\n","    \n","    for comment in comments_np[cat]: \n","        if comment['id'] in top_cid_np: \n","            top_ah_comments_np[cat].append(' '.join(clean_text(comment['body'])))\n","\n","    print(len(top_ah_comments_p[cat]), len(top_ah_comments_np[cat]))"],"metadata":{"id":"NgnKEBbv-lkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Visualizer:\n","    \"\"\"Wrapper for creating heatmaps for documents\"\"\"\n","    def __init__(self):\n","        self._header = r'''\\documentclass[10pt,a4paper]{article}\n","\\usepackage[left=1.00cm, right=1.00cm, top=1.00cm, bottom=2.00cm]{geometry}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}''' + '\\n\\n'\n","\n","        self._footer = r'''\\end{CJK*}\n","\\end{document}'''\n","\n","    def visualize(self,\n","                  word_list,\n","                  attention_list,\n","                  label_list,\n","                  latex_file,\n","                  title,\n","                  batch_size=20,\n","                  color='blue'):\n","        \"\"\"Routine to generate attention heatmaps for given texts\n","        ---------------------------------------------------------\n","        Input:\n","        :param word_list: list of texts (each text is a list of words)\n","        :param attention_list: scores for each word, dimension same as word_list\n","        :param label_list: label for each text\n","        :param latex_file: name of the latex file\n","        :param title: title of latex file\n","        :param batch_size: Number of comments in each batch\n","        :param color: color used for visualization, can be 'blue', 'red', 'green', etc.\n","        \"\"\"\n","        word_list_processed = []\n","        for x in word_list:\n","            word_list_processed.append(self._clean_word(x))\n","\n","        with open(latex_file, 'w', encoding='utf-8') as f:\n","            f.write(self._header)\n","            f.write('\\\\section{%s}\\n\\n' % title)\n","\n","            n_examples = len(word_list)\n","            n_batches = n_examples // batch_size\n","\n","            for i in range(n_batches):\n","                batch_word_list = word_list_processed[i * batch_size: (i + 1) * batch_size]\n","                batch_attention_list = attention_list[i * batch_size: (i + 1) * batch_size]\n","                batch_label_list = label_list[i * batch_size: (i + 1) * batch_size]\n","                f.write('\\\\subsection{Batch %d}\\n\\n' % (i + 1))\n","                for j in range(batch_size):\n","                    f.write('\\\\subsubsection{Comment %d - %s}\\n\\n' % (j + 1, batch_label_list[j]))\n","                    sentence = batch_word_list[j]\n","                    score = batch_attention_list[j]\n","                    assert len(sentence) == len(score)\n","                    f.write('\\\\noindent')\n","                    for k in range(len(sentence)):\n","                        f.write('\\\\colorbox{%s!%s}{' % (color, score[k]) + '\\\\strut ' + sentence[k] + '} ')\n","                    f.write('\\n\\n')\n","\n","            f.write(self._footer)\n","\n","    @staticmethod\n","    def _clean_word(word_list):\n","        new_word_list = []\n","        for word in word_list:\n","            for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\", \"{\", \"}\"]:\n","                if latex_sensitive in word:\n","                    word = word.replace(latex_sensitive, '\\\\' + latex_sensitive)\n","            new_word_list.append(word)\n","        return new_word_list"],"metadata":{"id":"ZplyFIE1BwP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_version = '/content/gdrive/MyDrive/DL/cnerg-bert-adhominem'\n","do_lower_case = True\n","model = BertModel.from_pretrained(model_version, output_attentions=True)\n","tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"],"metadata":{"id":"2ovjFkx2CwvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INTENSITY = 70\n","\n","def attention_scores(text, layers=None, heads=None):\n","    sentence_a = text\n","    inputs = tokenizer.encode_plus(sentence_a, None, return_tensors='pt', add_special_tokens=True)\n","    input_ids = inputs['input_ids']\n","    attention = model(input_ids)[-1]\n","    input_id_list = input_ids[0].tolist() # Batch index 0\n","    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n","    sz = len(tokens)\n","    matrix = [0 for j in range(sz)]\n","    if layers is None:\n","        layers = [x for x in range(12)]\n","    if heads is None:\n","        heads = [x for x in range(12)]\n","    for layer in layers:\n","        for head in heads:\n","            for j in range(sz):\n","                matrix[j] += attention[layer][0, head, 0, j].item()\n","    for j in range(sz):\n","        matrix[j] = (matrix[j]) / (len(layers) * len(heads))\n","    return (tokens, matrix)"],"metadata":{"id":"khoU4ucdDG99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_array(w, a):\n","    W = []\n","    A = []\n","    for i in range(len(w)):\n","        if (w[i].startswith('##')):\n","            W[len(W) - 1] += w[i][2:]\n","            A[len(A) - 1] = (A[len(A) - 1] + a[i]) / 2\n","        else:\n","            W.append(w[i])\n","            A.append(a[i])\n","    return clean_apos(W, A)\n","\n","def clean_apos(w, a):\n","    W = []\n","    A = []\n","    ctr = 0\n","    while ctr != len(w):\n","        if w[ctr] == '\\'':\n","            W[-1] += w[ctr] + w[ctr + 1]\n","            A[-1] = min(INTENSITY, A[-1] + a[ctr] + a[ctr + 1])\n","            ctr += 2\n","        else:\n","            W.append(w[ctr])\n","            A.append(a[ctr])\n","            ctr += 1\n","    return W, A"],"metadata":{"id":"mghB2g1UD23x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def top_three_tokens(text):\n","    words, attentions = attention_scores(text)\n","    words = words[1:-1] # Remove start and end tags\n","    attentions = attentions[1:-1]\n","    assert len(words) == len(attentions)\n","    words, attentions = clean_array(words, attentions)\n","    assert len(words) == len(attentions)\n","    top_tokens = list()\n","    for i in range(len(words)):\n","        top_tokens.append((attentions[i], i))\n","    top_tokens = sorted(top_tokens, reverse=True)\n","    ind = [0]\n","    cur = 1\n","    while len(ind) < 3:\n","        take = True\n","        for ids in ind:\n","            take = take and abs(top_tokens[ids][1] - top_tokens[cur][1]) > 2\n","        if take:\n","            ind.append(cur)\n","        cur += 1\n","    xx = []\n","    for x in ind:\n","        xx.append(top_tokens[x][1])\n","    scores = [0 for i in range(len(words))]\n","    for w in xx:\n","        lst = [w - 1, w, w + 1]\n","        for j in lst:\n","            if j >= 0 and j < len(words):\n","                scores[j] = INTENSITY\n","    return words, scores"],"metadata":{"id":"fxED-Q5_D5lB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["viz = Visualizer()\n","\n","def create_latex_file(cat, do_polar=True, filename='sample.tex'):\n","    top_ah_comments = top_ah_comments_p[cat] if do_polar else top_ah_comments_np[cat]\n","    words_list = list()\n","    scores_list = list()\n","\n","    for comment in top_ah_comments:\n","        try:\n","            words, scores = top_three_tokens(comment)\n","        except:\n","            continue\n","        words_list.append(words)\n","        scores_list.append(scores)\n","    \n","    label = 'Polar' if do_polar else 'Non-polar'\n","    labels_list = [label for _ in range(len(words_list))]\n","    \n","    viz.visualize(words_list, scores_list, labels_list,\n","                  latex_file=filename,\n","                  title=f'{label} comments in {cat} forum',\n","                  batch_size=len(words_list),\n","                  color='cyan')"],"metadata":{"id":"_W87a-60EA0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["category_ = 'world'"],"metadata":{"id":"yBWIHEKVGesV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_latex_file(category_, True)"],"metadata":{"id":"LomewtJtGog7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_latex_file(category_, False)"],"metadata":{"id":"p6mvZAHwIfvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_0VnEY6JMQgR"},"execution_count":null,"outputs":[]}]}