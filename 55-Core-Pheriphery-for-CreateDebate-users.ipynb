{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOtosuRA4ku+K/KtEZAMz4v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"0niAfY8BdPEr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUE5YxWxdEaU"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested/"],"metadata":{"id":"2f9M1hdldZdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This module is required for core-periphery analysis\n","!pip install cpnet\n","!pip install transformers"],"metadata":{"id":"DO9F1a3Zdlhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   copy                     import deepcopy\n","import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","import matplotlib\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))"],"metadata":{"id":"R7m9V0E2dptt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom routine to clean texts scraped from Web.\n","# It removes hyperlinks, punctuation marks (except apostrophe)\n","\n","tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"JOLekKQtd2_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"pTcbfWJoeFNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"uIPHNU2ed9FB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_score_comments = dict()\n","\n","for cat in categories_selected:\n","    for comment in comments[cat]:\n","        ah_score_comments[comment['id']] = 1 - comment['score']"],"metadata":{"id":"KqyJv_VUrsdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_tstring(tstring):\n","    \"\"\"\n","    Parses comment's time to an integer to enable\n","    comparison between comments based on their time of posting\n","    \"\"\"\n","    if tstring == 'Not Available':\n","        raise ValueError('Invalid posting time for parse_tstring')\n","    tstring = tstring.replace('T', '-').replace(':', '-').replace('+', '-').split('-')\n","    return int(''.join(tstring[:-2]))"],"metadata":{"id":"ll0hAjOVeaP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading AH score\n","\n","with open('/content/gdrive/MyDrive/Temp/47-ah-score.pkl', 'rb') as fp:\n","    ah_score = pickle.load(fp)\n","\n","# `ah_score` is a dictionary that contains the ah score of the comments written\n","# by all the users\n","\n","# key: category -> user\n","# value: list of ah_score for given user for given category\n","\n","# value > 0.5 --> ad hominem\n","# value < 0.5 --> non ad hominem"],"metadata":{"id":"-HISEV2Wei4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading CreateDebate profile characteristics into dataframe\n","df = pd.read_json('/content/gdrive/MyDrive/DL/CreateDebate/profile/results.json', lines=True)\n","\n","# Extract useful characteristics\n","reward_points_map = {k : v for k, v in zip(df['username'].tolist(), df['reward_points'].tolist())}\n","efficiency_map    = {k : v for k, v in zip(df['username'].tolist(), df['efficiency'].tolist())}\n","allies_map        = {k : len(v) for k, v in zip(df['username'].tolist(), df['allies'].tolist())}\n","enemies_map       = {k : len(v) for k, v in zip(df['username'].tolist(), df['enemies'].tolist())}\n","hostiles_map      = {k : len(v) for k, v in zip(df['username'].tolist(), df['hostiles'].tolist())}"],"metadata":{"id":"HW5s5m_WenZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def profile_characteristics_stats(user_subset):\n","    \"\"\"\n","    Returns average and standard deviation of profile characteristics for \n","    given subset of users.\n","\n","    :param user_subset: Iterable containing usernames\n","\n","    >>> avgs, stds = profile_characterisitics_stat(user_subset)\n","    >>> rewards_avg, efficiency_avg, n_allies_avg, n_enemies_avg, n_hostiles_avg = avgs\n","    >>> rewards_std, efficiency_std, n_allies_std, n_enemies_std, n_hostiles_std = stds\n","\n","    Note that profile characteristics for some users might not be present in our\n","    dataset as some users might have deleted their account when we scraped the\n","    forum to obtain these characteristics.\n","    \"\"\"\n","    rewards_ = list()\n","    efficiency_ = list()\n","    n_allies = list()\n","    n_enemies = list()\n","    n_hostiles = list()\n","\n","    for user in user_subset:\n","        try:\n","            rewards_.append(reward_points_map[user])\n","        except:pass\n","        try:\n","            efficiency_.append(efficiency_map[user])\n","        except:pass\n","        try:\n","            n_allies.append(allies_map[user])\n","        except:pass\n","        try:\n","            n_enemies.append(enemies_map[user])\n","        except:pass\n","        try:\n","            n_hostiles.append(hostiles_map[user])\n","        except:pass\n","    \n","    grpd_data = [rewards_, efficiency_, n_allies, n_enemies, n_hostiles]\n","    avgs = [np.average(x) for x in grpd_data]\n","    stds = [np.std(x) for x in grpd_data]\n","    \n","    return avgs, stds"],"metadata":{"id":"muupFl_rerng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Maximum ah score per category per author\n","#   key: category -> author\n","#   value: maximum ah score\n","\n","ah_score_max = dict()\n","\n","for category, author_data in ah_score.items():\n","    ah_score_max[category] = dict()\n","    for author, ah_scores in author_data.items():\n","        ah_score_max[category][author] = np.max(ah_scores)"],"metadata":{"id":"tm454lTeewPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comment_count = dict()\n","# key: category -> author\n","# value: number of comments written by author in the given forum\n","\n","for category in categories_selected:\n","    comment_count[category] = dict()\n","\n","    for comment in comments[category]:\n","        author = comment['author']\n","        try:\n","            comment_count[category][author] += 1\n","        except KeyError:\n","            comment_count[category][author] = 1"],"metadata":{"id":"kKDzt2czeyfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_list = set()\n","\n","for category in categories_selected:\n","    for comment in comments[category]:\n","        user_list.add(comment['author'])\n","\n","user_list = list(user_list)"],"metadata":{"id":"7Ticrmrie19V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_post_time = dict()\n","# key: category -> user\n","# value: post time of the first comment by given user in the given category\n","#        It is an integer as returned by parse_tstring routine\n","\n","for category in categories_selected:\n","    first_post_time[category] = dict()\n","\n","    for comment in comments[category]: \n","        if comment['time'] == 'Not Available':\n","            continue\n","        author = comment['author']\n","        try:\n","            first_post_time[category][author] = min(first_post_time[category][author], parse_tstring(comment['time']))\n","        except KeyError:\n","            first_post_time[category][author] = parse_tstring(comment['time'])"],"metadata":{"id":"6Kn-cb5Oe4Wo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_migrated_users(categories_1, categories_2, categories_1_origin=True, require_migration=True):\n","    \"\"\"\n","    Returns a list of usernames who migrated from categories_1 to categories_2\n","\n","    If categories_1_origin is True, we will consider all other major categories\n","    to compute post_time_2, so as to ensure that first post by the user is in \n","    categories_1\n","\n","    If require_migration is True, post_time_1 < post_time_2 condition is relaxed\n","    \"\"\"\n","\n","    resultant_list = list()\n","\n","    for user in user_list:\n","        post_time_1 = 20220101000000\n","        post_time_2 = 20220101000000\n","\n","        if not isinstance(categories_1, set):\n","            categories_1 = set(categories_1)\n","        if not isinstance(categories_2, set): \n","            categories_2 = set(categories_2)\n","        \n","        for category in categories_1:\n","            try:\n","                cur_post_time = first_post_time[category][user]\n","                post_time_1 = min(post_time_1, cur_post_time)\n","            except KeyError:\n","                pass\n","        \n","        for category in categories_2:\n","            try:\n","                cur_post_time = first_post_time[category][user]\n","                post_time_2 = min(post_time_2, cur_post_time) \n","            except KeyError:\n","                pass\n","\n","        if post_time_1 == 20220101000000 or post_time_2 == 20220101000000:\n","            continue\n","\n","        if categories_1_origin:\n","            for category in categories_selected:\n","                if not ((category in categories_1) or (category in categories_2)):\n","                    try:\n","                        cur_post_time = first_post_time[category][user]\n","                        post_time_2 = min(post_time_2, cur_post_time)\n","                    except KeyError:\n","                        pass\n","\n","        if post_time_1 < post_time_2 or not require_migration:\n","            resultant_list.append(user)\n","        \n","    return resultant_list"],"metadata":{"id":"3MeYgnLXe79W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition_migrated_users(migration_list, categories_1, categories_2):\n","    \"\"\"\n","    Partitions the users into 4 categories: \n","        AH-AH\n","        AH-NonAH\n","        NonAH-AH\n","        NonAH-NonAH\n","\n","    Users are classified as AH in a given category if they post at least one \n","    ad hominem comment in that category\n","    \n","    Note: migration_list should be obtained using get_migrated_users method\n","    \"\"\"\n","\n","    ah_ah_list = []\n","    ah_nonah_list = []\n","    nonah_ah_list = []\n","    nonah_nonah_list = []\n","\n","    for user in migration_list:\n","        max_score_1 = 0\n","        max_score_2 = 0\n","        for category in categories_1:\n","            max_score_1 = max(max_score_1, ah_score_max[category].get(user, 0))\n","        for category in categories_2:\n","            max_score_2 = max(max_score_2, ah_score_max[category].get(user, 0))\n","\n","        if max_score_1 > 0.5 and max_score_2 > 0.5:\n","            ah_ah_list.append(user)\n","\n","        elif max_score_1 > 0.5 and max_score_2 < 0.5:\n","            ah_nonah_list.append(user)\n","        \n","        elif max_score_1 < 0.5 and max_score_2 > 0.5:\n","            nonah_ah_list.append(user)\n","\n","        elif max_score_1 < 0.5 and max_score_2 < 0.5:\n","            nonah_nonah_list.append(user)\n","        \n","        else:\n","            print(user)\n","\n","    return ah_ah_list, ah_nonah_list, nonah_ah_list, nonah_nonah_list "],"metadata":{"id":"5C5xcw-FfFXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a list of all comment thread representative to build support and dispute \n","# networks\n","\n","threads = []\n","\n","for category in categories_selected:\n","    reader_addr = f'/content/gdrive/MyDrive/DL/CreateDebate/{category}/threads.log'\n","    reader = open(reader_addr, 'rb')\n","    try:\n","        while True:\n","            e = pickle.load(reader)\n","            threads.append(e)\n","    except:\n","        reader.close()"],"metadata":{"id":"nF1NcxN2fJ03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_graph(user_subset, n1 = 0, n2 = 0):\n","    \"\"\"\n","    Builds support graph and dispute graph from hyper-parameters n1 and n2\n","    \n","    Inputs\n","    ------\n","    :param n1: threshold on number of level-1 comments\n","    :param n2: threshold on number of direct replies\n","\n","    Output\n","    ------\n","    (\n","        author_map: dict,\n","        reverse_map: list,\n","        author_count: int, \n","        support_graph: nx.DiGraph,\n","        support_matrix: list, \n","        dispute_graph: nx.DiGraph, \n","        dispute_matrix: list\n","    )\n","    \"\"\"\n","\n","    # Uses globally defined `threads` variable to construct this dictionary.\n","    # You may choose which categories to be included while building `threads`\n","\n","    # key  : author name\n","    # value: count of level-1 comments\n","    athr = dict()\n","\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            for key in e.metaL['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","        if 'root' in e.metaR.keys():\n","            for key in e.metaR['root'].keys():\n","                cmnt = e.comments[key]\n","                cur_athr = cmnt.author\n","                try:\n","                    athr[cur_athr] += 1\n","                except:\n","                    athr[cur_athr] = 1\n","    \n","    # Filter those authors who satisfy the contraint on number of level-1 comments\n","    L1_athr = dict()\n","    for x in athr:\n","        if athr[x] >= n1:\n","            L1_athr[x] = True\n","\n","    # Now use `athr` for storing count of direct replies\n","    # key  : author name\n","    # value: count of direct replies received\n","    athr = dict()\n","\n","    # Depth-first search utility to get number of direct replies for each author\n","    def dfs(Map, cmntMap, athr, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs(Map[cid], cmntMap, athr, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        try:\n","            athr[cur_author] += len(Map[cid].keys())\n","        except:\n","            athr[cur_author] = len(Map[cid].keys())\n","\n","        for key in Map[cid].keys():\n","            dfs(Map[cid], cmntMap, athr, key)\n","\n","    # Traverse thread-tree to get number of direct replies for each author\n","    for e in threads:\n","        if 'root' in e.metaL.keys():\n","            dfs(e.metaL, e.comments, athr)\n","        if 'root' in e.metaR.keys():\n","            dfs(e.metaR, e.comments, athr) \n","    \n","    # Filter authors who now satify both the contrainsts on count of \n","    # - level-1 comments\n","    # - direct replies\n","    A = []\n","    for x in athr:\n","        if x not in user_subset:\n","            continue\n","        if athr[x] >= n2:\n","            try:\n","                z = L1_athr[x]\n","                A.append(x)\n","            except KeyError:\n","                pass\n","\n","    # key  : author name\n","    # value: corresponing node number in the support/dispute network\n","    author_map = dict()\n","\n","    # To get author name for node number\n","    reverse_map = [\"\" for _ in range(len(A))]\n","    author_count = len(A)\n","\n","    for i in range(author_count):\n","        author_map[A[i]] = i\n","        reverse_map[i] = A[i]\n","    \n","    # Weighted adjacency matrices for support and dispute network\n","    # Weight for directed edge b/w Node A and Node B corresponsds to the number\n","    # of times Node A supported/disputed Node B.\n","    support_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","    dispute_matrix = [[0 for j in range(author_count)] for i in range(author_count)]\n","\n","    # Depth-first search utility to build the adjacency matrices for support \n","    # and dispute graph. \n","    # Here, we use polarity of the comment to identify whether the authors are\n","    # showing support or dispute.\n","    def dfs1(Map, cmntMap, cid='root'):\n","        if cid == 'root':\n","            for key in Map[cid].keys():\n","                dfs1(Map[cid], cmntMap, key)\n","            return\n","\n","        cur_author = cmntMap[cid].author\n","        cur_pol = cmntMap[cid].polarity\n","        \n","        if cur_author in author_map and cur_pol != 'Not Available':\n","            cur_author_id = author_map[cur_author]\n","            for key in Map[cid].keys():\n","                nxt_author = cmntMap[key].author\n","                nxt_pol = cmntMap[key].polarity\n","                if nxt_author in author_map and nxt_pol != 'Not Available':\n","                    nxt_author_id = author_map[nxt_author]\n","                    if cur_pol == nxt_pol:\n","                        support_matrix[nxt_author_id][cur_author_id] += 1\n","                    else:\n","                        dispute_matrix[nxt_author_id][cur_author_id] += 1\n","\n","        for key in Map[cid].keys():\n","            dfs1(Map[cid], cmntMap, key)\n","\n","    for e in threads:\n","        if 'root' in e.metaL:\n","            dfs1(e.metaL, e.comments)\n","        if 'root' in e.metaR:\n","            dfs1(e.metaR, e.comments)\n","        \n","    # Create NetworkX graphs from the adjacency matrices.\n","    # We need nx graphs in order to get various network stats provided in nx\n","    # library.\n","    support_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if support_matrix[i][j] != 0:\n","                support_graph.add_weighted_edges_from([(i, j, support_matrix[i][j])])\n","\n","    dispute_graph = nx.DiGraph()\n","    for i in range(author_count):\n","        for j in range(author_count):\n","            if dispute_matrix[i][j] != 0:\n","                dispute_graph.add_weighted_edges_from([(i, j, dispute_matrix[i][j])])\n","    \n","    return (author_map, reverse_map, author_count, support_graph, support_matrix, dispute_graph, dispute_matrix)"],"metadata":{"id":"OTUtPVcCfNFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct global support and dispute network for entire CreateDebate corpus\n","user_map, user_reverse_map, user_count, support_graph, support_matrix, dispute_graph, dispute_matrix = build_graph(user_list)"],"metadata":{"id":"UJwzsKeZfUzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_reciprocity_stats(user_subset):\n","    \"\"\"\n","    Returns reciprocity for given subset of users in local support and dispute \n","    graphs\n","\n","    >>> support_r, dispute_r = get_reciprocity_stats(user_subset)\n","    \"\"\"\n","    _, _, _, support_graph_, _, dispute_graph_, _ = build_graph(user_subset)\n","\n","    try:\n","        support_graph_r = nx.algorithms.reciprocity(support_graph_)\n","    except:\n","        support_graph_r = 0\n","\n","    try:\n","        dispute_graph_r = nx.algorithms.reciprocity(dispute_graph_)\n","    except:\n","        dispute_graph_r = 0\n","\n","    return support_graph_r, dispute_graph_r"],"metadata":{"id":"Q8j_uDbaf2YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing centrality value for each node from global support and \n","# dispute networks. This will be used for computing stats for user subset.\n","support_centrality_dict = nx.algorithms.centrality.degree_centrality(support_graph)\n","dispute_centrality_dict = nx.algorithms.centrality.degree_centrality(dispute_graph)"],"metadata":{"id":"sEkVdT_GfXVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_centrality_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of degree centrality for given user \n","    subset in global support and dispute networks.\n","\n","    >>> s_avg, s_std, d_avg, d_std = get_centrality_stats(user_subset)\n","    \"\"\"\n","    s_c = []\n","    d_c = []\n","\n","    for user in user_subset:\n","        try:\n","            s_c.append(support_centrality_dict[user_map[user]])\n","        except:\n","            pass\n","        try:\n","            d_c.append(dispute_centrality_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(s_c), np.std(s_c), np.average(d_c), np.std(d_c)"],"metadata":{"id":"2tT3eDXSf-Wz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing clustering coeffieient for each node from global support \n","# and dispute networks. This will be used for computing stats for user subset.\n","support_clustering_dict = nx.algorithms.cluster.clustering(support_graph)\n","dispute_clustering_dict = nx.algorithms.cluster.clustering(dispute_graph)"],"metadata":{"id":"x8YHz2Mofo2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_clustering_stats(user_subset):\n","    \"\"\"\n","    Returns mean and standard deviation of clustering coefficient for given user \n","    subset in global support and dispute networks.\n","\n","    >>> s_avg, s_std, d_avg, d_std = get_clustering_stats(user_subset)\n","    \"\"\"\n","    s_c = []\n","    d_c = []\n","\n","    for user in user_subset:\n","        try:\n","            s_c.append(support_clustering_dict[user_map[user]])\n","        except:\n","            pass\n","        try:\n","            d_c.append(dispute_clustering_dict[user_map[user]])\n","        except:\n","            pass\n","    \n","    return np.average(s_c), np.std(s_c), np.average(d_c), np.std(d_c)"],"metadata":{"id":"hpnvgrfufzWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def core_periphery(G, normalize=True):\n","    # Get coreness value for each nodes as dictionary. We use Boyd algorithm.\n","    # For more info about the algorithm used, see\n","    # https://github.com/skojaku/core-periphery-detection\n","    algorithm = cpnet.MINRES()\n","    algorithm.detect(G)\n","    coreness = algorithm.get_coreness()\n","    if normalize:\n","        max_coreness_value = -1\n","        min_coreness_value = 1\n","\n","        for v in coreness.values():\n","            max_coreness_value = max(max_coreness_value, v)\n","            min_coreness_value = min(min_coreness_value, v)\n","        \n","        diff = max_coreness_value - min_coreness_value\n","        \n","        for k in coreness.keys():\n","            coreness[k] = (coreness[k] - min_coreness_value) / diff\n","\n","    return coreness        "],"metadata":{"id":"HaS6gjDbfQh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get dicts containing core-pheriphery values for each node from global support \n","# and dispute networks. This will be used for computing stats for user subset.\n","support_coreness = core_periphery(support_graph)\n","dispute_coreness = core_periphery(dispute_graph)"],"metadata":{"id":"66t9bkcUfrNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_dict(x):\n","    \"\"\"\n","    Normalize elements in given dictionary as\n","        element = (element - min_element) / (max_element - min_element)\n","    \"\"\"\n","    mini = min(x.values())\n","    maxa = max(x.values())\n","\n","    res = dict()\n","\n","    for k, v in x.items():\n","        res[k] = (v - mini) / (maxa - mini)\n","    return res"],"metadata":{"id":"cBYYRhgziBO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_array(x):\n","    \"\"\"\n","    Normalize elements in given array as\n","        element = (element - min_element) / (max_element - min_element)\n","    \"\"\"\n","    assert isinstance(x, (list, tuple)), \"Expected a list or tuple\"\n","    mini = min(x)\n","    maxa = max(x)\n","    res = []\n","    for e in x:\n","        res.append((e - mini) / (maxa - mini))\n","    return res"],"metadata":{"id":"Ro6TUgI1wrGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_core_periphery(user_subset, do_local_normalize=False):\n","    \"\"\"\n","    Returns coreness values as lists from given subset of users in\n","    global support and dispute network.\n","\n","    >>> s_coreness, d_coreness = get_core_periphery(user_subset)\n","    \"\"\"\n","    s_coreness = []\n","    d_coreness = []\n","\n","    for user in user_subset:\n","        try:\n","            s_coreness.append(support_coreness[user_map[user]])\n","        except:\n","            pass\n","        try:\n","            d_coreness.append(dispute_coreness[user_map[user]])\n","        except:\n","            pass\n","        \n","    if do_local_normalize:\n","        s_coreness = normalize_array(s_coreness)\n","        d_coreness = normalize_array(d_coreness)\n","    \n","    return s_coreness, d_coreness"],"metadata":{"id":"DK_B1cSDfulF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_support_coreness(user_subset, do_local_normalize=False):\n","    s_coreness, _ = get_core_periphery(user_subset, do_local_normalize)\n","    plt.hist(s_coreness, bins=[x / 100 for x in range(101)])\n","    plt.xlabel('Core-ness')\n","    plt.ylabel('#Users')\n","    plt.title('Core-ness distribution for support graph')"],"metadata":{"id":"piwZri7FrdOe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_dispute_coreness(user_subset, do_local_normalize=False):\n","    _, d_coreness= get_core_periphery(user_subset, do_local_normalize)\n","    plt.hist(d_coreness, bins=[x / 100 for x in range(101)])\n","    plt.xlabel('Core-ness')\n","    plt.ylabel('#Users')\n","    plt.title('Core-ness distribution for dispute graph')"],"metadata":{"id":"0lHoLpMeyKW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_core_periphery_stats(user_subset, do_local_normalize=False):\n","    \"\"\"\n","    Returns quantile distribution of coreness value for given user subset\n","\n","    >>> s_dist, d_dist = get_core_periphery_stats(user_subset, do_local_normalize=False)\n","    \"\"\"\n","    s_coreness, d_coreness = get_core_periphery(user_subset, do_local_normalize)\n","\n","    s_c_q = [0 for _ in range(4)]\n","    d_c_q = [0 for _ in range(4)]\n","\n","    for x in s_coreness:\n","        if x < 0.25:\n","            s_c_q[0] += 1\n","        elif x >= 0.25 and x < 0.5:\n","            s_c_q[1] += 1\n","        elif x >= 0.5 and x < 0.75:\n","            s_c_q[2] += 1\n","        else:\n","            s_c_q[3] += 1\n","        \n","    for x in d_coreness:\n","        if x < 0.25:\n","            d_c_q[0] += 1\n","        elif x >= 0.25 and x < 0.5:\n","            d_c_q[1] += 1\n","        elif x >= 0.5 and x < 0.75:\n","            d_c_q[2] += 1\n","        else:\n","            d_c_q[3] += 1\n","    \n","    return s_c_q, d_c_q"],"metadata":{"id":"1zpIvtJKyWTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_stats(user_subset, do_local_normalize=True):\n","    n                                          = len(user_subset)\n","    s_reci, d_reci                             = get_reciprocity_stats(user_subset) \n","    s_deg_avg, s_deg_std, d_deg_avg, d_deg_std = get_centrality_stats(user_subset)\n","    s_clu_avg, s_clu_std, d_clu_avg, d_clu_std = get_clustering_stats(user_subset)\n","    user_chr_avg, user_chr_std                 = profile_characteristics_stats(user_subset) \n","    s_quantile_dist, d_quantile_dist           = get_core_periphery_stats(user_subset, \n","                                                                          do_local_normalize=do_local_normalize)\n","\n","    print('Size: %d' % n)\n","    print('Support graph reciprocity: %.2f' % s_reci)\n","    print('Dispute graph reciprocity: %.2f' % d_reci)\n","\n","    print('Support graph degree centrality: %.5f ± %.5f' % (s_deg_avg, s_deg_std))\n","    print('Dispute graph degree centrality: %.5f ± %.5f' % (d_deg_avg, d_deg_std)) \n","\n","    print('Support graph clustering coeff: %.2f ± %.2f' % (s_clu_avg, s_clu_std))\n","    print('Dispute graph clustering coeff: %.2f ± %.2f' % (d_clu_avg, d_clu_std)) \n","\n","    print('Reward points: %.2f ± %.2f' % (user_chr_avg[0], user_chr_std[0]))\n","    print('Efficiency   : %.2f ± %.2f' % (user_chr_avg[1], user_chr_std[1]))\n","    print('# Allies     : %.2f ± %.2f' % (user_chr_avg[2], user_chr_std[2]))\n","    print('# Enemies    : %.2f ± %.2f' % (user_chr_avg[3], user_chr_std[3]))\n","    print('# Hostiles   : %.2f ± %.2f' % (user_chr_avg[4], user_chr_std[4]))\n","\n","    print(f'Support coreness quantile: {s_quantile_dist}')\n","    print(f'Dispute coreness quantile: {d_quantile_dist}')"],"metadata":{"id":"XRPjlEm1zEEM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Core-periphery"],"metadata":{"id":"pnf9NEZ90ZBv"}},{"cell_type":"code","source":["categories_1 = ['religion']\n","categories_2 = ['politics2']"],"metadata":{"id":"y4yiuS8a0YhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["migration_list = get_migrated_users(categories_1, categories_2, categories_1_origin=False, require_migration=False)"],"metadata":{"id":"VQGjDzW3z7ZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(migration_list)"],"metadata":{"id":"jJPZPHSD0lEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AA, AN, NA, NN = partition_migrated_users(migration_list, categories_1, categories_2)"],"metadata":{"id":"fVopzkzv0n5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('AH-AH: %d, AH-NONAH: %d, NONAH-AH: %d, NONAH-NONAH: %d' % (len(AA), len(AN), len(NA), len(NN)))"],"metadata":{"id":"pFGjqWkf0-j6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = AA + AN\n","N = NA + NN"],"metadata":{"id":"VgAXcstG1BMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(AA, True)"],"metadata":{"id":"Y77-1ufV1SqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(AN, True)"],"metadata":{"id":"5Bu0JbOF1cXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(NA, True)"],"metadata":{"id":"ocrsl3d_2rGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(NN, True)"],"metadata":{"id":"GlV2xH8923mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(A, True)"],"metadata":{"id":"ozyKxKei3E8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(N, True)"],"metadata":{"id":"sVTnFS2p-IgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_users_by_quantile_threshold(user_subset, threshold):\n","    _support_coreness = dict()\n","    _dispute_coreness = dict()\n","\n","    for user in user_subset:\n","        try:\n","            _support_coreness[user] = support_coreness[user_map[user]]\n","        except:pass\n","        try:\n","            _dispute_coreness[user] = dispute_coreness[user_map[user]]\n","        except:pass\n","\n","    _support_coreness = normalize_dict(_support_coreness)\n","    _dispute_coreness = normalize_dict(_dispute_coreness)\n","\n","    res = set() \n","\n","    for k, v in _support_coreness.items():\n","        if v > threshold:\n","            res.add(k)\n","    for k, v in _dispute_coreness.items(): \n","        if v > threshold: \n","            res.add(k)\n","    return res"],"metadata":{"id":"6dNquTCyg7AL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A_core_users = get_users_by_quantile_threshold(A, threshold=0.5)\n","N_core_users = get_users_by_quantile_threshold(N, threshold=0.5)"],"metadata":{"id":"HB1dVvIqkmjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(A_core_users)"],"metadata":{"id":"GxkwMylIkyhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_stats(N_core_users)"],"metadata":{"id":"5EYSfJ-Nk4a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Polar vs. non-polar posts and comments"],"metadata":{"id":"7qxI1JHBL9Ae"}},{"cell_type":"code","source":["n_polar_posts = dict()\n","n_nonpolar_posts = dict()\n","n_polar_comments = dict()\n","n_nonpolar_comments = dict()"],"metadata":{"id":"OCQuLE8KMEcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_polar_comments_ah = dict()\n","n_nonpolar_comments_ah = dict()"],"metadata":{"id":"R_VP5m8hppBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AA_polar_comments = dict()\n","AA_nonpolar_comments = dict()\n","\n","AN_polar_comments = dict()\n","AN_nonpolar_comments = dict()\n","\n","NA_polar_comments = dict()\n","NA_nonpolar_comments = dict()\n","\n","NN_polar_comments = dict()\n","NN_nonpolar_comments = dict()"],"metadata":{"id":"Y7VYYvU-XE9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AA = set(AA)\n","AN = set(AN)\n","NA = set(NA)\n","NN = set(NN)"],"metadata":{"id":"sz7gmpffX5ki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    n_polar_posts[cat] = 0\n","    n_nonpolar_posts[cat] = 0 \n","    n_polar_comments[cat] = 0 \n","    n_nonpolar_comments[cat] = 0 \n","\n","    n_polar_comments_ah[cat] = 0\n","    n_nonpolar_comments_ah[cat] = 0\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    for thread in threads:\n","        if len(thread.metaR) == 0:\n","            n_nonpolar_posts[cat] += 1\n","            n_nonpolar_comments[cat] += len(thread.comments)\n","\n","            for cid in thread.comments.keys():\n","                try:\n","                    if ah_score_comments[int(cid[3:])] > 0.5:\n","                        n_nonpolar_comments_ah[cat] += 1\n","                except:pass\n","            \n","            for comment in thread.comments.values():\n","                athr = comment.author\n","                if athr in AA:\n","                    try:\n","                        AA_nonpolar_comments[athr] += 1\n","                    except:\n","                        AA_nonpolar_comments[athr] = 1\n","                elif athr in AN:\n","                    try:\n","                        AN_nonpolar_comments[athr] += 1\n","                    except:\n","                        AN_nonpolar_comments[athr] = 1\n","                elif athr in NA:\n","                    try:\n","                        NA_nonpolar_comments[athr] += 1\n","                    except:\n","                        NA_nonpolar_comments[athr] = 1\n","                elif athr in NN:\n","                    try:\n","                        NN_nonpolar_comments[athr] += 1\n","                    except:\n","                        NN_nonpolar_comments[athr] = 1\n","\n","        else:\n","            n_polar_posts[cat] += 1\n","            n_polar_comments[cat] += len(thread.comments)\n","\n","            for cid in thread.comments.keys():\n","                try:\n","                    if ah_score_comments[int(cid[3:])] > 0.5:\n","                        n_polar_comments_ah[cat] += 1\n","                except:pass\n","            \n","            for comment in thread.comments.values():\n","                athr = comment.author\n","                if athr in AA:\n","                    try:\n","                        AA_polar_comments[athr] += 1\n","                    except:\n","                        AA_polar_comments[athr] = 1\n","                elif athr in AN:\n","                    try:\n","                        AN_polar_comments[athr] += 1\n","                    except:\n","                        AN_polar_comments[athr] = 1\n","                elif athr in NA:\n","                    try:\n","                        NA_polar_comments[athr] += 1\n","                    except:\n","                        NA_polar_comments[athr] = 1\n","                elif athr in NN:\n","                    try:\n","                        NN_polar_comments[athr] += 1\n","                    except:\n","                        NN_polar_comments[athr] = 1"],"metadata":{"id":"gsSX1-n5MmI-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_polar_posts.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"0uj-ReYQaqLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_nonpolar_posts.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"bKfXXHPdPnO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_polar_comments.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"scPnq4UHS1CS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_nonpolar_comments.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"8exSi10bTGiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_polar_comments_ah.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"dVlTPSokdm3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in n_nonpolar_comments_ah.items():\n","    print(f'{k} - {v}')"],"metadata":{"id":"83nL9lbdtGD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_polarity_stats(group):\n","    pol  = 'list(' + group + '_polar_comments.values())'\n","    npol = 'list(' + group + '_nonpolar_comments.values())'\n","\n","    p_avg = np.mean(eval(pol))\n","    p_std = np.std(eval(pol))\n","\n","    np_avg = np.mean(eval(npol))\n","    np_std = np.std(eval(npol))\n","\n","    print(f'Group: {group}')\n","    print(f'# Polar comments: {p_avg: .2f} ± {p_std: .2f}')\n","    print(f'# Non-polar comments: {np_avg: .2f} ± {np_std: .2f}')"],"metadata":{"id":"geJ3OSQ2tgx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_polarity_stats('AA')"],"metadata":{"id":"0Tsj4scdZAMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_polarity_stats('AN')"],"metadata":{"id":"OviE-mhFaNhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_polarity_stats('NA')"],"metadata":{"id":"lf5veFw1arh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_polarity_stats('NN')"],"metadata":{"id":"AxQKd63gaxlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vtawPPRtazwT"},"execution_count":null,"outputs":[]}]}