{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM8GpFHLrN1erugQy2eVYqt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Empath results for For-against vs. Perspective debates\n","\n","This notebook is different from the previous notebook in following ways:\n","* Uses lemmatization for better match\n","* Extracts noun pharses"],"metadata":{"id":"VW0gX84ANvAi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXjgUYEINl5t"},"outputs":[],"source":["# Mount Google drive to Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["# Clone `CreateDebateScraper` library from github\n","!git clone https://github.com/utkarsh512/CreateDebateScraper.git\n","%cd CreateDebateScraper/src/nested"],"metadata":{"id":"9Lr-97x4OLIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from   collections              import namedtuple\n","from   copy                     import deepcopy\n","# import cpnet\n","from   itertools                import accumulate\n","import json\n","from   matplotlib               import pyplot as plt\n","import networkx as nx\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import spacy\n","from   scipy                    import stats\n","import textwrap\n","from   thread                   import Comment, Thread\n","from   tqdm                     import tqdm\n","nltk.download('punkt') # For tokenizers\n","nltk.download('stopwords')\n","nltk.download('wordnet') # For lemmatizers\n","nltk.download('omw-1.4')\n","import matplotlib\n","from   nltk.stem                import WordNetLemmatizer\n","from   nltk.tokenize            import TweetTokenizer\n","from   nltk.corpus              import stopwords\n","from   pprint                   import pprint\n","# from   transformers             import BertModel, BertTokenizer\n","# import shifterator as sh\n","# import wordcloud\n","# import skbio\n","matplotlib.rcParams.update({'font.size': 18})\n","matplotlib.rcParams[\"figure.figsize\"] = (12, 5)\n","STOP_WORDS = list(stopwords.words('english'))\n","!python -m spacy download en_core_web_sm"],"metadata":{"id":"j3aIfYnIOWXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy import displacy"],"metadata":{"id":"wIEAzi4gUa6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scapy_nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"q8hcuG6sYWHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_empath_dictionary():\n","    \"\"\"\n","    Returns a dict[str, list] object where keys are categories and values are \n","    associated words for that category\n","    \"\"\"\n","    empath_dict = dict()\n","    with open('/content/gdrive/MyDrive/DL/empath/dictionary.tsv', 'r') as f:\n","        for line in f:\n","            cols = line.strip().split(\"\\t\")\n","            name = cols[0]\n","            terms = cols[1:]\n","            empath_dict[name] = list()\n","            for t in set(terms):\n","                empath_dict[name].append(t)\n","    return empath_dict"],"metadata":{"id":"ZNd0AnRQOhfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empath = load_empath_dictionary()"],"metadata":{"id":"6UKGikfdOq7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(empath.keys())"],"metadata":{"id":"F0Blo40uOvBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_count = list()\n","for v in empath.values():\n","    tokens_count.append(len(v))"],"metadata":{"id":"qwfh7JJ5PA7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Average token count {np.average(tokens_count)}, Std. dev {np.std(tokens_count)}')"],"metadata":{"id":"B8jEO5yWPCkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We hand-picked categories that are more close to ad hominem triggers\n","\n","empath_selected_tokens = list()\n","\n","with open('/content/gdrive/MyDrive/DL/empath/empath_selected_categories.txt', 'r') as f:\n","    for line in f:\n","        empath_selected_tokens.append(line.strip())"],"metadata":{"id":"ZIupHY-OPFlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SOTA slur word dictionary (from Punyajoy)\n","slur_words_dict = json.load(open('/content/gdrive/MyDrive/DL/slurwords/slur_dictionary.json'))"],"metadata":{"id":"UJC2zKS-PRBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine selected empath tokens and slur words to create list of triggers\n","\n","triggers = dict()\n","triggers.update(slur_words_dict)\n","for key in empath_selected_tokens:\n","    triggers[key] = list()\n","    for token in empath[key]:\n","        triggers[key].append(' '.join(token.split('_')))"],"metadata":{"id":"OT_lUpBvRL3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","\n","triggers_lemma = dict()\n","for k in tqdm(triggers.keys()):\n","    triggers_lemma[k] = set(map(lemmatizer.lemmatize, triggers[k]))"],"metadata":{"id":"TvAcE2hERx5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["triggers_lemma"],"metadata":{"id":"7qdF5kD_ZbHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom routine to clean texts scraped from Web.\n","# It removes hyperlinks, punctuation marks (except apostrophe)\n","\n","tknz = TweetTokenizer()\n","\n","def clean_text(text):\n","    \"\"\"\n","    Preprocessing text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"www\\S+\", \"\", text)\n","    text = re.sub(\"-\", \" \", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    text = re.sub(\"\\u2018\", \"X\", text) \n","    text = re.sub(\"\\u2019\", \"X\", text) \n","    text = re.sub(\"\\'\", \"X\", text) \n","    wordTokens_ = tknz.tokenize(text)\n","    wordTokens = list()\n","    for x in wordTokens_:\n","        x = ''.join([v for v in x if v.isalnum() or v == ' '])\n","        if len(x) > 0 and x != 'X':\n","            x = x.replace('X', '\\'')\n","            wordTokens.append(x)\n","    return wordTokens"],"metadata":{"id":"Ji7JY4_YWIKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments = dict()\n","\n","# Topical forums on CreateDebate. We have scraped comments for all of the\n","# following forurm.\n","categories = ['business', 'comedy', 'entertainment', 'health', 'law', 'nsfw',\n","              'politics2', 'religion', 'science', 'shopping', 'sports',\n","              'technology', 'travel', 'world']\n","\n","# However, we will be analyzing comments from selected forum only!\n","# These forum have at least 10k comments each.\n","categories_selected = ['politics2', 'religion', 'world', \n","                       'science', 'law', 'technology']\n","\n","for x in categories_selected:\n","    comments[x] = list()"],"metadata":{"id":"pGnz848nXEJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading comments from select forums\n","\n","for cat in tqdm(categories_selected):\n","    fp = open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/threads.log', 'rb')\n","\n","    # Get all the `Thread` objects pickled while scraping.\n","    threads = list()\n","    try:\n","        while True:\n","            e = pickle.load(fp)\n","            threads.append(e)\n","    except EOFError:\n","        fp.close()\n","\n","    # While classifying CreateDebate comments, we used comments as per author mode.\n","    # Hence, using the same mode to attach classification score with the comments.\n","    # \n","    # score < 0.5 -> ad hominem comment\n","    #       > 0.5 -> non ad hominem comment\n","    authors = dict()\n","    for thread in threads:\n","        for k, v in thread.comments.items():\n","            try:\n","                authors[v.author].append((v, k))\n","            except:\n","                authors[v.author] = list()\n","                authors[v.author].append((v, k))\n","\n","    ctr = 0\n","    # Load the classification score of the comments.\n","    with open('/content/gdrive/MyDrive/DL/CreateDebate/' + cat + '/comments_with_score.log', 'rb') as fp:\n","        cws = pickle.load(fp)\n","    # Attach classification score with the comments.\n","    for author in authors.keys():\n","        for i in range(len(authors[author])):\n","            comment, cid = authors[author][i]\n","            foo = deepcopy(comment.__dict__)\n","            foo['tag'] = cat\n","            foo['score'] = cws[ctr][0]\n","            foo['validation'] = cws[ctr][1][0]\n","            foo['id'] = int(cid[3:])\n","            comments[cat].append(foo)\n","            ctr += 1"],"metadata":{"id":"oddlzNzuXH6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_score_comments = dict()\n","\n","for cat in categories_selected:\n","    for comment in comments[cat]:\n","        ah_score_comments[comment['id']] = 1 - comment['score']"],"metadata":{"id":"uCQHrtB5XKZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_tstring(tstring):\n","    \"\"\"\n","    Parses comment's time to an integer to enable\n","    comparison between comments based on their time of posting\n","    \"\"\"\n","    if tstring == 'Not Available':\n","        raise ValueError('Invalid posting time for parse_tstring')\n","    tstring = tstring.replace('T', '-').replace(':', '-').replace('+', '-').split('-')\n","    return int(''.join(tstring[:-2]))"],"metadata":{"id":"iz6uLsLtXRiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_debates = dict()\n","perspective_debates = dict()\n","\n","for cat in categories_selected:\n","    for_against_debates[cat] = list()\n","    perspective_debates[cat] = list()\n","\n","    for comment in comments[cat]:\n","        if comment['polarity'] == 'Not Available':\n","            perspective_debates[cat].append(deepcopy(comment))\n","        else:\n","            for_against_debates[cat].append(deepcopy(comment))"],"metadata":{"id":"g7ABf5E9XTp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","for cat in categories_selected:\n","    print(len(for_against_debates[cat]), len(perspective_debates[cat]))"],"metadata":{"id":"n15oEkEcXVth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dependency_graph(doc):\n","    \"\"\"Create dependency graph of tokens using scapy\n","    \"\"\"\n","    dependency_edges = list() # (parent, child, relationship)\n","    id_to_text = dict()\n","    id_to_token = dict()\n","    root = None\n","    node_count = 0\n","\n","    for token in doc:\n","        node_count += 1\n","        parent = token.head.i\n","        child = token.i\n","        relationship = token.dep_\n","        id_to_text[child] = lemmatizer.lemmatize(token.text)\n","        id_to_token[child] = token\n","        if relationship == 'ROOT':\n","            root = child\n","            continue\n","        dependency_edges.append((parent, child, relationship))\n","\n","    dependency_graph = dict()\n","    for i in range(node_count): \n","        dependency_graph[i] = list()\n","    for p, c, r in dependency_edges:\n","        dependency_graph[p].append((c, r))\n","        dependency_graph[c].append((p, r))\n","    \n","    return dependency_graph, id_to_text, id_to_token, root"],"metadata":{"id":"0rDRCGhxYn-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_personal_pronoun_ids(id_to_token):\n","    \"\"\"Generates ids which are indices of personal pronouns\n","    \"\"\"\n","    for k, v in id_to_token.items():\n","        if v.tag_ == 'PRP': # Personal pronoun tag in scapy\n","            yield k"],"metadata":{"id":"ovpFVvLBneVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pronoun_ids(id_to_token):\n","    \"\"\"Generates ids which are indices of pronouns\n","    \"\"\"\n","    for k, v in id_to_token.items():\n","        if v.pos_ == 'PRON': # Pronoun tag in scapy\n","            yield k"],"metadata":{"id":"IzY3BFAx5XLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_ids(id_to_text, trigger_type):\n","    \"\"\"Generates ids which are indices of triggers\n","\n","    :param id_to_text: id_to_text returned by create_dependency_graph\n","    :type id_to_text: dict\n","    :param trigger_type: What type of triggers? Must be a key of triggers_lemma\n","    :type trigger_type: str\n","    \"\"\"\n","    for k, v in id_to_text.items():\n","        if v in triggers_lemma[trigger_type]:\n","            yield k"],"metadata":{"id":"x7f0AamKonh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import deque"],"metadata":{"id":"MEzsFEcaqe83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def breadth_first_search(dependency_graph, source):\n","    \"\"\"Performs breadth first search\n","\n","    :param dependency_graph: Dependency graph returned by create_dependency_graph\n","    :type dependency_graph: dict\n","    :param source: Source node ID\n","    :type source: int\n","    \"\"\"\n","    q = deque()\n","    used = set()\n","    d = dict() # distance of nodes from source\n","    p = dict() # parent in bfs\n","    r = dict() # relation observed\n","\n","    q.append(source)\n","    used.add(source)\n","    p[source] = -1\n","    d[source] = 0\n","\n","    while len(q):\n","        v = q.popleft()\n","        for u, rel in dependency_graph[v]:\n","            if u in used:\n","                continue\n","            used.add(u)\n","            q.append(u)\n","            d[u] = d[v] + 1\n","            p[u] = v\n","            r[u] = rel\n","\n","    return d, p, r"],"metadata":{"id":"K2GjfmUcreMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_path_from_bfs(source, dest, dist_dict, parent_dict, relation_dict):\n","    \"\"\"Generate path from source to dest. Path will contain relationships \n","    encountered in bfs.\n","    \"\"\"\n","    assert dist_dict[source] == 0\n","    assert dest in dist_dict \n","\n","    path = list()\n","    cur = dest\n","    while cur != source:\n","        path.append(relation_dict[cur])\n","        cur = parent_dict[cur]\n","    \n","    return '->'.join(path)"],"metadata":{"id":"NqDA8NyauCOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_count(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes count of ad hominem triggers associated with indices generated\n","    by index_generator\n","    \"\"\"\n","\n","    trigger_count = dict()\n","    for trigger_type in triggers_lemma.keys():\n","        trigger_count[trigger_type] = 0\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in triggers_lemma.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        trigger_count[trigger_type] += 1\n","    \n","    return trigger_count"],"metadata":{"id":"Vo9ZBCXOo8K3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_count_by_path(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes count of ad hominem triggers associated with indices generated\n","    by index_generator\n","    \"\"\"\n","\n","    trigger_count_by_path = dict()\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size)\n","\n","    for doc in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in triggers_lemma.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        path = generate_path_from_bfs(index, trigger_id,\n","                                                      dist, parent, relation)\n","                        if path in trigger_count_by_path:\n","                            trigger_count_by_path[path] += 1\n","                        else:\n","                            trigger_count_by_path[path] = 1\n","    \n","    return trigger_count_by_path"],"metadata":{"id":"E1TbkE3e0WA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_trigger_path_adhominem_score(texts, index_generator, n_process=2, batch_size=1000):\n","    \"\"\"Computes count of ad hominem triggers associated with indices generated\n","    by index_generator\n","    \"\"\"\n","\n","    trigger_path_adhominem_score = dict()\n","\n","    docs = scapy_nlp.pipe(texts, n_process=n_process, batch_size=batch_size,\n","                          as_tuples=True)\n","\n","    for doc, context in tqdm(docs, total=len(texts)):\n","        # Parse comment text and create dependency graph\n","        dependency_graph, id_to_text, id_to_token, root \\\n","                                             = create_dependency_graph(doc)\n","\n","        # Extract the indices using iterator\n","        for index in index_generator(id_to_token):\n","            dist, parent, relation = breadth_first_search(dependency_graph,\n","                                                          index)\n","            for trigger_type in triggers_lemma.keys():\n","                for trigger_id in get_trigger_ids(id_to_text, trigger_type):\n","                    if trigger_id in dist:\n","                        path = generate_path_from_bfs(index, trigger_id,\n","                                                      dist, parent, relation)\n","                        if path not in trigger_path_adhominem_score:\n","                            trigger_path_adhominem_score[path] = list()\n","                        trigger_path_adhominem_score[path].append(context['score'])\n","    \n","    return trigger_path_adhominem_score"],"metadata":{"id":"fA2puRS4dwOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fa_politics_texts = [(comment['body'].lower(), {'score': 1 - comment['score']}) \\\n","                     for comment in for_against_debates['politics2']]"],"metadata":{"id":"tx5R78e4GSTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fa_politics_trigger_count = \\\n","      get_trigger_path_adhominem_score(fa_politics_texts, get_personal_pronoun_ids)"],"metadata":{"id":"MPeUCxvdHzpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/61-for-against-debates-trigger-personal-pronoun-count-path-ahscore.pkl', 'wb') as f:\n","    pickle.dump(fa_politics_trigger_count, f)"],"metadata":{"id":"fXCARrXsLOkT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/61-for-against-debates-trigger-personal-pronoun-count-path-ahscore.pkl', 'rb') as f:\n","    fa_politics_trigger_count = pickle.load(f)"],"metadata":{"id":"ADQccmcLTSCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pers_politics_texts = [(comment['body'].lower(), {'score': 1 - comment['score']}) \\\n","                       for comment in perspective_debates['politics2']]"],"metadata":{"id":"PyogCMKES-er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pers_politics_trigger_count = \\\n","      get_trigger_path_adhominem_score(pers_politics_texts, get_personal_pronoun_ids)"],"metadata":{"id":"uZ7eNoWLTJH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/61-perspective-debates-trigger-personal-pronoun-count-path-ahscore.pkl', 'wb') as f:\n","    pickle.dump(pers_politics_trigger_count, f)"],"metadata":{"id":"sVLjnsvfT7Yq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/gdrive/MyDrive/Temp/61-perspective-debates-trigger-personal-pronoun-count-path-ahscore.pkl', 'rb') as f:\n","    pers_politics_trigger_count = pickle.load(f)"],"metadata":{"id":"w6vOV8fMVkTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_ah_relations(trigger_count, n, th):\n","    s = list()\n","    for k, v in trigger_count.items():\n","        if len(v) < th:\n","            continue\n","        avg = np.average(v)\n","        s.append((k, avg))\n","    s = sorted(s, reverse=True, key=lambda z: z[1])\n","    for foo in s[:n]:\n","        yield foo"],"metadata":{"id":"2p9V_beQpaCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x, y in get_ah_relations(pers_politics_trigger_count, 10, 100):\n","    x = x.split('->')[::-1]\n","    x = list(map(spacy.explain, x))\n","    x = '-->'.join(x)\n","    print(f'{y:.3f}: {x}')"],"metadata":{"id":"LB5xpNOIp_J_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_top_relations(trigger_count, th):\n","    s = list()\n","    for k, v in trigger_count.items():\n","        s.append((k, v))\n","    s = sorted(s, reverse=True, key=lambda z: z[1])\n","    res = set()\n","    for k, v in s[:th]:\n","        res.add(k)\n","    return res"],"metadata":{"id":"XX2IL69AMlJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = []\n","y = []\n","\n","for th in (10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000):\n","    k1 = get_top_relations(fa_politics_trigger_count, th)\n","    k2 = get_top_relations(pers_politics_trigger_count, th)\n","    val = len(k1 & k2)\n","    x.append(th)\n","    y.append(val/ th)"],"metadata":{"id":"-_8WXzI8WZ7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot(x, y):\n","    ticks = np.arange(len(x))\n","    width = 0.60\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks, y, width, tick_label=x)\n","\n","    ax.set_ylabel('Overlap')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"Xurf6aUdXb7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot(x, y)"],"metadata":{"id":"YlDZQ9ttX3Iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fa_politics_trigger_count"],"metadata":{"id":"FLAfPdjQx9rI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for_against_comment_count = len(for_against_debates['politics2'])\n","perspective_comment_count = len(perspective_debates['politics2'])"],"metadata":{"id":"Ea08DIkKWxFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def plot_trigger_count(trigger_types):\n","    x = trigger_types\n","    y1 = [] \n","    y2 = [] \n","\n","    for trigger_type in trigger_types:\n","        y1.append(fa_politics_trigger_count[trigger_type] / for_against_comment_count)\n","        y2.append(pers_politics_trigger_count[trigger_type] / perspective_comment_count)\n","\n","    ticks = np.arange(len(x))\n","    width = 0.30\n","\n","    fig, ax = plt.subplots()\n","    subplot1 = ax.bar(ticks - width / 2, y1, width, label='for-against', tick_label=x)\n","    subplot2 = ax.bar(ticks + width / 2, y2, width, label='perspective', tick_label=x)\n","\n","    ax.set_ylabel('Trigger count')\n","    ax.set_xticks(ticks)\n","    ax.set_xticklabels(x, rotation=45, ha='right')\n","    ax.legend()\n","    plt.show()"],"metadata":{"id":"oByiF2R4V1LG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trigger_types = list(triggers_lemma.keys())"],"metadata":{"id":"7kuWGdBiXqJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_trigger_count(trigger_types[50:])"],"metadata":{"id":"WSVQfaQGX4V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fa_politics_trigger_count = \\\n","#       get_trigger_count(fa_politics_texts, get_personal_pronoun_ids)\n","ForAgainstTriggerCount = dict()\n","PerspectiveTriggerCount = dict()\n","\n","for category in categories_selected: \n","    fa_texts = [comment['body'].lower() \\\n","                for comment in for_against_debates[category]]\n","    ps_texts = [comment['body'].lower() \\\n","                for comment in perspective_debates[category]]\n","    ForAgainstTriggerCount[category] = \\\n","                get_trigger_count(fa_texts, get_personal_pronoun_ids)\n","    PerspectiveTriggerCount[category] = \\\n","                get_trigger_count(ps_texts, get_personal_pronoun_ids)"],"metadata":{"id":"gUesFOsEYFlJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FKCu3ZFPaocI"},"execution_count":null,"outputs":[]}]}